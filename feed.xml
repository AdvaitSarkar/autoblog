<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>Talking about papers</title>
    <link href="https://advaitsarkar.github.io/autoblog/feed.xml" rel="self" />
    <link href="https://advaitsarkar.github.io/autoblog" />
    <updated>2025-04-10T16:07:11+01:00</updated>
    <author>
        <name>TAP Communications</name>
    </author>
    <id>https://advaitsarkar.github.io/autoblog</id>

    <entry>
        <title>Visual Programming Tools Make Learning Probabilistic Programming Easier for Novices</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/visual-programming-tools-make-learning-probabilistic-programming-easier-for-novices.html"/>
        <id>https://advaitsarkar.github.io/autoblog/visual-programming-tools-make-learning-probabilistic-programming-easier-for-novices.html</id>

        <updated>2025-04-10T16:07:11+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the findings of a study that explored how multiple representation tools can help novice programmers&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the findings of a study that explored how multiple representation tools can help novice programmers learn probabilistic programming. The research demonstrates that visualizing code as both text and diagrams significantly reduces effort in learning and applying probabilistic programming concepts. The study evaluates a custom-built development environment that shows code, network diagrams, and probability distributions simultaneously, revealing measurable benefits for beginners.</p><p><strong>Reference</strong>: Gorinova, M. I., Sarkar, A., Blackwell, A. F., &amp; Syme, D. (2016). A live, multiple-representation probabilistic programming environment for novices. In <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em> (pp. 2533–2537). ACM. <a href="https://doi.org/10.1145/2858036.2858221">https://doi.org/10.1145/2858036.2858221</a></p><h2 id="what-makes-probabilistic-programming-so-challenging-to-learn">What Makes Probabilistic Programming So Challenging to Learn?</h2>
<p>Traditional programming is built on a deceptively simple concept: variables hold specific values. You assign the number 5 to variable “x,” and that’s what x contains until you change it. For generations of programmers, this fundamental concept has been the entry point to computational thinking.</p><p>But what if a variable doesn’t represent a single value, but an entire probability distribution? What if changing one variable automatically updates the probabilities of other variables? Welcome to the mind-bending world of probabilistic programming, a growing paradigm that’s becoming increasingly important for data analytics, machine learning, and statistical modeling.</p><p>“The conventional understanding that a variable holds only one value has been suggested as the single most important concept determining whether someone is able to learn programming,” notes the paper. “The conceptual model of probabilistic programming, where variables embody distributions, involves a substantial shift from this convention.”</p><p>This conceptual leap makes probabilistic programming notoriously difficult for newcomers to grasp. Traditional code editors show only text, forcing learners to mentally visualize complex probability relationships while simultaneously wrestling with unfamiliar syntax.</p><h2 id="multiple-representation-environments-show-code-in-different-ways-simultaneously">Multiple Representation Environments Show Code in Different Ways Simultaneously</h2>
<p>The research team developed a specialized programming environment that displays three synchronized views of the same program:</p><ol>
<li>A traditional text editor showing the actual code</li>
<li>A Bayesian network diagram showing variables and their dependencies as nodes and arrows</li>
<li>Charts displaying the probability distributions of each variable</li>
</ol>
<p>This approach builds on established educational theory about multiple representations. When we see the same concept presented in different formats simultaneously, our understanding deepens as we make connections between these representations.</p><p>The system maintains “liveness,” meaning that as you type code, the visual representations update in real-time. This immediate feedback loop creates a powerful learning environment where students can see the consequences of their changes instantly.</p><p>As one study participant enthused when switching from the visual environment back to a text-only editor: “Argh, no graph, nooo!”</p><h2 id="the-study-shows-significant-reductions-in-programming-effort-with-visual-tools">The Study Shows Significant Reductions in Programming Effort with Visual Tools</h2>
<p>To measure the actual impact of this multi-view approach, the researchers conducted a controlled experiment with 16 undergraduate computer science and mathematics students. None had prior experience with probabilistic programming, though all had basic knowledge of probability theory.</p><p>The experiment unfolded in two phases. In the practice phase, students were divided into two groups: one using the Multiple Representation Environment (MRE) and one using a Conventional Editor (CE) showing only code. All students worked through the same tutorial materials and introductory exercises.</p><p>Even in this initial learning phase, the benefits became apparent. Students using the visual system typed significantly fewer keystrokes to complete the same exercises, suggesting they grasped concepts more quickly and made fewer errors.</p><p>The real payoff came in the second phase, where all participants performed debugging and description tasks using both editors in a counterbalanced design. The results were striking:</p><p>“With the MRE, participants gained a median task time improvement of 71s, a median reduction of 63.5 keystrokes, and a median reduction of 5.5 deletions.”</p><p>In practical terms, students completed tasks faster, typed less, and made fewer mistakes when they could see the visual representations alongside the code.</p><h2 id="visualizations-change-how-learners-think-about-programs">Visualizations Change How Learners Think About Programs</h2>
<p>Perhaps most interesting were the qualitative observations about how the visualizations changed students’ mental models. Without visuals, students tended to interpret programs line-by-line, reciting statements like “when Cloudy is true, Rain is Bernoulli(0.7), otherwise it is Bernoulli(0.1).”</p><p>With the visual environment, they shifted to higher-level thinking about relationships: “It’s more likely to rain when it is Cloudy, and consequently it is more likely to be wet when it is Cloudy.”</p><p>One participant remarked: “Looking at the code is horrific… but looking at the graph is not that bad.” Another announced: “I’m not going to look at the code anymore!” after discovering the visual representation.</p><p>These comments reveal how the right visualization can transform an incomprehensible wall of code into an intuitive model that corresponds to how we naturally think about probability relationships.</p><h2 id="study-limitations-suggest-caution-when-interpreting-results">Study Limitations Suggest Caution When Interpreting Results</h2>
<p>While the results are promising, several limitations should be noted. The sample size was relatively small at 16 participants, and all were undergraduates with strong technical backgrounds. Whether these benefits would extend to other populations remains an open question.</p><p>Additionally, the study focused on relatively simple exercises rather than the complex models data scientists might build in professional settings. The researchers acknowledge this limitation, noting: “A natural next step would be to extend this investigation from interpretation and debugging tasks, to more exploratory development tasks (as carried out by professional data analysts).”</p><p>The study also didn’t include tasks where participants had to create programs from scratch, focusing instead on modification and interpretation exercises. This reflected the reality that novice end-user developers often modify existing programs rather than authoring new ones, but it doesn’t tell us whether the visual approach would help with program creation.</p><h2 id="visual-programming-tools-offer-promise-for-making-advanced-concepts-more-accessible">Visual Programming Tools Offer Promise for Making Advanced Concepts More Accessible</h2>
<p>Despite these limitations, the research offers compelling evidence that multiple representation environments can make challenging programming concepts more approachable. By leveraging our visual processing abilities and connecting abstract code to familiar diagrammatic conventions, these tools lower the barriers to entry for probabilistic programming.</p><p>As data analytics and machine learning become increasingly central to fields beyond computer science, tools that make these concepts accessible to broader audiences grow increasingly important. The visual approach tested in this study might be just the beginning of a new generation of programming environments designed not just for efficiency, but for learning and comprehension.</p><p>The next time you find yourself struggling with an abstract programming concept, remember: sometimes what you need isn’t just more code examples, but a different way of seeing the problem.</p><h2 id="references">References</h2>
<p>Gorinova, M. I., Sarkar, A., Blackwell, A. F., &amp; Syme, D. (2016). A live, multiple-representation probabilistic programming environment for novices. In <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em> (pp. 2533–2537). ACM. <a href="https://doi.org/10.1145/2858036.2858221">https://doi.org/10.1145/2858036.2858221</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Learning With Machines: How Constructivist Design Shapes Interactive Machine Learning Systems</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/learning-with-machines-how-constructivist-design-shapes-interactive-machine-learning-systems.html"/>
        <id>https://advaitsarkar.github.io/autoblog/learning-with-machines-how-constructivist-design-shapes-interactive-machine-learning-systems.html</id>

        <updated>2025-04-10T15:48:21+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the concepts presented in a paper exploring how constructivist learning theory can improve the design&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the concepts presented in a paper exploring how constructivist learning theory can improve the design of interactive machine learning systems. The research examines how the feedback loop between users and machine learning systems creates a learning environment similar to educational settings, suggesting new design approaches that could make these systems more effective and user-friendly. </p><p><strong>Reference</strong>: Sarkar, A. (2016). <em>Constructivist design for interactive machine learning</em>. In <em>Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em> (pp. 1467–1475). ACM. <a href="https://doi.org/10.1145/2851581.2892547">https://doi.org/10.1145/2851581.2892547</a></p><h2 id="interactive-machine-learning-systems-help-non-experts-build-statistical-models">Interactive Machine Learning Systems Help Non-Experts Build Statistical Models</h2>
<p>In today’s data-driven world, machine learning has become increasingly important. But what happens when ordinary people without statistical expertise need to use these complex tools? This is where interactive machine learning (IML) systems come into play.</p><p>Interactive machine learning systems are designed to help people who aren’t experts in statistics or computer science build and apply machine learning models for their own purposes. Unlike traditional machine learning approaches that require extensive technical knowledge, IML systems create user-friendly interfaces that allow regular people to train algorithms through simple interactions.</p><p>“As machine learning makes rapid advances, researchers are increasingly interested in enabling people to build and apply machine learning models for their own use in a variety of scenarios,” the paper explains. “These end-users are typically not experts in statistics or machine learning, so careful interaction design is applied, in order to reduce the expertise barriers imposed by the hard concepts of statistical modelling and model programming.”</p><p>The core of IML involves a training loop where users provide examples and judgments to help the system learn. As users label data or make adjustments, the system provides immediate visual feedback about what it’s learning, creating a continuous conversation between human and machine.</p><h2 id="constructivist-learning-theory-provides-a-framework-for-understanding-user-interaction">Constructivist Learning Theory Provides a Framework for Understanding User Interaction</h2>
<p>What exactly is constructivism? It’s an educational theory that views learning not as passive information reception but as an active process where knowledge is constructed through interactions between our ideas and experiences.</p><p>“Constructivism is a theory describing the learning process; the manner in which human knowledge is generated. It posits the view that human knowledge is constructed as a result of the interaction between a person’s mental models and their experiential perceptions,” the paper states.</p><p>This stands in contrast to what the paper calls the “instructionist” view, where learning is treated simply as information delivery. Constructivism, largely attributed to psychologist Jean Piaget, has significantly influenced educational approaches despite mixed evidence about its effectiveness as a teaching tool.</p><p>The connection between constructivism and IML becomes clear when we look at how users interact with these systems. When someone trains a machine learning model, they’re not just inputting data. They’re building understanding through a continuous cycle of action and feedback, much like how children learn through experimentation in constructivist educational environments.</p><h2 id="the-hidden-learning-objectives-of-interactive-machine-learning-systems">The Hidden Learning Objectives of Interactive Machine Learning Systems</h2>
<p>When people use interactive machine learning systems, they’re often focused on practical goals: building a working model or analyzing data. However, the paper argues that beneath these surface-level objectives lie implicit learning outcomes.</p><p>These learning outcomes include:</p><p>“Model-building: learning about the model instance, its strengths, weaknesses, coverage of training data, fitted parameters, etc.”</p><p>“Analysis: learning about the structure of the data, its statistical properties and features.”</p><p>“Exposition to statistical concepts: learning about a particular algorithm, or general concepts about training and testing such as class representation, noisy data, outliers, etc.”</p><p>The paper suggests that by recognizing these hidden learning objectives, developers can design better systems that explicitly support the learning process rather than treating it as an incidental side effect.</p><h2 id="errors-and-mistakes-should-be-embraced-as-learning-opportunities">Errors and Mistakes Should Be Embraced as Learning Opportunities</h2>
<p>In the world of software development, errors are typically viewed as problems to be eliminated. But in constructivist learning, “perturbations” or unexpected results are actually the engine that drives learning forward.</p><p>“The concept of perturbation (or disequilibration, in Piagetian terms) is the engine driving the learning process. It refers to a stimulus which does not conform, or gently subverts, the expectations and mental model of the user, forcing them to construct new knowledge in order to ‘accommodate’ this experience,” the paper explains.</p><p>This perspective suggests that when a machine learning model makes mistakes or produces unexpected results, these moments shouldn’t be treated merely as failures to be corrected. Instead, they represent valuable learning opportunities that help users develop deeper understanding of both their data and the statistical models being applied.</p><p>Current IML systems already focus on addressing errors, but they could benefit from a more constructivist approach that frames mistakes positively rather than negatively.</p><h2 id="future-interactive-machine-learning-systems-should-support-reflection-and-collaboration">Future Interactive Machine Learning Systems Should Support Reflection and Collaboration</h2>
<p>The paper identifies several key areas where interactive machine learning systems could be improved by drawing on constructivist design principles. Two particularly important ones are reflexivity and collaboration.</p><p>Reflexivity refers to users’ ability to critically reflect on their own learning process: “A critical self-awareness of one’s learning, beliefs and knowledge is central to constructivist environments. Reflective users take control over and responsibility for their thoughts, and create a defensible catalogue of provenance for their knowledge.”</p><p>Current IML systems don’t typically support this kind of self-reflection. The paper suggests that capturing detailed interaction histories and allowing users to query and browse these histories could help users better understand how they’ve arrived at particular conclusions.</p><p>Similarly, collaboration is largely missing from most interactive machine learning systems, despite the fact that learning is fundamentally a social activity: “Learning takes place in a social context. The construction of meaning, like so many other activities, seldom occurs individually.”</p><p>Incorporating collaborative features could significantly enhance the effectiveness of these systems, particularly for analytical tasks where multiple perspectives can lead to richer insights.</p><h2 id="the-philosophical-implications-of-machine-learning-tools-deserve-attention">The Philosophical Implications of Machine Learning Tools Deserve Attention</h2>
<p>The paper discusses how interactive machine learning systems act as “cultural mediators” that shape how we think about and understand data.</p><p>“IML systems, especially those with an emphasis on analytical outcomes, need to be aware of their role as cultural mediators,” the paper argues. It notes that these systems “embody epistemological and ontological assertions,” which are currently implicit rather than explicit.</p><p>In other words, the design choices embedded in these systems subtly influence what kinds of knowledge users can produce and how they conceptualize data relationships. This raises important questions about whose perspectives and assumptions are being encoded into these increasingly influential tools.</p><p>The paper highlights ongoing disputes between different statistical approaches, such as “frequentist versus Bayesian approaches, which differ on such fundamental axioms as the interpretation of ‘truth’.” These philosophical differences have practical implications for how systems are designed and what kinds of conclusions they facilitate.</p><p>Ultimately, interactive machine learning systems aren’t neutral tools. They actively shape our understanding of data and influence the kinds of knowledge we can create. By acknowledging this role and designing systems with constructivist principles in mind, developers can create more effective, transparent, and empowering tools.</p><h2 id="references">References</h2>
<p>Sarkar, A. (2016). <em>Constructivist design for interactive machine learning</em>. In <em>Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em> (pp. 1467–1475). ACM. <a href="https://doi.org/10.1145/2851581.2892547">https://doi.org/10.1145/2851581.2892547</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>When Spreadsheets Sing: The Fusion of Music and Programming</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/when-spreadsheets-sing-the-fusion-of-music-and-programming.html"/>
        <id>https://advaitsarkar.github.io/autoblog/when-spreadsheets-sing-the-fusion-of-music-and-programming.html</id>

        <updated>2025-04-10T15:18:07+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down a research paper that explores how spreadsheets can be transformed into accessible tools for music&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down a research paper that explores how spreadsheets can be transformed into accessible tools for music programming and data sonification. The paper introduces “SheetMusic,” a prototype that integrates musical sound effects into the familiar spreadsheet environment, examining whether such a tool should be considered a musical instrument, a programming language, or both. </p><p><strong>Reference</strong>: Sarkar, A. (2016, September). <em>Towards spreadsheet tools for end-user music programming</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 228–231).</p><h2 id="spreadsheets-are-musical-instruments-waiting-to-happen">Spreadsheets Are Musical Instruments Waiting to Happen</h2>
<p>The humble spreadsheet, typically associated with accounting and data analysis, has a secret musical talent that few have recognized. A prototype called “SheetMusic” reveals that beneath the rows and columns of traditional spreadsheets lies potential for creating and manipulating sound in ways that could transform both music composition and data analysis.</p><p>At first glance, SheetMusic looks like any standard spreadsheet application. However, it harbors special functions that allow users to generate musical notes and sequences directly from cells. For example, typing “p(‘c 5’)” plays a single note, while “s([‘c 5’, ‘g 4’, ‘a 4’, ‘e 4’], 1)” plays a sequence of notes.</p><p>“Apart from the play/pause controls, it is largely indistinguishable from a regular spreadsheet,” the paper notes. This familiarity is intentional, leveraging the widespread comfort many non-programmers have with spreadsheet environments.</p><p>This approach sits at the intersection of music and programming. SheetMusic allows users to create simple drum beats with formulas like “if(tick%2==0) p(‘snare’) else p(‘kick’)” which alternates between snare and kick drum sounds on each “tick” or beat.</p><h2 id="the-spreadsheet-format-offers-two-key-advantages-for-musical-coding">The Spreadsheet Format Offers Two Key Advantages for Musical Coding</h2>
<p>The grid-based layout of spreadsheets provides unique benefits that other programming environments might not. According to the paper, “The spreadsheet paradigm is well-known to be an excellent interface for novice end-user programming.” This accessibility means people without formal programming training can create musical compositions through a familiar interface.</p><p>Additionally, the grid format enables what programmers call “rich secondary notation” through cell layout. Users can organize related musical elements visually by grouping them together, separating them with blank cells, or highlighting them with different colors. This visual organization helps create intuitive structure without affecting how the program actually runs.</p><p>The combination of these advantages makes spreadsheets a potentially powerful platform for people to experiment with programmable music without the steep learning curve associated with traditional music programming languages.</p><h2 id="is-sheetmusic-an-instrument-or-a-programming-language">Is SheetMusic an Instrument or a Programming Language?</h2>
<p>The distinction between musical instruments and programming languages becomes blurry when examining tools like SheetMusic. The paper addresses this directly: “A central design question is whether SheetMusic is intended as a composition tool, a musical instrument, or a programming language.”</p><p>This question gets to the heart of how we define musical instruments in the digital age. The paper argues that “the defining characteristic of a musical instrument is a player with intent and agency to musically affect the output of the instrument.” Under this definition, programming environments can indeed function as instruments when users actively manipulate them to create sound.</p><p>What distinguishes SheetMusic from both traditional instruments and simple music playback is its ability to create “music as an interactive, reactive, data-dependent experience.” Unlike a piano that plays the note you press, or a recorded song that plays the same way each time, SheetMusic can generate music that responds to changing data or conditions.</p><h2 id="data-sonification-makes-spreadsheet-information-audible-and-accessible">Data Sonification Makes Spreadsheet Information Audible and Accessible</h2>
<p>One practical application of SheetMusic lies in data sonification, the process of converting data into sound. The paper suggests that “a few SheetMusic formulae could instantly create ‘auditory scatterplots’ or line graphs, where data values are mapped to pitches and played in rapid succession.”</p><p>This approach has been “known to be highly perceptually effective for several types of analysis,” according to the paper. Imagine hearing stock market trends as rising and falling pitches, or rainfall patterns translated into rhythmic intensity. Such auditory representations can reveal patterns that might be missed in visual analyses alone.</p><p>This capability also improves accessibility for visually impaired users, who could hear data trends rather than needing to see them on a chart or graph. The paper highlights that pitch mappings could be “tailored specifically to the data domain,” such as changing octaves or keys at important thresholds within the data.</p><h2 id="time-flows-differently-in-musical-spreadsheets">Time Flows Differently in Musical Spreadsheets</h2>
<p>Traditional musical notation and digital sequencers typically represent time explicitly, with measures, beats, and notes arranged in chronological order. SheetMusic takes a different approach by decoupling time from the physical layout of the spreadsheet.</p><p>The paper explains: “In SheetMusic, time passes independently of the spreadsheet, communicating its current value to all the formulae in the spreadsheet once per ‘tick’.” This means users can organize their musical elements based on logical relationships rather than temporal sequence.</p><p>This approach contrasts with other grid-based music tools like Manhattan, which “sacrifices much layout flexibility by committing columns to denote tracks which execute in parallel, and rows to represent time slices which execute sequentially.” SheetMusic prioritizes layout flexibility, “freeing the layout of the spreadsheet for use as arbitrary secondary notation.”</p><h2 id="limitations-that-need-to-be-addressed-in-future-versions">Limitations That Need to Be Addressed in Future Versions</h2>
<p>While the SheetMusic prototype demonstrates interesting possibilities, it remains in the early stages of development with several limitations. The paper acknowledges that “the precise specification of music/sound-generating library is still emergent,” suggesting that the current functionality is limited and experimental.</p><p>The implementation of time as discrete “ticks” may prove inadequate for complex musical compositions requiring precise timing variations. Currently, notes requiring “sub-tick durations” must be expressed through “stretching/squeezing and offset parameters,” which the paper admits is “an inconvenient notation.”</p><p>Additionally, as with any end-user programming tool, there will be a tension between simplicity and expressiveness. While the prototype allows for “arbitrary Javascript code” in each cell, providing tremendous flexibility for advanced users, this might create barriers for novices who lack programming experience.</p><p>The prototype also appears to lack many standard features of music production software, such as mixing capabilities, effects processing, and instrument selection. These would need to be developed for SheetMusic to compete with established music creation tools.</p><h2 id="references">References</h2>
<p>Sarkar, A. (2016, September). <em>Towards spreadsheet tools for end-user music programming</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 228–231).</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Looking Where You Click: How Eye-Tracking Technology Can Replace Your Mouse for Data Analysis</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/looking-where-you-click-how-eye-tracking-technology-can-replace-your-mouse-for-data-analysis.html"/>
        <id>https://advaitsarkar.github.io/autoblog/looking-where-you-click-how-eye-tracking-technology-can-replace-your-mouse-for-data-analysis.html</id>

        <updated>2025-04-10T15:02:19+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article examines a study developing a gaze-directed lens tool for data visualization that allows users to interact with&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article examines a study developing a gaze-directed lens tool for data visualization that allows users to interact with graphs using only their eye movements. The research shows that eye-tracking interfaces can be as effective as traditional mouse controls while encouraging more focused exploration of key data points. </p><p><strong>Reference</strong>: Chander, A., &amp; Sarkar, A. (2016, September). <em>A gaze-directed lens for touchless analytics</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 232–241).</p><h2 id="eye-tracking-technology-turns-your-gaze-into-a-powerful-analytical-tool">Eye-tracking technology turns your gaze into a powerful analytical tool</h2>
<p>Imagine exploring complex data visualizations without ever touching a mouse or keyboard. Your eyes naturally move to interesting parts of a graph - what if the computer could follow your gaze and automatically zoom in on whatever captures your attention? This isn’t science fiction; it’s the reality of gaze-directed interfaces for data analysis.</p><p>The study developed a specialized lens tool that follows your eye movements to help explore data visualizations. The technology addresses situations where traditional input devices are impractical, such as “shared public display walls, where each individual user cannot be given mice and keyboards, and touchscreens cannot be implemented for reasons of cost and robustness; or operating theatres, where sterility is a primary concern.”</p><p>This approach treats data visualization as a form of “end-user programming” - where each interaction with the visualization tool produces a new representation of data. The researchers created specialized magnification and filtering lenses that respond to eye movements rather than mouse clicks.</p><h2 id="the-challenge-of-eye-tracking-is-knowing-when-youre-looking-versus-controlling">The challenge of eye tracking is knowing when you’re looking versus controlling</h2>
<p>A fundamental challenge in eye-tracking interfaces is what researchers call the “gaze multiplexing problem” or the “Midas touch problem.” The system must determine whether you’re simply looking at something or actively trying to interact with it.</p><p>“Eye tracking applications have to guess whether the user is reading, intends for a lens to move, or engage a selection, etc., and act accordingly,” the paper explains. “It is tricky to infer intent from eye movements alone; for instance, when trying to read a label placed along the edge of a gaze-driven lens, the user might inadvertently move the lens itself because the centre of their gaze has shifted.”</p><p>Previous solutions to this problem often relied on “temporal multiplexing” - waiting for a certain amount of time (dwell time) before triggering an action. But this approach feels clunky and unnatural. Others used multimodal inputs, combining eye tracking with other controls like head movements or keyboards, sacrificing the touchless nature of pure eye-tracking interfaces.</p><h2 id="the-flat-lens-design-solves-the-problem-of-accidental-interactions">The flat lens design solves the problem of accidental interactions</h2>
<p>To overcome these challenges, the researchers created a “flat lens” design with two concentric boxes. The inner box shows what part of the graph is being magnified, while a translucent outer box displays the magnified content. </p><p>This design solves the gaze multiplexing problem because both boxes share a common center: “With a common centre, gaze location signals the user’s unambiguous intent to magnify a region and inspect it, greatly alleviating the gaze multiplexing problem.”</p><p>The lens can perform different functions, such as magnification or selective labeling (only showing labels within the lens area to reduce clutter in dense visualizations). But the raw data from eye-tracking is extremely jittery - if the lens followed every tiny eye movement exactly, it would be unusable.</p><h2 id="dynamic-exponential-smoothing-makes-eye-movements-usable-as-input">Dynamic Exponential Smoothing makes eye movements usable as input</h2>
<p>Raw eye-tracking data is notoriously jumpy. Even when you think you’re staring at a fixed point, the recorded gaze coordinates can jump around by up to 50 pixels. Standard smoothing techniques like exponential averaging couldn’t balance stability with responsiveness - either the lens moved too slowly when you wanted to look somewhere else, or it jiggled too much when you tried to inspect something within the lens.</p><p>The solution was an algorithm called Dynamic Exponential Smoothing (DES), which adapts to the user’s behavior. When you’re examining something within the lens, tiny eye movements are smoothed out to keep the lens stable. But when you look at a completely different part of the graph, the algorithm detects the larger change and moves the lens more quickly.</p><p>“DES is based on the principle that large shifts in the quantity being controlled should cause the controller to accelerate faster than small shifts… but unlike [other control methods], DES is altered to better fit the specific requirements of gaze data smoothing,” the paper explains.</p><h2 id="people-were-just-as-fast-with-their-eyes-as-with-a-mouse">People were just as fast with their eyes as with a mouse</h2>
<p>To test their system, the researchers conducted an experiment with 11 undergraduate students who performed analytical tasks using both the eye-tracking lens and a traditional mouse-controlled lens. The tasks involved finding specific features on graphs, like peaks, troughs, and inflection points.</p><p>The results were promising: “Using log-normalised times, a non-inferiority test… gives a confidence level of 94.4%… This demonstrates that the gaze-directed interface is strictly not inferior in efficiency when compared to using a mouse.” In fact, participants completed tasks slightly faster with the eye-tracking interface, spending an average of 0.115 seconds less per question.</p><p>More importantly, participants spent more time investigating key regions of the graphs when using the eye-tracking interface: “Participants spent a median of 2.56s more time investigating key regions with the eye tracker than with the mouse.”</p><h2 id="eye-tracking-interfaces-encourage-more-focused-data-exploration">Eye-tracking interfaces encourage more focused data exploration</h2>
<p>A noteworthy finding was how the eye-tracking interface changed user behavior. When using the mouse, participants spent only about 35.2% of their time investigating the key regions of graphs. With the eye-tracking interface, this increased to 48%.</p><p>This suggests that eye-tracking interfaces may not just match mouse interfaces in efficiency - they might actually improve how people interact with data. By creating a more direct connection between what catches your attention and what gets magnified, the eye-tracking lens encourages more focused exploration.</p><p>The research has limitations worth noting. The sample size of only 11 participants was relatively small, and all were undergraduate students at the same university. The tasks were also fairly simple, involving finding specific features on graphs, rather than complex analytical work. More extensive testing with diverse user groups and more complex analytical tasks would be needed to fully validate the approach.</p><p>Nevertheless, the study demonstrates the potential of touchless, gaze-directed interfaces for data visualization and analysis. As data visualization tools become more widespread and computing extends beyond traditional desktop environments, such interfaces could make interactive data analysis more accessible in a variety of settings.</p><h2 id="references">References</h2>
<p>Chander, A., &amp; Sarkar, A. (2016, September). <em>A gaze-directed lens for touchless analytics</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 232–241).</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Untangling Data: How Data Noodles Transforms Spreadsheet Manipulation for Everyone</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/untangling-data-how-data-noodles-transforms-spreadsheet-manipulation-for-everyone.html"/>
        <id>https://advaitsarkar.github.io/autoblog/untangling-data-how-data-noodles-transforms-spreadsheet-manipulation-for-everyone.html</id>

        <updated>2025-04-10T14:50:58+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a 2016 paper that introduces “Data Noodles,” an approach to spreadsheet transformation&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a 2016 paper that introduces “Data Noodles,” an approach to spreadsheet transformation that makes data wrangling more accessible to non-technical users. The system allows users to reorganize spreadsheet data through a visual interface that synthesizes transformation programs based on examples rather than requiring manual programming. </p><p><strong>Reference</strong>: Gorinova, M. I., Sarkar, A., Blackwell, A. F., &amp; Prince, K. (2016, September). <em>Transforming spreadsheets with data noodles</em>. In <em>2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</em> (pp. 236–237). IEEE. <a href="https://doi.org/10.1109/VLHCC.2016.7739694">https://doi.org/10.1109/VLHCC.2016.7739694</a></p><h2 id="data-wrangling-is-a-common-challenge-that-affects-more-than-just-data-scientists">Data wrangling is a common challenge that affects more than just data scientists</h2>
<p>Data wrangling, the process of reorganizing data into a new structure before analysis begins, has traditionally been the domain of data scientists and programmers. It’s a crucial step in any data workflow but often requires technical expertise that many end-users lack.</p><p>Consider the hospital staff who need to extract patient records for analysis but don’t have programming skills. Or the marketing team wanting to restructure customer data without calling IT for help. Or the researcher who needs to reorganize experimental results but doesn’t know SQL.</p><p>“Customisable information systems often result in complex databases, from which users need to extract data in a simpler form for further analysis,” explains the paper. This challenge is universal across industries and disciplines.</p><p>The problem isn’t just technical - it’s conceptual. Traditional data transformation tools require users to understand and select specific transformations to apply in sequence. This puts the burden on users to know in advance what transformations exist and how they work.</p><h2 id="data-noodles-creates-a-visual-programming-language-anyone-can-understand">Data Noodles creates a visual programming language anyone can understand</h2>
<p>The Data Noodles system takes a fundamentally different approach. Rather than asking users what steps they want to take, it asks them what result they want to achieve.</p><p>The interface splits the screen into two parts: the original data on the left and the desired output format on the right. Users then create visual connections - the eponymous “noodles” - between input and output cells to demonstrate what they want.</p><p>“The user interacts with a split screen showing two spreadsheets – original input data on the left, and an output area on the right. The user first selects a range of cells on the left, producing a ‘noodle’ (hanging wire) that can be dragged to the right to specify the required transformation,” the paper describes.</p><p>These visual connections aren’t just static lines. They animate with physics-based movements, appearing to dangle between input and output cells like actual noodles. This playful design aims to make data manipulation more engaging for non-technical users.</p><p>Once users create a few example connections, the system automatically synthesizes a program that can transform the entire dataset following the pattern demonstrated in the example.</p><h2 id="programming-by-example-makes-complex-transformations-accessible-without-coding">Programming by example makes complex transformations accessible without coding</h2>
<p>The core technology behind Data Noodles is called programming by example (PBE), where users demonstrate desired behavior with concrete values rather than writing abstract code.</p><p>For instance, imagine a spreadsheet with columns for “Name,” “Year,” and “Papers” that lists how many academic papers a researcher published each year. A traditional organization might have multiple rows for each researcher - one for each year they published. With Data Noodles, users can easily transform this into a format with one row per researcher and separate columns for each year’s publication count.</p><p>To accomplish this transformation, users would:</p><ol>
<li>Create a noodle connecting names from input to output to preserve the researcher names</li>
<li>Create a noodle connecting year values to column headers in the output</li>
<li>Create a noodle connecting the paper counts to the appropriate cells in the grid</li>
</ol>
<p>“The example is now complete, and the user can request the rest of the output spreadsheet to be filled in automatically,” notes the paper. The system infers the pattern from these examples and applies it to the entire dataset.</p><p>This approach could be applied to many everyday scenarios outside academia. For instance, a small business owner could transform sales data from a daily transaction log (date, product, quantity sold) into a monthly view with products as rows and months as columns. Or a fitness enthusiast could convert daily workout records (date, exercise, repetitions) into a weekly summary showing progress for each exercise across different days.</p><p>This approach dramatically lowers the barrier to entry for data transformation tasks. Users don’t need to understand technical concepts like “fold” and “unfold” operations - they just need to demonstrate what they want.</p><h2 id="the-playful-interface-serves-a-serious-purpose-in-improving-user-engagement">The playful interface serves a serious purpose in improving user engagement</h2>
<p>What makes Data Noodles particularly interesting is its deliberate emphasis on creating an enjoyable user experience. The curved, animated noodles aren’t just a visual gimmick - they represent a thoughtful design decision to encourage user engagement.</p><p>“Although program synthesis by example is technically complex, and presents new usability challenges, previous research has not given explicit consideration to hedonic pleasure in the design of data management tools,” the paper points out. “The playful appearance (and indeed, the name) of Data Noodles is intended to draw in and engage users who might otherwise find data manipulation a dry and unengaging task.”</p><p>This approach acknowledges that technical capability alone isn’t enough. For a tool to be truly accessible, users must want to use it. The physics-based animation and catenary geometry (the natural curve formed by a hanging chain) provide both visual appeal and perceptual cues that help users understand the transformations.</p><h2 id="the-design-has-limitations-that-warrant-further-investigation">The design has limitations that warrant further investigation</h2>
<p>Despite its innovations, Data Noodles isn’t without potential drawbacks. The authors acknowledge several limitations that would need further study.</p><p>The curved noodle visualization, while engaging, might increase cognitive load compared to straight lines. Complex transformations requiring many noodles could potentially create a visual tangle that becomes difficult to interpret - an actual “noodle soup” of connections.</p><p>“Although complex transformations can be specified with only 5-10 noodles, how many can we include before creating an impossible-to-detangle noodle soup?” the authors question. They admit making “a deliberate trade-off of perceptual efficiency in favour of pleasurable engagement.”</p><p>The paper also doesn’t present formal user studies evaluating whether the noodle metaphor actually improves usability or user satisfaction compared to more traditional interfaces. Future work would need to validate whether the aesthetic benefits outweigh any potential confusion or scalability issues.</p><h2 id="references">References</h2>
<p>Gorinova, M. I., Sarkar, A., Blackwell, A. F., &amp; Prince, K. (2016, September). <em>Transforming spreadsheets with data noodles</em>. In <em>2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</em> (pp. 236–237). IEEE. <a href="https://doi.org/10.1109/VLHCC.2016.7739694">https://doi.org/10.1109/VLHCC.2016.7739694</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Teach Your Spreadsheet to Think: How BrainCel Makes Machine Learning More Accessible</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/teach-your-spreadsheet-to-think-how-braincel-makes-machine-learning-more-accessible.html"/>
        <id>https://advaitsarkar.github.io/autoblog/teach-your-spreadsheet-to-think-how-braincel-makes-machine-learning-more-accessible.html</id>

        <updated>2025-04-10T14:32:36+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down a research paper introducing BrainCel, an interactive visual system that makes machine learning accessible within&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down a research paper introducing BrainCel, an interactive visual system that makes machine learning accessible within ordinary spreadsheets. By combining multiple visualizations with a familiar spreadsheet interface, BrainCel helps non-experts build and understand machine learning models without specialized knowledge. The research presents both the design of the system and findings from a user study with non-experts who successfully completed machine learning tasks despite having no prior experience. </p><p><strong>Reference</strong>: Sarkar, A., Jamnik, M., Blackwell, A. F., &amp; Spott, M. (2015, October). <em>Interactive visual machine learning in spreadsheets</em>. In <em>2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</em> (pp. 159–163). IEEE. <a href="https://doi.org/10.1109/VLHCC.2015.7357211">https://doi.org/10.1109/VLHCC.2015.7357211</a></p><h2 id="spreadsheets-can-learn-from-your-data-without-you-being-a-data-scientist">Spreadsheets Can Learn From Your Data Without You Being a Data Scientist</h2>
<p>Remember the last time you laboriously sorted through a spreadsheet, manually categorizing items or filling in missing values? Imagine if your spreadsheet could learn from the examples you provide and automatically complete the rest. That’s exactly what BrainCel offers, bringing machine learning capabilities into the familiar grid-based interface used by millions.</p><p>Machine learning (algorithms that learn patterns from data) has traditionally been the domain of specialists with advanced technical knowledge. Current tools like WEKA or scikit-learn require significant expertise, placing them far beyond the reach of everyday users. As the paper notes, these tools are “designed for professional statisticians or computer scientists, and are conceptually complex compared to end-user data manipulation tools, such as spreadsheets.”</p><p>Yet machine learning increasingly touches our daily lives. From email spam filters to Netflix recommendations, we regularly interact with learning systems without needing to understand the complex statistics behind them. The problem is that these applications are domain-specific, letting users train models only for predetermined tasks like filtering emails or rating movies.</p><p>BrainCel bridges this gap by bringing general-purpose machine learning into spreadsheets, an environment already familiar to millions of non-technical users. Rather than requiring users to write code or understand complex statistical concepts, BrainCel lets them teach by example.</p><h2 id="users-select-good-examples-and-the-system-learns-from-them">Users Select Good Examples and the System Learns From Them</h2>
<p>The core interaction in BrainCel is simple. Users select rows of data they believe are correct examples, press “Learn,” and the system builds a model from these examples. To make predictions, users select empty cells and click “Guess,” prompting the system to intelligently fill them based on patterns learned from the training data.</p><p>Under the hood, BrainCel uses the k-nearest neighbors algorithm (k-NN), which makes predictions based on the most similar examples in the training data. While this approach may seem simplistic to machine learning experts, its behavior is considerably more transparent to novices than more complex algorithms.</p><p>The value of BrainCel isn’t in the complexity of its underlying algorithm, but in how it visualizes the model’s “thinking” process to help users understand what’s happening.</p><h2 id="color-coding-shows-where-the-model-is-confident-or-confused">Color Coding Shows Where the Model Is Confident or Confused</h2>
<p>One of BrainCel’s features is its color-based confidence visualization. Each row in the spreadsheet is colored on a red-green scale (with varying lightness to accommodate color blindness), where red indicates low confidence and green indicates high confidence.</p><p>“Through colour, the overview summarizes confidence over the entire spreadsheet; green areas are well-modelled by the training data, whereas red areas are dissimilar to their nearest neighbours in the training set, marking them out either as being inadequately represented in the training set or as outliers.”</p><p>This visual guidance subtly “nudges” users toward adding the most informative examples to their training data, improving the model more efficiently than if they selected examples at random.</p><h2 id="a-network-visualization-shows-how-the-model-makes-decisions">A Network Visualization Shows How the Model Makes Decisions</h2>
<p>Another aspect of BrainCel is its network visualization, which illustrates how the k-NN algorithm works. Each row in the spreadsheet appears as a node in the network, with lines connecting it to its nearest neighbors in the training set. The length of each line represents the distance (or dissimilarity) between rows.</p><p>This visualization serves multiple purposes:</p><ul>
<li>It explains why a particular prediction was made by showing which training examples influenced it</li>
<li>It shows how predictions are made in general by visualizing the overall structure of the model</li>
<li>It reveals natural clusters in the data as the model evolves</li>
</ul>
<p>For example, when working with the classic Iris dataset (which contains measurements of three different species of iris flowers), the network initially shows all data as a single cluster. As the user adds examples from different species, the visualization branches into distinct clusters representing each species.</p><h2 id="statistics-charts-help-users-build-representative-models">Statistics Charts Help Users Build Representative Models</h2>
<p>BrainCel also provides charts comparing the distribution of values in the training data versus the overall dataset. These visualizations help users identify whether certain types of data are under- or over-represented in their training examples.</p><p>For instance, if users have added many examples of “Iris-setosa” but few examples of “Iris-versicolor” to their training set, the distribution chart would highlight this imbalance, prompting them to add more diverse examples.</p><p>Additional line graphs track the model’s evolution over time, showing how the user’s actions affect both the representativeness of the training data and the overall confidence of the model.</p><h2 id="non-experts-built-working-models-in-under-45-minutes">Non-Experts Built Working Models in Under 45 Minutes</h2>
<p>To test BrainCel’s effectiveness, a study was conducted with participants from humanities departments who had no prior exposure to statistics or machine learning. These participants were given spreadsheets with missing values and asked to fill them using BrainCel.</p><p>The results were encouraging: “All [participants] successfully completed both tasks (populated the spreadsheets with correct values) in under 45 minutes.”</p><p>The study also revealed how BrainCel helped users overcome learning barriers. When using the system without visualizations, users encountered basic obstacles in selecting and coordinating their actions. With visualizations enabled, they moved beyond these basic issues to more sophisticated questions about how the model was working.</p><p>As one participant demonstrated: “It’s gotten that correct, but why is it still not confident about it?” This shows how the visualizations helped users “extend the zone of proximal development past the simpler concepts of the spreadsheet training paradigm to more sophisticated conceptual issues regarding critical assessment of the model.”</p><h2 id="limitations-suggest-opportunities-for-future-development">Limitations Suggest Opportunities for Future Development</h2>
<p>Despite its innovative approach, BrainCel has several limitations. The system currently uses a basic implementation of the k-NN algorithm, without allowing users to adjust feature weights or other parameters. Additionally, while k-NN provides relatively transparent explanations, more sophisticated algorithms might offer better performance for complex tasks.</p><p>The small sample size of the user study also limits the statistical significance of the findings, though the qualitative insights remain valuable.</p><p>Furthermore, the research focused on explaining k-NN specifically, leaving open the question of how other machine learning algorithms might be visualized effectively within a spreadsheet interface.</p><h2 id="machine-learning-for-the-rest-of-us-within-reach">Machine Learning for the Rest of Us Within Reach</h2>
<p>BrainCel represents a step toward democratizing machine learning by bringing it into a familiar environment that millions already use daily. As we increasingly live in a world shaped by algorithmic decisions, tools that make these algorithms accessible and understandable to non-experts become increasingly valuable.</p><p>The research demonstrates that with thoughtful interface design and appropriate visualizations, even complete novices can successfully build and understand machine learning models. This suggests a future where spreadsheet users might routinely incorporate machine learning into their work without needing to consult data scientists or learn programming.</p><p>For those looking to explore this intersection of spreadsheets and machine learning, BrainCel offers a glimpse of what’s possible when sophisticated technology is wrapped in an interface that prioritizes understanding alongside functionality.</p><h2 id="references">References</h2>
<p>Sarkar, A., Jamnik, M., Blackwell, A. F., &amp; Spott, M. (2015, October). <em>Interactive visual machine learning in spreadsheets</em>. In <em>2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</em> (pp. 159–163). IEEE. <a href="https://doi.org/10.1109/VLHCC.2015.7357211">https://doi.org/10.1109/VLHCC.2015.7357211</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Opening Machine Learning to Everyone Who Can Use Spreadsheets</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/opening-machine-learning-to-everyone-who-can-use-spreadsheets.html"/>
        <id>https://advaitsarkar.github.io/autoblog/opening-machine-learning-to-everyone-who-can-use-spreadsheets.html</id>

        <updated>2025-04-10T14:15:30+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down research on making machine learning accessible through spreadsheet interfaces, allowing non-experts to use powerful statistical&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down research on making machine learning accessible through spreadsheet interfaces, allowing non-experts to use powerful statistical models without specialized training. The paper presents approaches that leverage familiar spreadsheet environments to democratize access to machine learning tools, potentially closing digital divides in data analytics.</p><p><strong>Reference</strong>: Sarkar, A. (2015, October). <em>Spreadsheet interfaces for usable machine learning</em>. In <em>2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</em> (pp. 283–284). IEEE. <a href="https://doi.org/10.1109/VLHCC.2015.7357228">https://doi.org/10.1109/VLHCC.2015.7357228</a></p><h2 id="data-science-is-currently-reserved-for-those-with-specialized-training">Data Science Is Currently Reserved for Those with Specialized Training</h2>
<p>In our data-rich world, professionals across countless industries have potentially valuable datasets sitting idle on their computers. A retail manager might have years of sales data that could predict future trends. A teacher might have student performance metrics that could identify which pupils need extra support. A sports coach might have player statistics that could optimize team strategies.</p><p>Yet most of these professionals lack the tools to unlock these insights.</p><p>“The ability to build and apply powerful statistical models (e.g. neural networks, linear regression, and decision trees), is currently only available to users with expertise in both programming as well as statistics and machine learning,” notes the research paper. This creates what the paper calls “new digital divides,” where only those with specialized skills can participate in the “ongoing analytics revolution.”</p><p>The solution might be hiding in plain sight: the humble spreadsheet.</p><h2 id="spreadsheets-are-the-worlds-most-popular-programming-environment">Spreadsheets Are the World’s Most Popular Programming Environment</h2>
<p>When people think of programming, they often imagine darkened rooms filled with developers typing cryptic code. But the world’s most widely used programming environment isn’t Python or Java - it’s Microsoft Excel and Google Sheets.</p><p>Spreadsheets are programming environments disguised as office productivity tools. They allow users to define relationships between data, create formulas, and build complex data structures, all without writing a single line of traditional code. This makes spreadsheets an ideal gateway for introducing sophisticated machine learning capabilities to non-experts.</p><p>“Since end-users are likely to be readily familiar with the manipulation of data in spreadsheets, integrating statistical modelling into the spreadsheet environment appears to be a promising avenue,” the paper suggests.</p><h2 id="teaching-spreadsheets-to-learn-from-data">Teaching Spreadsheets to Learn from Data</h2>
<p>The researchers developed prototype systems that embed machine learning into the familiar spreadsheet interface. One system, called “Teach and Try,” demonstrated how simple spreadsheet selections could be used to mark training and testing data - a fundamental concept in machine learning.</p><p>“Teach and Try demonstrated how using spreadsheet selections to demarcate training and testing data is a quick-to-learn interface technique which causes end-users to develop an appreciation for statistical inference and its limitations,” the paper reports.</p><p>A more recent prototype called “BrainCel” went further by visualizing the machine learning model itself, helping users understand how the system makes predictions. Early testing showed that “this interface enabled end-users with no statistical training to build k-nearest neighbours (k-NN) models on various datasets, and to apply these models to make predictions for incomplete data.” K-nearest neighbors is a simple but powerful machine learning algorithm that makes predictions based on similar examples in the training data.</p><h2 id="commercial-products-are-following-academic-research">Commercial Products Are Following Academic Research</h2>
<p>The technology industry has recognized the potential of bringing machine learning to spreadsheets. Microsoft introduced a feature called “flash fill” that automatically completes patterns the user starts typing - essentially a form of programming by example. Google added “smart autofill” to its spreadsheet products.</p><h2 id="the-vision-spreadsheets-that-make-predictions-without-programming">The Vision: Spreadsheets That Make Predictions Without Programming</h2>
<p>Imagine a small business owner who wants to predict which customers are likely to cancel their subscriptions. With spreadsheet-based machine learning, they could:</p><ol>
<li>Highlight existing customer data (including those who have cancelled) as “training data”</li>
<li>Click a button to build a model</li>
<li>Run the model to see which current customers might be at risk</li>
</ol>
<p>No coding. No statistics degree. Just using familiar spreadsheet interactions to harness the power of machine learning.</p><p>The paper cautions that this technology is still developing, with several challenges ahead: “Three important issues remain to be addressed,” including how to visualize different types of machine learning models beyond k-NN, finding optimal interaction patterns for users, and developing better evaluation frameworks.</p><h2 id="democratizing-data-science-faces-legitimate-challenges">Democratizing Data Science Faces Legitimate Challenges</h2>
<p>While the vision of bringing machine learning to everyone is appealing, the paper acknowledges important limitations. Creating effective data scientists “requires years of higher education to lay the necessary foundational knowledge of mathematics and computation, and then further years of experience in the domain of the data they analyse - no amount of clever interface design can overcome these requirements.”</p><p>There are also risks in putting powerful prediction tools in the hands of users who may not understand their limitations or potential biases.</p><p>The researchers are clear about their goals: “We are not aiming to replace the entire statistical profession, but rather intending that through our tools, a wider range of end-users will be able to address data-related issues which they previously could not.”</p><h2 id="the-spreadsheet-revolution-is-just-beginning">The Spreadsheet Revolution Is Just Beginning</h2>
<p>The next time a spreadsheet automatically completes a pattern for you or suggests a formula, remember that you’re witnessing the early stages of a revolution in how ordinary people interact with machine learning.</p><p>As interfaces improve and algorithms become more accessible, the power to derive insights from data will extend beyond specialists with advanced degrees. The familiar grid of rows and columns that has organized our data for decades may be the gateway to sophisticated predictive analytics for everyone.</p><h2 id="references">References</h2>
<p>Sarkar, A. (2015, October). <em>Spreadsheet interfaces for usable machine learning</em>. In <em>2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</em> (pp. 283–284). IEEE. <a href="https://doi.org/10.1109/VLHCC.2015.7357228">https://doi.org/10.1109/VLHCC.2015.7357228</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>When Machines Need to Explain Themselves: The Role of Metamodels in Human-AI Interaction</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/when-machines-need-to-explain-themselves-the-role-of-metamodels-in-human-ai-interaction.html"/>
        <id>https://advaitsarkar.github.io/autoblog/when-machines-need-to-explain-themselves-the-role-of-metamodels-in-human-ai-interaction.html</id>

        <updated>2025-04-10T14:05:11+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down a research paper that proposes a new framework for human-machine interaction involving artificial intelligence. The&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down a research paper that proposes a new framework for human-machine interaction involving artificial intelligence. The paper suggests that as AI systems become more complex and probabilistic, we need new ways to understand how they “think” - using “metamodels” that track a system’s confidence, command of a domain, and decision complexity.</p><p><strong>Reference</strong>: Sarkar, A. (2015, July). Confidence, command, complexity: Metamodels for structured interaction with machine intelligence. In Proceedings of the 26th Annual Conference of the Psychology of Programming Interest Group (PPIG 2015) (pp. 23–36).</p><h2 id="our-relationship-with-ai-is-shifting-from-programming-to-dialogue">Our Relationship with AI Is Shifting from Programming to Dialogue</h2>
<p>Think about how you interact with Spotify’s recommendations, your thermostat that learns your schedule, or your email’s spam filter. You’re not exactly programming these systems in the traditional sense. Instead, you’re having an ongoing conversation with them - feeding them examples, reacting to their decisions, nudging them in the right direction when they’re wrong.</p><p>This represents a change in how humans and machines communicate.</p><p>“We increasingly inhabit an inferred world,” the paper explains, “and the outcome of computer algorithms is becoming predominantly probabilistic and data-dependent, rather than deterministic.”</p><p>Traditional programming is straightforward: you write precise instructions, and the computer follows them. But machine learning doesn’t work that way. When an AI system makes a recommendation or prediction you don’t like, you can’t simply look at the code to understand what went wrong. The decision-making process is buried in complex statistical models and millions of parameters that even experts struggle to interpret.</p><p>The paper describes this as a paradigm shift from asking “are you thinking what I’m thinking?” to wondering “what are we thinking?”</p><h2 id="ai-systems-need-metacognition-to-have-better-conversations-with-humans">AI Systems Need Metacognition to Have Better Conversations with Humans</h2>
<p>Humans have metacognition - the ability to think about our thinking. We know when we’re confident in a decision versus when we’re guessing. We recognize when we’re in familiar territory versus when we’re out of our depth. We understand when a problem requires simple versus complex reasoning.</p><p>The paper argues that AI systems need similar abilities if they’re going to have meaningful dialogues with us.</p><p>“By explicitly acknowledging the interaction as dialogue, we are able to take a structured approach, which is descriptive as well as prescriptive,” the author notes.</p><p>This isn’t just about making AI more understandable. It’s about developing a richer interaction where both parties can critically evaluate their understanding and make progress together.</p><h2 id="three-key-metamodels-could-transform-how-we-interact-with-machine-learning">Three Key Metamodels Could Transform How We Interact with Machine Learning</h2>
<p>The paper proposes three fundamental “metamodels” that could give AI systems a form of metacognition:</p><ol>
<li><strong>Confidence</strong>: How sure is the system about its output? For example, when Netflix recommends a movie, how confident is it that you’ll actually like it?</li>
</ol>
<p>“The confidence of a decision tree in a given output can be measured as the cumulative information gain from the root to the outputted leaf node,” the paper suggests as one technical implementation.</p><ol start="2">
<li><strong>Command</strong>: How well does the system know the domain? Has it seen enough examples across the full range of possibilities to make reliable judgments?</li>
</ol>
<p>The paper explains this concept by contrasting two views: “If we view command as some integral of confidence, then an algorithm with high levels of confidence in the majority of the domain can be considered to have a good command of the domain. If we view command as some integral of the occurrences of training examples encountered, then an algorithm which has received a uniform spread of training examples may be considered to have a good command of the domain.”</p><ol start="3">
<li><strong>Complexity</strong>: Did the system use simple or complex reasoning to arrive at its conclusion? This helps identify when systems might be taking problematic shortcuts.</li>
</ol>
<p>“Deep Blue may have astonished with its famous defeat of Kasparov in 1997, but it did so not because it was following a complex algorithm; far from it. It did so because the input space and the domain carried with it considerable complexity,” the paper notes.</p><h2 id="real-world-applications-of-metamodels-would-make-ai-more-transparent-and-trustworthy">Real-World Applications of Metamodels Would Make AI More Transparent and Trustworthy</h2>
<p>Imagine an email spam filter that not only categorizes messages but shows you how confident it is in each decision, identifies types of emails it hasn’t seen much of before, and flags when it’s using unexpectedly complex or simple reasoning.</p><p>Or consider a self-driving car that could explain when it’s in unfamiliar territory or when it’s less confident in its decisions - perhaps even knowing when to defer to a human driver.</p><p>The paper outlines several practical applications, including image recognition systems that could show users not just their classifications but their confidence levels, helping users know where to provide additional training examples.</p><p>“Through metamodels of confidence, a driverless car might be able to identify situations where it defers to the judgment of a human driver,” the paper suggests. “Similarly, through metamodels of command, the car might be able to identify road and scenery types which it had not previously encountered, and alert the driver to this.”</p><h2 id="the-limitations-of-metamodels-still-need-to-be-addressed">The Limitations of Metamodels Still Need to Be Addressed</h2>
<p>While the paper presents an intriguing framework, several limitations should be noted. The proposed metamodels are still conceptual rather than fully implemented systems. The paper acknowledges that calculating accurate metamodel values for complex AI systems remains challenging.</p><p>Additionally, the paper doesn’t address how these metamodels should be visualized or communicated to non-expert users in intuitive ways. Simply providing confidence scores or complexity metrics might overwhelm rather than enlighten average users.</p><p>There’s also the risk that metamodels could create a false sense of security. A system might be very confident yet completely wrong - as the paper notes regarding image classifiers that can be fooled by carefully crafted inputs.</p><p>“Recent work has demonstrated how some apparently straightforward images with carefully injected noise, as well as completely unrecognisable images, are still classified with high confidence by a state-of-the-art image classifier,” the paper cautions.</p><h2 id="the-future-of-human-ai-interaction-depends-on-better-communication">The Future of Human-AI Interaction Depends on Better Communication</h2>
<p>As AI systems become more integrated into our daily lives, the need for better communication between humans and machines grows more urgent. Traditional debugging approaches won’t work when we’re training systems through our behaviors rather than explicit programming.</p><p>The metamodel approach provides a framework for thinking about this challenge. By giving machines the ability to communicate their uncertainty, knowledge boundaries, and reasoning complexity, we might create more transparent, trustworthy, and ultimately useful AI systems.</p><p>The world is rapidly shifting from one where humans give explicit instructions to machines to one where humans and machines engage in ongoing dialogue. As this paper suggests, making that dialogue richer and more meaningful may require giving our AI systems something akin to metacognition - an awareness of their own thinking processes.</p><h2 id="references">References</h2>
<p>Sarkar, A. (2015, July). Confidence, command, complexity: Metamodels for structured interaction with machine intelligence. In Proceedings of the 26th Annual Conference of the Psychology of Programming Interest Group (PPIG 2015) (pp. 23–36).</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Colorful Code Makes Programmers&#x27; Lives Easier: Evidence from Eye-Tracking Research</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/colorful-code-makes-programmers-lives-easier-evidence-from-eye-tracking-research.html"/>
        <id>https://advaitsarkar.github.io/autoblog/colorful-code-makes-programmers-lives-easier-evidence-from-eye-tracking-research.html</id>

        <updated>2025-04-08T10:53:47+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article examines the impact of syntax highlighting on program comprehension, breaking down the results of an empirical study&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article examines the impact of syntax highlighting on program comprehension, breaking down the results of an empirical study published in 2015. Using eye-tracking technology and controlled experiments, researchers discovered that syntax highlighting significantly improves code comprehension speed, with a more pronounced effect among novice programmers. The findings suggest that visual cues in programming environments may reduce cognitive load and improve programming efficiency.</p><p><strong>Reference:</strong> Sarkar, A. (2015). The impact of syntax colouring on program comprehension. In Proceedings of the 26th Annual Conference of the Psychology of Programming Interest Group (PPIG 2015) (pp. 49–58).</p><h2 id="colorful-code-what-is-syntax-highlighting">Colorful Code: What is Syntax Highlighting?</h2>
<p>Most programmers wouldn’t dream of writing code without their editor’s familiar kaleidoscope of colors illuminating different parts of their syntax. For instance, keywords might be colored in bold blue, strings in gentle green, comments in muted gray. This coloring is so deeply ingrained in modern programming that many developers would feel lost without it. But while the value of such coloring is intuitively understood by programmers, there has not been a lot of scientific evidence that syntax highlighting actually helps programmers understand code more efficiently.</p><p>“Syntax colouring, commonly known as syntax highlighting, is a feature of some text editors which colours lexical tokens in source code text according to a certain categorisation,” explains the research paper published by the University of Cambridge. It’s essentially a visual aid that makes different parts of program code easier to distinguish, like coloring different grammatical elements in sentences.</p><h2 id="syntax-highlighting-significantly-speeds-up-code-comprehension-tasks">Syntax Highlighting Significantly Speeds Up Code Comprehension Tasks</h2>
<p>The research team conducted a carefully designed experiment using eye-tracking technology to precisely measure how programmers interact with code. Ten graduate computer science students with varying levels of programming experience were asked to mentally execute Python functions and determine their outputs. Each participant completed pairs of nearly identical programming tasks - one with syntax highlighting and one without.</p><p>The results were clear and statistically significant: syntax highlighting reduced task completion time by a median of 8.4 seconds. That might not sound dramatic for a single small code snippet, but consider how those seconds accumulate over thousands of interactions with code throughout a programmer’s day.</p><p>“Task completion times for highlighted versions of the tasks were significantly lower,” the study reports. This finding confirms what many programmers have intuitively felt but couldn’t necessarily prove - that colorful code is genuinely easier to work with.</p><h2 id="more-experienced-programmers-benefit-less-from-syntax-coloring">More Experienced Programmers Benefit Less From Syntax Coloring</h2>
<p>Interestingly, the research uncovered an inverse relationship between programming experience and the benefits of syntax highlighting. The researchers found “that programming experience was negatively correlated with time advantage. It appears that syntax highlighting improves program comprehension speed to a greater extent in novice programmers than in experienced programmers.”</p><p>This suggests that as programmers gain expertise, they develop mental models and pattern recognition abilities that partially compensate for the lack of visual cues. However, the researchers note this effect might be related to the brevity of the experimental tasks, suggesting that “repeating the study with longer programs may reveal that experienced programmers stand to gain just as much as less experienced programmers.”</p><h2 id="how-visual-attention-reveals-the-mental-process-of-programming">How Visual Attention Reveals The Mental Process of Programming</h2>
<p>Perhaps the most intriguing findings came from analyzing where programmers were looking when reading code. The eye-tracking data revealed that syntax highlighting significantly reduced what researchers called “context switches” - instances where programmers had to look back at the input values while working through the code.</p><p>“Our results show that the need to be reminded of the input values was significantly greater when the code was not highlighted,” the paper states. This observation provides a window into the mental processes happening during code comprehension.</p><p>The researchers propose an explanation: “it is plausible that the mental overhead required to process and understand plain code is greater than the mental overhead required to process highlighted code, since highlighted code contains additional semantic richness by virtue of the colours of the tokens.”</p><h2 id="syntax-coloring-may-actually-free-up-mental-resources-during-programming">Syntax Coloring May Actually Free Up Mental Resources During Programming</h2>
<p>When programming without syntax highlighting, participants needed to work harder to track what each element in the code was doing. This additional mental burden appears to push other important information (like input values) out of working memory, forcing programmers to constantly check back to remind themselves of these details.</p><p>The research team hypothesizes “that syntax highlighting improves the ability of the programmer to mentally retain the state of the execution, and that highlighted code incurs a lower mental comprehension overhead.”</p><p>In some particularly striking cases, the eye-tracking data suggested programmers could almost ignore highlighted keywords entirely - as if their brain could process them peripherally without direct focus. “In some cases, the eye-tracking data suggested that the participants were able to ignore highlighted keywords entirely, as though perceiving them peripherally was enough to incorporate their semantics into the computation.”</p><h2 id="programming-editors-should-continue-to-prioritize-visual-comprehension-aids">Programming Editors Should Continue to Prioritize Visual Comprehension Aids</h2>
<p>While this study focused specifically on syntax highlighting, it points to broader implications about how visual presentation affects our ability to comprehend complex information. The findings suggest that programming environments should continue to evolve with visual aids that reduce cognitive load and make code easier to understand.</p><p>For novice programmers and educators, these findings are particularly relevant. Making programming more visually accessible through syntax highlighting and other visual cues may significantly flatten the learning curve for beginners struggling to grasp programming concepts.</p><p>The next time you open your code editor and see that familiar splash of colors, you can appreciate that those highlights aren’t just pretty decoration. They’re actually helping your brain process information more efficiently - a small but meaningful boost to your programming productivity.</p><h2 id="references">References</h2>
<p>Sarkar, A. (2015). The impact of syntax colouring on program comprehension. In Proceedings of the 26th Annual Conference of the Psychology of Programming Interest Group (PPIG 2015) (pp. 49–58).</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Uncertainty in Data Visualization Can Be Controlled with Draggable Error Bars</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/uncertainty-in-data-visualization-can-be-controlled-with-draggable-error-bars.html"/>
        <id>https://advaitsarkar.github.io/autoblog/uncertainty-in-data-visualization-can-be-controlled-with-draggable-error-bars.html</id>

        <updated>2025-04-08T10:41:40+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article explains how a new direct-manipulation interface allows users to control uncertainty in data visualizations through draggable error&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article explains how a new direct-manipulation interface allows users to control uncertainty in data visualizations through draggable error bars. Breaking down the results from a 2015 paper, it demonstrates how this intuitive interface enables users to make informed decisions about the trade-offs between accuracy and computational resources. The research shows that users can spontaneously discover and correctly use the interface, with their behavior adapting based on computation time constraints.</p><p><strong>Reference:</strong> Sarkar, A., Blackwell, A. F., Jamnik, M., &amp; Spott, M. (2015). Interaction with uncertainty in visualisations. In Proceedings of the 17th Eurographics/IEEE VGTC Conference on Visualization (EuroVis 2015). <a href="https://doi.org/10.2312/eurovisshort.20151138">https://doi.org/10.2312/eurovisshort.20151138</a></p><h2 id="why-uncertainty-in-data-visualization-matters">Why uncertainty in data visualization matters</h2>
<p>When working with massive datasets, a critical challenge emerges: how can analysts interact with data that’s simply too large to process completely in real time? A clever solution has been developing in recent years - approximate computation techniques that provide fast results with quantifiable error bounds. These techniques are gaining traction, but they introduce a new problem: how do users interact with and understand the uncertainty these approximations create?</p><p>“With the emergence of approximate computation techniques, we now have sources of uncertainty that make a trade-off between accuracy and time/space resources,” notes the paper from the 2015 Eurographics Conference on Visualization. This represents a fundamental shift in how we think about uncertainty in data visualization.</p><p>Traditionally, uncertainty has been treated as a fixed property of data - something inherent that we simply need to represent. But with new approximate computation techniques, uncertainty becomes something that can be manipulated and controlled, just like any other aspect of a visualization.</p><h2 id="draggable-error-bars-offer-an-intuitive-way-to-control-uncertainty">Draggable error bars offer an intuitive way to control uncertainty</h2>
<p>The paper introduces a novel solution: draggable error bars. This direct-manipulation interface allows users to literally grab and adjust the level of uncertainty they’re willing to accept in a visualization.</p><p>The interface is simple. Error bars represent the uncertainty in data points, and users can drag these bars to reduce uncertainty. When dragging, a horizontal indicator appears, showing the estimated time required for recomputation. This “resource cost estimation bar” is crucial - it helps users decide whether the increased accuracy is worth the computational cost.</p><p>“When dragging, a horizontal indicator bar appears, visualising the estimated duration of recomputation,” the paper explains. “This ‘resource cost estimation bar’ is an essential component of the interface, as it allows the user to judge whether they are willing to invest their resources (in this case, time) in exchange for an improvement in accuracy.”</p><p>Upon releasing the mouse button, the recomputation begins. The cost estimation indicator shrinks as the computation progresses, and the data point and error bars move to their newly accurate positions.</p><p>This interface can be applied to various types of charts, including scatterplots, bar charts, and line graphs. It works with multiple approximate computation techniques like sampling, sketching, and online aggregation.</p><h2 id="people-can-spontaneously-discover-how-to-use-the-interface">People can spontaneously discover how to use the interface</h2>
<p>But does this interface actually work for users? To find out, the researchers conducted a web-based study with 39 participants.</p><p>The results were encouraging. When asked to reduce the uncertainty associated with a point on a plot, 87% of participants successfully discovered the drag operation without any instructions. The median discovery time was just 19.3 seconds. This suggests the interface aligns well with users’ intuitive expectations about how to manipulate uncertainty.</p><p>“From this we conclude that the interface corresponds well with users’ prior assumptions about how one might manipulate uncertainty in a visualisation, i.e., it is intuitive,” states the paper.</p><h2 id="users-make-strategic-decisions-about-uncertainty-based-on-computation-time">Users make strategic decisions about uncertainty based on computation time</h2>
<p>After being given a full explanation of the interface, participants were asked to perform 30 comparison tasks, each involving two data points with error bars. Although it was always possible to give a reasonable interpretation without interacting with the error bars, 77% of participants chose to use drag operations.</p><p>Some participants (13 out of 39) used drag operations in all 30 tasks, averaging 2.53 drags per task. On average, they reduced uncertainty by 67.6% per drag. This demonstrates that participants found value in manipulating uncertainty rather than just accepting it as a fixed property.</p><p>What’s particularly interesting is how participants adapted their behavior based on recomputation time. There was a clear negative correlation between maximum recompute time and uncertainty reduced per drag per task. In other words, when recomputation was expensive (time-consuming), participants preferred to make multiple small adjustments rather than one large one.</p><p>“This corresponds to behaviour observed in our pilot studies where participants preferred to make multiple small drags if recomputation was expensive,” the paper notes.</p><h2 id="the-future-of-interactive-uncertainty-visualization-looks-promising">The future of interactive uncertainty visualization looks promising</h2>
<p>The implications of this research extend beyond just this specific interface. As approximate computation becomes more common in data analysis, interfaces that allow direct manipulation of uncertainty will become increasingly important.</p><p>This research demonstrates that users can understand and effectively use such interfaces. They can make informed decisions about the trade-offs between accuracy and computational resources, adapting their behavior based on these trade-offs.</p><p>The study shows a path forward for interactive visualization of large datasets. Instead of treating uncertainty as an undesirable but unavoidable aspect of data visualization, we can embrace it as a parameter that users can control.</p><p>“Our study provides evidence that the representation of required computational resources modulates interaction as intended, allowing skilled users to modify their usage to achieve the resource/accuracy tradeoff that suits their needs,” concludes the paper.</p><p>For data scientists and analysts working with increasingly large datasets, this approach offers a promising new way to interact with data. Rather than waiting for complete, exact computations or accepting highly uncertain approximations, users can actively participate in deciding what level of uncertainty is acceptable for their specific needs.</p><p>This represents a shift from passive consumption of visualizations to active engagement with the underlying computational processes - a shift that could make large-scale data analysis more accessible and effective.</p><h2 id="references">References</h2>
<p>Sarkar, A., Blackwell, A. F., Jamnik, M., &amp; Spott, M. (2015). Interaction with uncertainty in visualisations. In Proceedings of the 17th Eurographics/IEEE VGTC Conference on Visualization (EuroVis 2015). <a href="https://doi.org/10.2312/eurovisshort.20151138">https://doi.org/10.2312/eurovisshort.20151138</a></p>
            ]]>
        </content>
    </entry>
</feed>
