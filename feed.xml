<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>Talking about papers</title>
    <link href="https://advaitsarkar.github.io/autoblog/feed.xml" rel="self" />
    <link href="https://advaitsarkar.github.io/autoblog" />
    <updated>2025-04-14T11:19:47+01:00</updated>
    <author>
        <name>TAP Communications</name>
    </author>
    <id>https://advaitsarkar.github.io/autoblog</id>

    <entry>
        <title>Dual View Spreadsheets Could Help You Make Fewer Errors and Work Faster: Learn About Calculation View</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/dual-view-spreadsheets-could-help-you-make-fewer-errors-and-work-faster.html"/>
        <id>https://advaitsarkar.github.io/autoblog/dual-view-spreadsheets-could-help-you-make-fewer-errors-and-work-faster.html</id>

        <updated>2025-04-14T11:19:12+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a scientific paper that explores how adding a textual programming interface to&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a scientific paper that explores how adding a textual programming interface to spreadsheets can reduce errors and increase efficiency. The researchers designed a feature called “Calculation View” that displays formulas in a text-based format alongside the traditional grid, allowing users to work with spreadsheets at a higher level of abstraction. The study found that this approach significantly reduced task completion time and cognitive load without negatively affecting user confidence.</p><p>Reference: Sarkar, A., Gordon, A. D., Jones, S. P., &amp; Toronto, N. (2018, October). Calculation View: Multiple-representation editing in spreadsheets. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 85–93). IEEE. <a href="https://doi.org/10.1109/VLHCC.2018.8506584">https://doi.org/10.1109/VLHCC.2018.8506584</a></p><h2 id="why-spreadsheets-are-both-wonderful-and-difficult">Why Spreadsheets Are Both Wonderful and Difficult</h2>
<p>Spreadsheets are ubiquitous in modern workplaces. They’re flexible, powerful, and accessible to almost anyone with a computer. But they can also be error-prone.</p><p>The problem is fundamental to how spreadsheets work. “Spreadsheets excel at showing data, while hiding computation,” the paper notes. When you look at a spreadsheet, you see numbers and charts, but the formulas that generate those results are hidden from view. To see how a calculation works, you need to click on each cell individually and examine its formula in a tiny box at the top of the screen.</p><p>This invisibility of computational structure makes spreadsheets challenging to understand, debug, and maintain. And these aren’t just minor inconveniences. Spreadsheet errors have real-world consequences, from financial miscalculations to flawed research findings.</p><h2 id="the-hidden-world-of-spreadsheet-programming">The Hidden World of Spreadsheet Programming</h2>
<p>Most people don’t think of themselves as programmers when they use Excel. But they are.</p><p>Every time someone writes a formula like <code>&quot;=SUM(A1:A10)&quot;</code> or <code>&quot;=B2*0.15&quot;</code>, they’re creating code. The problem is that this code is scattered across potentially thousands of cells, with no easy way to view it all at once or understand the relationships between formulas.</p><p>Consider a common spreadsheet pattern: a column of data with several columns of calculations based on that data:</p><pre><code>Data   Formula 1   Formula 2   ...   Formula k
d1     F1(d1)      F2(d1)      ...   Fk(d1)
d2     F1(d2)      F2(d2)      ...   Fk(d2)
...    ...         ...         ...   ...
dn     F1(dn)      F2(dn)      ...   Fk(dn)
</code></pre>
<p>In this example, there are only k distinct formulas (one per column), but they’re duplicated across n rows. Editing these formulas becomes tedious and error-prone because you need to make sure your changes are applied consistently across potentially hundreds or thousands of cells.</p><p>Another major issue is that cell references like “A1” or “B2” are not helpful as variable names. They tell you nothing about what the cell represents. Is A1 a tax rate? A growth percentage? Without context, spreadsheet formulas become cryptic.</p><h2 id="calculation-view-makes-spreadsheet-code-visible-and-editable">Calculation View Makes Spreadsheet Code Visible and Editable</h2>
<p>What if spreadsheets had an alternative view optimized for working with formulas rather than data? This is the core idea behind Calculation View (CV).</p><p>CV displays a text-based representation of a spreadsheet’s formulas alongside the traditional grid. Edits in either view are immediately reflected in the other. This approach preserves all the benefits of the familiar grid interface while adding new capabilities for working with formulas.</p><p>The most powerful feature of CV is range assignment. Instead of typing a formula in one cell and then copying it down or across, you can write:</p><pre><code>B1:B10 = SQRT(A1)
</code></pre>
<p>This single line assigns the formula =SQRT(A1) to cell B1 and copies it down through B10, automatically adjusting cell references in the same way Excel’s copy-paste or drag-fill would. But it’s much more explicit about what’s happening.</p><p>CV also makes it easy to give meaningful names to cells or ranges:</p><pre><code>TaxRate A1 = 0.01
</code></pre>
<p>This puts the value 0.01 in cell A1 and gives it the name “TaxRate”. Other formulas can then refer to “TaxRate” instead of “A1”, making them more readable and less prone to errors.</p><h2 id="multiple-representations-help-users-work-at-different-levels-of-abstraction">Multiple Representations Help Users Work at Different Levels of Abstraction</h2>
<p>The core principle behind CV is known as “multiple representations,” an approach that has been successful in educational contexts. By showing the same information in different ways, users can better understand complex concepts and choose the representation that works best for their current task.</p><p>“By offering multiple representations of the same core object (in our case, the program exemplified by the spreadsheet), we can help the user learn to move fluently between different levels of abstraction, choosing the abstraction appropriate for the task at hand,” the paper explains.</p><p>This is particularly valuable in spreadsheets, where users often need to switch between thinking about individual values and thinking about patterns of calculation across many rows or columns.</p><h2 id="users-completed-tasks-faster-and-with-less-mental-effort">Users Completed Tasks Faster and With Less Mental Effort</h2>
<p>Does adding a second view actually help users? The study tested this question with 22 participants who performed spreadsheet authoring and debugging tasks both with and without CV.</p><p>The results were clear: with CV, participants completed authoring tasks 37% faster (median) and debugging tasks 41% faster (median). They also reported lower cognitive load, particularly in terms of frustration.</p><p>Interestingly, the benefits of CV applied to both experienced and novice spreadsheet users, though in slightly different ways. Less experienced users saw greater speed improvements for authoring tasks, while experienced users were more likely to notice a reduction in physical demand (likely because they were already using various shortcuts and techniques to minimize the physical effort of operations like drag-filling).</p><p>It’s worth noting a limitation of the study: the tasks were fairly constrained, focusing specifically on formula creation, copying, and debugging, rather than the full range of spreadsheet activities. Additional research would be needed to determine whether these benefits extend to all types of spreadsheet work.</p><h2 id="the-future-of-spreadsheet-interfaces-could-involve-multiple-views">The Future of Spreadsheet Interfaces Could Involve Multiple Views</h2>
<p>CV represents just a first step in exploring how multiple representations might improve spreadsheet usability. The paper suggests several potential extensions, such as supporting non-rectangular ranges, sequence generation, and enhanced programming tools for experts.</p><p>Different representations could also be developed for different purposes. Beyond textual code, spreadsheets could potentially be viewed as block-based programs (similar to Scratch) or flow charts that emphasize data dependencies.</p><p>By expanding the ways users can interact with spreadsheets, these tools could make complex data work more accessible and reliable. The traditional grid isn’t going anywhere, but complementary views could help us avoid the pitfalls that have plagued spreadsheets for decades.</p><h2 id="references">References</h2>
<p>Sarkar, A., Gordon, A. D., Jones, S. P., &amp; Toronto, N. (2018, October). Calculation View: Multiple-representation editing in spreadsheets. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 85–93). IEEE. <a href="https://doi.org/10.1109/VLHCC.2018.8506584">https://doi.org/10.1109/VLHCC.2018.8506584</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>How Spreadsheet Users Really Learn Excel: The Informal, Social Path to Spreadsheet Expertise</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/how-spreadsheet-users-really-learn-excel-the-informal-social-path-to-spreadsheet-expertise.html"/>
        <id>https://advaitsarkar.github.io/autoblog/how-spreadsheet-users-really-learn-excel-the-informal-social-path-to-spreadsheet-expertise.html</id>

        <updated>2025-04-14T11:11:04+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the findings of a research paper that investigated how people actually learn to use spreadsheet&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the findings of a research paper that investigated how people actually learn to use spreadsheet software like Microsoft Excel. The study reveals that most Excel learning happens informally through social interactions rather than formal training. The paper analyzed interviews with seven participants of varying expertise levels to understand the spreadsheet learning process. </p><p>Reference: Sarkar, A., &amp; Gordon, A. D. (2018, September). How do people learn to use spreadsheets? (Work in progress). In Proceedings of the 29th Annual Conference of the Psychology of Programming Interest Group (PPIG 2018) (pp. 28–35).</p><h2 id="spreadsheet-learning-rarely-happens-in-classrooms">Spreadsheet Learning Rarely Happens in Classrooms</h2>
<p>Forget formal training courses and thick Excel manuals. The reality of how most people learn spreadsheet skills is far more organic and messy. According to research, Excel users typically don’t learn through structured training but instead through a combination of necessity, observation, and social connections.</p><p>“I never did a course in Excel. I’ve never taken a formal learning in Excel,” admitted one study participant who works as a small company accountant. This sentiment was echoed across participants of various expertise levels, from occasional users to sophisticated developers who create custom spreadsheet add-ins.</p><p>The formal documentation that comes with spreadsheet software often goes unused. Users reported finding built-in help features inadequate for discovering new functions. As one examination marking overseer put it: “I don’t find help in [the tool] particularly useful as a way of discovering a function. It’s very useful when you’ve worked out what function, to understand how it works. But I don’t find it particularly useful for finding the right function to use.”</p><h2 id="learning-spreadsheets-is-driven-by-immediate-problems-not-abstract-interest">Learning Spreadsheets Is Driven by Immediate Problems, Not Abstract Interest</h2>
<p>Most spreadsheet learning is opportunistic. Users typically acquire new skills when they encounter a specific problem that demands a solution beyond their current knowledge base.</p><p>“I know what I know how to use because it’s what I’ve needed,” explained one study participant. “Effectively, I’ve learnt the functions as I’ve come across a need to use them, rather than just learning things abstractly.”</p><p>This problem-driven approach creates an interesting learning pattern. Rather than systematically working through tutorials, users develop their skillset in an uneven, almost haphazard way based on what challenges they’ve faced. Someone might be proficient with complex financial functions but completely unaware of basic data visualization techniques simply because they’ve never needed to create charts.</p><p>A professional accountant in the study described a situation where necessity forced learning: “There was this part of the APT (arbitrage pricing theory) that required a normal distribution and there was no function in Excel to do that. And given an obstacle, I then had to get around it, because it was a University deliverable. Given that obstacle, many hours were spent beating my head against the glass pane of that Mac trying to work out how to do it, and as a consequence, I learnt Excel.”</p><h2 id="the-excel-guru-effect-how-spreadsheet-knowledge-spreads-through-social-networks">The Excel Guru Effect: How Spreadsheet Knowledge Spreads Through Social Networks</h2>
<p>Perhaps the most notable finding is how critical social connections are for spreadsheet learning. Within organizations, knowledge about Excel features and techniques typically spreads from person to person rather than from documentation to person.</p><p>“I used to have a colleague years ago. He was a real whiz with spreadsheets,” recalled one large company accountant. “Occasionally he would point out ‘oh, you can simplify this by putting this in there’ and I guess subliminally you take these things in.”</p><p>This social learning creates informal networks of expertise within organizations, with certain individuals becoming known as spreadsheet “gurus” who others seek out for help. The learning process resembles an apprenticeship model where novices learn by watching and getting guidance from more experienced users.</p><p>An energy demand modeller explained: “Consultancies work on an apprenticeship model, so there’s usually someone more senior on the project, who will advise… you can access advice from people in your team and other gurus around the company.”</p><p>This social spread of knowledge helps explain why certain Excel techniques become common in particular workplaces while remaining unknown in others. When an influential person within an organization adopts a particular approach, it tends to spread through the office.</p><h2 id="the-feature-adoption-trinity-discovery-expertise-and-attention-investment">The Feature Adoption Trinity: Discovery, Expertise, and Attention Investment</h2>
<p>The research suggests that adopting new Excel features involves three distinct components: discovery, expertise acquisition, and attention investment (a concept describing how users decide whether learning something new is worth the effort).</p><p>Feature discovery often happens when users see the feature being used in someone else’s spreadsheet. As one climate change modeller explained: “So for example at some point there was introduced SUMIFS. I must have just seen that in a spreadsheet somewhere and thought ‘what the hell is that?’ and at some point come back to it and looked at it in help, and decided that was the right thing to do.”</p><p>The visibility of features in shared spreadsheets creates natural learning moments. When users see something unfamiliar in a colleague’s work, it sparks curiosity that can lead to learning. This helps explain why visible features tend to spread more effectively than hidden ones.</p><h2 id="motivation-makes-the-difference-in-excel-mastery">Motivation Makes the Difference in Excel Mastery</h2>
<p>Not all spreadsheet users approach learning the same way. The research identifies a spectrum of intrinsic motivation that significantly impacts how people engage with the software.</p><p>Users with low intrinsic motivation tend to learn passively, picking up techniques only when absolutely necessary or when directly shown by colleagues. They often develop “coping mechanisms” that let them accomplish tasks without learning more efficient methods. For example, they might manually type calculated values rather than learning formulas, or use arithmetic operators instead of functions like SUM.</p><p>In contrast, highly motivated users actively seek out knowledge, often through online forums and experimentation. “I went to those kind of fora, and then largely trial and error,” said one climate change modeller. “So trying it out and putting myself in the developer’s mindset here ‘surely this is how they would have done it’ and you try out something assuming that this is how they’ve done it and see if it works as it’s meant to work.”</p><p>These high-motivation users often approach spreadsheet tasks with an eye toward solving not just the immediate problem but a more general version that will be useful in the future. “And my approach to Excel was always to… on any given task, try and solve the task that was one step more general,” explained one participant.</p><h2 id="limitations-and-calls-for-further-research">Limitations and Calls for Further Research</h2>
<p>While insightful, this research was preliminary, based on interviews with only seven participants. This small sample size limits how confidently these findings can be generalized to the broader population of spreadsheet users. Additionally, the convenience sampling method may have introduced selection bias.</p><p>The gender distribution was also heavily skewed, with only one female participant. Previous research has suggested that gender differences may exist in technology adoption patterns, making this an important area for future research with more representative samples.</p><p>Despite these limitations, the findings align with earlier research on informal learning practices and offer valuable insights for designing spreadsheet features that better match how people actually learn.</p><h2 id="references">References</h2>
<p>Sarkar, A., &amp; Gordon, A. D. (2018, September). How do people learn to use spreadsheets? (Work in progress). In Proceedings of the 29th Annual Conference of the Psychology of Programming Interest Group (PPIG 2018) (pp. 28–35).</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Making Sense of Word Meanings: How Visual Interfaces Help Us Navigate Hidden Semantic Spaces</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/making-sense-of-word-meanings-how-visual-interfaces-help-us-navigate-hidden-semantic-spaces.html"/>
        <id>https://advaitsarkar.github.io/autoblog/making-sense-of-word-meanings-how-visual-interfaces-help-us-navigate-hidden-semantic-spaces.html</id>

        <updated>2025-04-14T11:03:07+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a research paper that introduces a novel visualization tool for exploring latent&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a research paper that introduces a novel visualization tool for exploring latent semantic spaces, helping non-experts make sense of complex language data. The tool combines hierarchical clustering with interactive exploration features, allowing users to discover meaningful word relationships more effectively than conventional approaches. </p><p>Refernce: Šemrov, A., Blackwell, A. F., &amp; Sarkar, A. (2018). Visualising latent semantic spaces for sense-making of natural language text. In P. Chapman, G. Stapleton, A. Moktefi, S. Perez-Kriz, &amp; F. Bellucci (Eds.), Diagrammatic representation and inference (pp. 517–525). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-91376-6_49">https://doi.org/10.1007/978-3-319-91376-6_49</a></p><h2 id="the-hidden-dimensions-of-words-remain-mysterious-without-proper-tools">The Hidden Dimensions of Words Remain Mysterious Without Proper Tools</h2>
<p>Words have relationships that aren’t always obvious. The word “bank” relates to “money” and “account,” but also to “river” and “shore” - in completely different contexts. Computers can detect these relationships using statistical techniques like Latent Semantic Analysis (LSA), but the results aren’t easy for humans to interpret.</p><p>“Latent Semantic Analysis is widely used for natural language processing, but is difficult to visualise and interpret,” notes the paper introducing a new approach to this problem. </p><p>LSA works by transforming words into mathematical vectors in a multi-dimensional space. Words with similar meanings cluster together in this space. The problem? These spaces typically have many dimensions, making them impossible to visualize directly. Traditional visual representations make sense to data scientists but remain opaque to everyone else.</p><p>“Unfortunately, users do not find it easy to interpret the dimensions extracted from LSA,” the researchers point out.</p><p>This disconnect between powerful analytical tools and human comprehension represents a significant usability gap. When language processing tools produce results that humans can’t interpret, their practical value diminishes substantially. The visualization challenge becomes crucial.</p><h2 id="clusters-of-word-meanings-make-complex-relationships-visible">Clusters of Word Meanings Make Complex Relationships Visible</h2>
<p>The new interface developed for this study takes a different approach. Instead of overwhelming users with thousands of individual words scattered across a plot, it organizes words into visual clusters based on their semantic relationships.</p><p>“We visualise semantic structure using geometric regions that summarise clusters of related words,” explain the authors.</p><p>These clusters can be explored interactively. Users can select any cluster to zoom in and see the words it contains. This exploration process can continue recursively, allowing users to navigate a semantic hierarchy - from broad themes down to specific, closely related words.</p><p>The visualization strategy transforms what would otherwise be a chaotic scatterplot of 30,000 words into an organized, navigable space. Users can see the overall structure at a glance, then dive deeper into areas of interest.</p><h2 id="four-key-elements-create-an-intuitive-navigation-experience">Four Key Elements Create an Intuitive Navigation Experience</h2>
<p>The system combines four distinct components that work together to make semantic spaces comprehensible:</p><p>First, the hierarchically-clustered scatterplot forms the heart of the interface. Each cluster appears as a distinct shape, with darker colors indicating clusters containing more words. Users can double-click to expand clusters, revealing nested subclusters.</p><p>Second, a heatmap matrix serves as a navigation aid. “We plot the sampled probability density of the data as a heatmap, with colour mapped to density,” the paper explains. This gives users an overview of all dimension pairs and lets them select which dimensions to display in the main view.</p><p>Third, a word cloud appears when users click on a cluster. “Only the 30 words closest to the cluster centroid are displayed, as these are most representative of the cluster,” note the authors. The size and color of words in the cloud correspond to their distance from the center of the cluster.</p><p>Finally, a graphical history track keeps users oriented as they explore. When users expand a cluster, the system preserves a snapshot of the previous view, allowing them to retrace their steps or understand how subclusters relate to the broader space.</p><p>Together, these elements create an interface that supports both broad exploration across dimensions and deep dives into specific word relationships.</p><h2 id="the-new-visualization-outperforms-conventional-approaches">The New Visualization Outperforms Conventional Approaches</h2>
<p>To test their interface, the developers conducted a study with 12 university students who had no prior exposure to LSA. These participants used three different visualization systems: the new clustered approach, a traditional scatterplot with pan and zoom capabilities, and a heatmap-based alternative.</p><p>The results were clear. “Participants assigned meaning to significantly more word groups using [the cluster diagram] (average of 7.92 groups) versus [the heatmap] (5.33 word groups) and [the scatterplot] (4.25),” reports the paper.</p><p>Participants also explored words differently with the new interface. While they switched dimensions more frequently with the traditional scatterplot, they inspected far more individual words with the clustered approach - an average of 1,517 words compared to just 479 with the heatmap and 772 with the scatterplot.</p><p>“[The cluster diagram] therefore promoted a more depth-first style of exploration due to the ease of navigating the hierarchy, facilitating model interpretation grounded in specific words,” the researchers observed.</p><p>The usability ratings showed similar patterns. The cluster-based interface received significantly better usability scores on standardized questionnaires than either alternative approach.</p><h2 id="understanding-the-limitations-of-this-visualization-approach">Understanding the Limitations of This Visualization Approach</h2>
<p>Despite its advantages, the study has several limitations worth noting. The sample size was relatively small at just 12 participants, all university undergraduates, which limits how broadly the findings can be generalized. </p><p>The study used a snapshot of Wikipedia as its text corpus, which may have different characteristics than other types of text collections. Real-world applications might involve more specialized or domain-specific language.</p><p>Additionally, while the interface improved users’ ability to find related word groups, it didn’t significantly improve their ability to interpret the meaning of individual dimensions in the semantic space - one of the stated goals of the research.</p><p>The hierarchical clustering approach also imposes a particular structure on the semantic space, which might not always match the natural organization of concepts. Different clustering algorithms could potentially yield different results.</p><h2 id="visualization-tools-bridge-the-gap-between-statistical-models-and-human-understanding">Visualization Tools Bridge the Gap Between Statistical Models and Human Understanding</h2>
<p>Language models continue to grow in sophistication and importance. From search engines to recommendation systems to AI assistants, these models shape our digital experiences in countless ways. Yet their inner workings often remain mysterious, even to those who use them regularly.</p><p>Tools that make these models more transparent and interpretable serve an important purpose. They help non-experts engage with complex data in more meaningful ways, potentially leading to new insights and applications.</p><p>“Latent semantic spaces are a valuable tool for the analysis of large text corpora. However, interpreting latent semantic spaces is difficult, and visual scalability is a major design challenge, as is accessibility for non-experts,” the paper concludes.</p><p>By addressing these challenges, visualization interfaces like the one described here take a meaningful step toward making advanced language processing techniques more accessible and useful to a broader audience.</p><h2 id="references">References</h2>
<p>Šemrov, A., Blackwell, A. F., &amp; Sarkar, A. (2018). Visualising latent semantic spaces for sense-making of natural language text. In P. Chapman, G. Stapleton, A. Moktefi, S. Perez-Kriz, &amp; F. Bellucci (Eds.), Diagrammatic representation and inference (pp. 517–525). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-91376-6_49">https://doi.org/10.1007/978-3-319-91376-6_49</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Solving the Hidden Patterns Puzzle: How the Gatherminer Data Visualization Tool Makes Sense of Time Series Data</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/solving-the-hidden-patterns-puzzle-how-data-visualization-tools-make-sense-of-time-series-data.html"/>
        <id>https://advaitsarkar.github.io/autoblog/solving-the-hidden-patterns-puzzle-how-data-visualization-tools-make-sense-of-time-series-data.html</id>

        <updated>2025-04-14T10:52:13+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the research findings of a paper that introduces Gatherminer, a visual analytics tool designed to&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the research findings of a paper that introduces Gatherminer, a visual analytics tool designed to help analysts discover and explain patterns in time series data. The tool combines color-mapped matrices with automated rearrangement algorithms and machine learning to make pattern detection more efficient and accurate. The paper presents evidence that this specialized approach significantly outperforms general-purpose visualization tools for time series analysis tasks. </p><p>Reference: Sarkar, A., Spott, M., Blackwell, A. F., &amp; Jamnik, M. (2016, September). Visual discovery and model-driven explanation of time series patterns. In 2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 78–86). IEEE. <a href="https://doi.org/10.1109/VLHCC.2016.7739668">https://doi.org/10.1109/VLHCC.2016.7739668</a></p><h2 id="time-series-analysis-has-always-been-a-complex-visual-challenge">Time series analysis has always been a complex visual challenge</h2>
<p>In telecommunications networks, healthcare systems, financial markets, and countless other domains, analysts face mountains of time series data: sequences of values measured at successive time intervals. Finding meaningful patterns in these sequences requires sophisticated visualization techniques.</p><p>Imagine being a network analyst at a major telecommunications company like BT. Every day, thousands of devices on the network experience faults. Your job is to identify unusual patterns in these faults and explain them by connecting them to specific attributes like device types, locations, or customer demographics.</p><p>“Analysts identify patterns of faults in subsets of this large database of time series, and look for potential causes of these patterns,” the paper explains. “Once interesting behaviour, such as ‘devices with unusually high fault rates,’ or ‘devices with an inflexion in the time series’ is found, a corresponding explanation is sought.”</p><p>This task presents two significant challenges. First, you don’t know what patterns you’re looking for until you see them. Second, with hundreds of possible explanatory attributes, each with multiple values, there are simply too many combinations to check manually.</p><h2 id="traditional-analysis-tools-force-analysts-to-search-like-theyre-playing-hide-and-seek-in-the-dark">Traditional analysis tools force analysts to search like they’re playing hide-and-seek in the dark</h2>
<p>When facing these challenges with conventional tools, analysts often resort to what the paper describes as “opportunistic approaches” that have serious drawbacks:</p><ol>
<li>They rely heavily on the analyst’s domain expertise to guide exploration</li>
<li>Interesting patterns may be overlooked entirely</li>
<li>False correlations might be “discovered”</li>
<li>The process requires extensive manual inspection of attributes</li>
</ol>
<p>The result? “Incomplete, inaccurate, and slow analyses” that leave analysts feeling unconfident about their conclusions.</p><p>Consider a typical workflow using a general-purpose tool like Tableau. An analyst might create visualizations for each attribute value separately, checking one by one whether any particular attribute correlates with an interesting pattern. With just a few attributes, this approach might work. But in real-world scenarios with hundreds of attributes, this manual process becomes practically impossible.</p><p>“You’ve got too many dimensions to visualize simultaneously,” one analyst remarked during the study. “I feel like I’m missing a lot if I do it manually.”</p><h2 id="gatherminer-transforms-pattern-finding-with-a-simple-visualization-approach">Gatherminer transforms pattern finding with a simple visualization approach</h2>
<p>The Gatherminer tool tackles these challenges through a compact but powerful visualization method. Instead of traditional line graphs, it represents each time series as a row of color-coded cells in a matrix, where the color intensity corresponds to the value at each time point.</p><p>This approach is space efficient. A traditional line graph requires substantial screen space to show multiple series clearly. In contrast, a color-mapped matrix can compress each data point to a single pixel, allowing hundreds or even thousands of time series to be displayed simultaneously.</p><p>But the real magic happens when these rows are automatically rearranged or “gathered” to place similar time series next to each other. The paper explains that this gathering process “exposes groups of series bearing interesting analytical features such as peaks and trends.”</p><p>When time series with similar patterns are grouped together, they form visually distinct regions—blobs, streaks, or clusters of similar colors—that immediately draw the analyst’s attention. This visual grouping makes pattern detection intuitive and robust, even when the analyst doesn’t know what specific patterns they’re looking for.</p><h2 id="machine-learning-transforms-analyst-selections-into-explanatory-insights">Machine learning transforms analyst selections into explanatory insights</h2>
<p>After identifying an interesting pattern visually, the analyst can select that region to trigger an automated explanation process. This is where Gatherminer truly shines.</p><p>The system uses this selection to train a machine learning model (specifically, a decision tree algorithm) that identifies which attributes best distinguish the selected time series from the rest. This approach elegantly scales to datasets with hundreds of attributes, as the algorithm automatically focuses on the most relevant ones.</p><p>“A key advantage of deploying the ID3 algorithm in this manner is that this method of mining explanations scales to arbitrarily large attribute-value spaces,” the paper notes. For a dataset with just 8 attributes, there could be over 100,000 possible attribute-value combinations to check. The decision tree, however, might represent the most relevant combinations in just 100 nodes.</p><p>The system also provides simple bar charts showing the distribution of attribute values in the selection compared to the overall dataset, offering an intuitive visualization of potential explanations that doesn’t require statistical expertise.</p><h2 id="real-experts-found-more-patterns-faster-and-felt-more-confident-with-the-specialized-tool">Real experts found more patterns faster and felt more confident with the specialized tool</h2>
<p>To test Gatherminer’s effectiveness, the researchers conducted a study with six professional analysts from BT Research who regularly analyze network data using Tableau. Each participant completed matched pairs of tasks using both Tableau and Gatherminer.</p><p>The results were striking. With Gatherminer, participants:</p><ul>
<li>Found 50% more interesting features</li>
<li>Discovered 66.7% more correct explanations</li>
<li>Took significantly less time (a median improvement of 110.5 seconds per feature)</li>
<li>Reported much higher confidence in their analyses</li>
</ul>
<p>“Am I confident I have discovered all the features? Yes, of course, I have seen it,” remarked one participant when using Gatherminer.</p><h2 id="understanding-the-limitations-provides-context-for-these-impressive-results">Understanding the limitations provides context for these impressive results</h2>
<p>While the results are compelling, several limitations should be noted. The experimental tasks used synthetic data with deliberately planted patterns, rather than real-world messy data. This controlled environment allowed for cleaner comparison between tools but might not fully represent the complexity of real-world analysis scenarios.</p><p>Additionally, the study involved only six participants, all from the same company. A larger and more diverse sample would strengthen the generalizability of the findings.</p><p>The tasks also involved relatively simple patterns (spikes) and fairly small datasets (500 time series with 6 attributes each). Real-world scenarios often involve more subtle patterns and much larger datasets.</p><p>Finally, the comparison was against Tableau, a general-purpose visual analytics tool. While this represents a realistic baseline given that specialized tools for this task are not widely available, it’s worth noting that Tableau wasn’t specifically designed for this particular time series analysis workflow.</p><h2 id="domain-knowledge-remains-a-critical-component-of-effective-data-analysis">Domain knowledge remains a critical component of effective data analysis</h2>
<p>An intriguing insight from this research concerns the role of domain expertise. The experiments deliberately used meaningless codes for attributes to separate the tool’s utility from the analysts’ domain knowledge.</p><p>“These comments illustrate how domain expertise actually plays a significant role in the analyst’s heuristic approach to discovering explanations of interesting features,” the paper observes. This highlights that even the best visualization tools are most effective when complemented by human expertise.</p><p>Gatherminer represents a powerful example of how specialized visual analytics tools can transform difficult analytical tasks into intuitive visual processes, augmenting rather than replacing human judgment.</p><h2 id="references">References</h2>
<p>Sarkar, A., Spott, M., Blackwell, A. F., &amp; Jamnik, M. (2016, September). Visual discovery and model-driven explanation of time series patterns. In 2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 78–86). IEEE. <a href="https://doi.org/10.1109/VLHCC.2016.7739668">https://doi.org/10.1109/VLHCC.2016.7739668</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Sorting Videos with Setwise Comparison Is More Efficient and Consistent Than Traditional Methods</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/sorting-videos-with-setwise-comparison-is-more-efficient-and-consistent-than-traditional-methods.html"/>
        <id>https://advaitsarkar.github.io/autoblog/sorting-videos-with-setwise-comparison-is-more-efficient-and-consistent-than-traditional-methods.html</id>

        <updated>2025-04-14T10:43:45+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a scientific paper that introduces a new method called “setwise comparison” for&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a scientific paper that introduces a new method called “setwise comparison” for labeling videos with continuous scores more efficiently and consistently than traditional methods. The technique was developed to support computer vision systems in healthcare but could apply to many domains requiring expert judgment. The researchers found that their new approach not only reduced labeling time dramatically but also unexpectedly improved consistency between different raters.</p><p>Reference: Sarkar, A., Morrison, C., Dorn, J. F., Bedi, R., Steinheimer, S., Boisvert, J., Burggraaff, J., D’Souza, M., Kontschieder, P., Rota Bulò, S., Walsh, L., Kamm, C. P., Zaykov, Y., Sellen, A., &amp; Lindley, S. (2016). Setwise comparison: Consistent, scalable, continuum labels for computer vision. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (pp. 261–271). ACM. <a href="https://doi.org/10.1145/2858036.2858199">https://doi.org/10.1145/2858036.2858199</a></p><h2 id="the-challenge-of-putting-numbers-on-what-we-see-in-videos">The challenge of putting numbers on what we see in videos</h2>
<p>Imagine you’re a neurologist watching videos of patients with multiple sclerosis performing simple movements. Your job is to rate each patient’s tremor severity on a scale from 0 to 4. Sounds straightforward, right?</p><p>Not quite. When multiple neurologists rate the same videos, they often disagree significantly. In one study, agreement on previously established “gold standard” videos ranged from just 23% to 69%.</p><p>This inconsistency poses a significant problem for computer vision systems that need reliable labels to learn from. The paper “Setwise Comparison: Consistent, Scalable, Continuum Labels for Computer Vision” addresses this challenge of providing what the authors call “continuum labels” - single, real number scores that capture properties observed in videos.</p><p>“Continuum labelling suggests that the boundaries of a particular rating category (e.g., the boundary between a motor ability label of ‘1’ versus ‘2’) can never be exactly determined,” the paper explains.</p><h2 id="traditional-labeling-approaches-face-an-efficiency-consistency-tradeoff">Traditional labeling approaches face an efficiency-consistency tradeoff</h2>
<p>The problem of labeling videos consistently has traditionally led to an unfortunate tradeoff. Quick methods like assigning numeric scores (0-4) take mere seconds per video but yield inconsistent results between raters. Meanwhile, more consistent methods like pairwise comparison (comparing every possible pair of videos) become impractical as collections grow.</p><p>“Comparing every video to every other in the set of 50 required 1,225 comparisons. Neurologists required between 87 and 146 minutes to rate this set of videos,” the researchers noted. “As the number of comparisons grows quadratically with the number of videos, this is clearly not tractable for even a small video set of 300, which would require 44,850 comparisons, or over 37h of continuous labelling.”</p><p>For real-world applications like medical assessments or sports coaching, this inefficiency renders traditional methods unusable at scale.</p><h2 id="setwise-comparison-dramatically-improves-both-efficiency-and-consistency">Setwise comparison dramatically improves both efficiency and consistency</h2>
<p>The paper introduces a novel interaction technique called “setwise comparison” that solves this dilemma. Rather than comparing videos in pairs, raters sort small groups (sets) of videos relative to each other. The key innovation combines three elements:</p><ol>
<li>Human relative judgment capabilities</li>
<li>A visual interface for comparing multiple videos simultaneously </li>
<li>The TrueSkill algorithm (originally developed for ranking online gamers)</li>
</ol>
<p>The system, called SorTable, presents users with sets of videos as thumbnails. Users can:</p><ul>
<li>Sort videos from least to greatest severity</li>
<li>Stack videos they judge to be equal</li>
<li>Use gestures to compare videos side-by-side</li>
</ul>
<p>“Our approach of enforcing which videos were available to compare employs a design-with-intent philosophy,” the researchers explained.</p><p>The real magic happens behind the scenes. Since each set shares some videos with other sets, the TrueSkill algorithm can mathematically combine these partial rankings into a complete ordering of all videos - even inferring comparisons between videos that never appeared together in the same set.</p><h2 id="the-study-found-surprising-improvements-in-both-efficiency-and-consistency">The study found surprising improvements in both efficiency and consistency</h2>
<p>When eight neurologists tested both methods on a set of 40 patient videos, the results were striking:</p><p>“Setwise comparison was significantly more efficient than pairwise comparison. Task time was reduced by an average of 54 minutes”</p><p>For a modest training set of 400 videos, pairwise comparison would take approximately 100 hours, whereas setwise comparison would take only about 3.5 hours - a dramatic improvement in efficiency.</p><p>But the real surprise was that consistency also improved. The global intraclass correlation coefficient (a measure of agreement between raters from 0 to 1) increased from 0.70 for pairwise comparison to 0.83 for setwise comparison. In medical research, results above 0.8 are considered excellent.</p><p>“We were surprised that setwise comparison was not only more efficient, but also significantly more consistent,” the researchers acknowledged.</p><p>Remarkably, there was no significant difference in cognitive load between the two methods, suggesting that sorting multiple videos wasn’t mentally harder than comparing pairs. Though note, the absence of a significant difference should not be interpreted as evidence of a non-difference.</p><h2 id="different-users-adopted-distinct-sorting-strategies">Different users adopted distinct sorting strategies</h2>
<p>An interesting finding was that neurologists developed different approaches to the task. Some used an “insertion sort” strategy - viewing many videos first, then relying on memory to place them in order. Others used more of a “bubble sort” approach, making rapid comparisons and swaps between neighboring videos.</p><p>“Some participants viewed a large number of videos and relied on their memory to then sort them, interacting with most videos only once,” the paper explains. “Others did rapid comparisons and swaps of immediate neighbours, bubbling videos to the correct position.”</p><p>Despite these different strategies, all raters achieved similar levels of consistency, suggesting that the system accommodates various cognitive approaches.</p><h2 id="the-implications-extend-beyond-clinical-assessment">The implications extend beyond clinical assessment</h2>
<p>Although developed for a specific medical application (ASSESS MS), the setwise comparison technique has broad implications for many domains requiring expert judgment of videos on a continuous scale:</p><ul>
<li>Sports coaching evaluations</li>
<li>Affect recognition (detecting emotions)</li>
<li>Movement analysis</li>
<li>Any domain where experts need to provide consistent ratings</li>
</ul>
<p>The study’s limitations include a small sample size of eight neurologists, though this reflects the realistic constraints of specialized expert evaluation. Additionally, the technique was only tested on one type of neurological assessment (the Finger Nose test), and its applicability to other types of video evaluation remains to be confirmed.</p><p>Nevertheless, the improvement in both efficiency and consistency suggests that setwise comparison could transform how videos are labeled for machine learning applications, particularly in specialized fields where expert time is limited and expensive.</p><h2 id="references">References</h2>
<p>Sarkar, A., Morrison, C., Dorn, J. F., Bedi, R., Steinheimer, S., Boisvert, J., Burggraaff, J., D’Souza, M., Kontschieder, P., Rota Bulò, S., Walsh, L., Kamm, C. P., Zaykov, Y., Sellen, A., &amp; Lindley, S. (2016). Setwise comparison: Consistent, scalable, continuum labels for computer vision. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (pp. 261–271). ACM. <a href="https://doi.org/10.1145/2858036.2858199">https://doi.org/10.1145/2858036.2858199</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Sketching Data Bridges the Gap Between Data Analysts and Their Clients</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/sketching-data-bridges-the-gap-between-data-analysts-and-their-clients.html"/>
        <id>https://advaitsarkar.github.io/autoblog/sketching-data-bridges-the-gap-between-data-analysts-and-their-clients.html</id>

        <updated>2025-04-14T10:36:18+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a scientific paper that explores how data analysts and their clients can&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a scientific paper that explores how data analysts and their clients can communicate more effectively through data sketching tools. The research reveals how analysts often struggle to translate high-level business questions into data analysis tasks, and proposes a prototype tool that allows collaborative sketching of time series data to clarify hypotheses.</p><p>Reference: Marasoiu, M., Blackwell, A. F., Sarkar, A., &amp; Spott, M. (2016). Clarifying hypotheses by sketching data. In Eurographics/IEEE VGTC Conference on Visualization (EuroVis 2016). The Eurographics Association. <a href="https://doi.org/10.2312/eurovisshort.20161173">https://doi.org/10.2312/eurovisshort.20161173</a></p><h2 id="data-analysts-spend-significant-time-teaching-statistics-to-their-clients">Data analysts spend significant time teaching statistics to their clients</h2>
<p>When was the last time you tried to explain a complex technical concept to someone with no background in your field? If you’re a data analyst, this might be a daily occurrence. </p><p>Data analysts frequently find themselves in the awkward position of being statistics teachers rather than pure analysts. Their clients come to them with vague requests and imprecise questions, often lacking the statistical vocabulary to articulate what they actually need.</p><p>“Discussions between data analysts and colleagues or clients with no statistical background are difficult, as the analyst often has to teach and explain their statistical and domain knowledge,” notes the research paper. This creates a peculiar dynamic where analysts must first educate before they can analyze.</p><p>The paper’s investigation into the work practices of data analysts who collaborate with non-experts uncovered several key challenges. Analysts reported that “they often need a conversation with the requester after receiving an email from them in order to clarify and fill in missing details of their high-level question.” These clarification calls are crucial but inefficient, especially when the data isn’t readily available during the conversation.</p><p>As one analyst in the study explained: “So when I get something like that it will usually be let’s have a look at it, let’s think about where the gaps are, got to list all the gaps, all of the things that I need answered now, then ask that question, get the information that I actually need to finish it off.” </p><p>Another analyst simply said: “Most often it’s a quick call saying ‘is this exactly what you’re after, do you need this, this and this and this?’”</p><h2 id="function-composition-offers-a-structured-way-to-sketch-time-series-data">Function composition offers a structured way to sketch time series data</h2>
<p>What if there was a better way to have these conversations? The researchers developed a prototype tool called “SelfRaisingData” that allows analysts to sketch time series data during these clarification calls, even when the actual data isn’t yet available.</p><p>Rather than free-hand drawing, which might be imprecise or difficult to adjust, the tool uses function composition. This mathematical approach allows users to build complex data patterns by combining simpler functions - like linear trends, periodic cycles, or exponential growth.</p><p>“We chose time series visualisation as our interviews suggest it to be one of the most used types of data that analysts work with,” explains the paper.</p><p>The prototype tool features a time series chart as its central component, with data points generated around shapes described through function composition. Functions can be dragged onto the chart from a tool panel, and each function can be modified independently through interactive controls.</p><p>This approach has distinct advantages over traditional sketching. When a user adjusts parameters of one function - such as changing the slope of a linear trend - it doesn’t affect other components of the visualization. This preserves the semantic meaning behind each component, making it easier to discuss specific hypotheses about the data.</p><p>“In contrast to sketching a line on paper, function composition adds hypothesis semantics to the visualisation,” the researchers write.</p><h2 id="users-found-the-function-composition-approach-intuitive-and-valuable">Users found the function composition approach intuitive and valuable</h2>
<p>To evaluate their tool, the researchers conducted two usability studies with participants who had technical backgrounds similar to data analysts. The results were encouraging.</p><p>Participants described the tool as “easy to use” and “fairly intuitive.” They appreciated the function composition principles, with one participant noting, “I liked being able to separate out the functions.”</p><p>Most importantly, an experienced visual analytics professional confirmed the tool’s potential usefulness in real-world scenarios:</p><p>“One of the biggest challenges I have in communicating to a client is to create dummy data to try and describe what I’m expecting to see, so actually being able to manipulate the dataset like that is a really useful concept.”</p><p>This validation from someone with extensive experience suggests the approach could address real communication gaps in the industry.</p><h2 id="the-balance-between-precision-and-sketchiness-remains-challenging">The balance between precision and sketchiness remains challenging</h2>
<p>Despite positive feedback, the tool revealed an interesting tension between precision and sketchiness. While some participants embraced the sketchy nature of the visualization, others wanted more control.</p><p>“I think the data points, being kinda farther away from the line served as a bit more of a distraction. I feel like it would be handy to be able to manipulate the lines more precisely,” noted one participant.</p><p>The researchers intentionally rendered the visualizations as ‘noisy’ scatterplots rather than precise line graphs to “reinforce that the sketch is a provisional, transient conversational aid.” This design choice pushes against users’ natural tendency to fixate on creating exact visualizations.</p><p>Some participants suggested that being able to adjust the variance of the data point distribution would be useful, either to show the degree of accuracy of the function composition or to make the visualization more realistic for different datasets.</p><h2 id="limitations">Limitations</h2>
<p>Several limitations should be noted when interpreting this research. The study focused primarily on usability rather than evaluating how the tool would function in real collaborative settings. The researchers acknowledge that “further development would benefit from more discussions and studies of data analysts” and that they are “looking at testing the system in a business environment and studying how it would be used in collaborative analytics practice.”</p><p>Additionally, the current system only supports time series data visualized as scatter charts, a decision driven by their analyst interviews. While time series data might be common, it represents just one type of data analysis that professionals conduct.</p><p>The small sample size of the usability studies (6 participants each) and the fact that most participants were graduate students rather than professional analysts also suggests caution when generalizing the results.</p><p>Nevertheless, the research points to promising directions for improving collaboration between data analysts and their clients in the critical early stages of the analytical process when hypotheses are being clarified and refined.</p><h2 id="references">References</h2>
<p>Marasoiu, M., Blackwell, A. F., Sarkar, A., &amp; Spott, M. (2016). Clarifying hypotheses by sketching data. In Eurographics/IEEE VGTC Conference on Visualization (EuroVis 2016). The Eurographics Association. <a href="https://doi.org/10.2312/eurovisshort.20161173">https://doi.org/10.2312/eurovisshort.20161173</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Visual Programming Tools Make Learning Probabilistic Programming Easier for Novices</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/visual-programming-tools-make-learning-probabilistic-programming-easier-for-novices.html"/>
        <id>https://advaitsarkar.github.io/autoblog/visual-programming-tools-make-learning-probabilistic-programming-easier-for-novices.html</id>

        <updated>2025-04-10T16:07:11+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the findings of a study that explored how multiple representation tools can help novice programmers&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the findings of a study that explored how multiple representation tools can help novice programmers learn probabilistic programming. The research demonstrates that visualizing code as both text and diagrams significantly reduces effort in learning and applying probabilistic programming concepts. The study evaluates a custom-built development environment that shows code, network diagrams, and probability distributions simultaneously, revealing measurable benefits for beginners.</p><p><strong>Reference</strong>: Gorinova, M. I., Sarkar, A., Blackwell, A. F., &amp; Syme, D. (2016). A live, multiple-representation probabilistic programming environment for novices. In <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em> (pp. 2533–2537). ACM. <a href="https://doi.org/10.1145/2858036.2858221">https://doi.org/10.1145/2858036.2858221</a></p><h2 id="what-makes-probabilistic-programming-so-challenging-to-learn">What Makes Probabilistic Programming So Challenging to Learn?</h2>
<p>Traditional programming is built on a deceptively simple concept: variables hold specific values. You assign the number 5 to variable “x,” and that’s what x contains until you change it. For generations of programmers, this fundamental concept has been the entry point to computational thinking.</p><p>But what if a variable doesn’t represent a single value, but an entire probability distribution? What if changing one variable automatically updates the probabilities of other variables? Welcome to the mind-bending world of probabilistic programming, a growing paradigm that’s becoming increasingly important for data analytics, machine learning, and statistical modeling.</p><p>“The conventional understanding that a variable holds only one value has been suggested as the single most important concept determining whether someone is able to learn programming,” notes the paper. “The conceptual model of probabilistic programming, where variables embody distributions, involves a substantial shift from this convention.”</p><p>This conceptual leap makes probabilistic programming notoriously difficult for newcomers to grasp. Traditional code editors show only text, forcing learners to mentally visualize complex probability relationships while simultaneously wrestling with unfamiliar syntax.</p><h2 id="multiple-representation-environments-show-code-in-different-ways-simultaneously">Multiple Representation Environments Show Code in Different Ways Simultaneously</h2>
<p>The research team developed a specialized programming environment that displays three synchronized views of the same program:</p><ol>
<li>A traditional text editor showing the actual code</li>
<li>A Bayesian network diagram showing variables and their dependencies as nodes and arrows</li>
<li>Charts displaying the probability distributions of each variable</li>
</ol>
<p>This approach builds on established educational theory about multiple representations. When we see the same concept presented in different formats simultaneously, our understanding deepens as we make connections between these representations.</p><p>The system maintains “liveness,” meaning that as you type code, the visual representations update in real-time. This immediate feedback loop creates a powerful learning environment where students can see the consequences of their changes instantly.</p><p>As one study participant enthused when switching from the visual environment back to a text-only editor: “Argh, no graph, nooo!”</p><h2 id="the-study-shows-significant-reductions-in-programming-effort-with-visual-tools">The Study Shows Significant Reductions in Programming Effort with Visual Tools</h2>
<p>To measure the actual impact of this multi-view approach, the researchers conducted a controlled experiment with 16 undergraduate computer science and mathematics students. None had prior experience with probabilistic programming, though all had basic knowledge of probability theory.</p><p>The experiment unfolded in two phases. In the practice phase, students were divided into two groups: one using the Multiple Representation Environment (MRE) and one using a Conventional Editor (CE) showing only code. All students worked through the same tutorial materials and introductory exercises.</p><p>Even in this initial learning phase, the benefits became apparent. Students using the visual system typed significantly fewer keystrokes to complete the same exercises, suggesting they grasped concepts more quickly and made fewer errors.</p><p>The real payoff came in the second phase, where all participants performed debugging and description tasks using both editors in a counterbalanced design. The results were striking:</p><p>“With the MRE, participants gained a median task time improvement of 71s, a median reduction of 63.5 keystrokes, and a median reduction of 5.5 deletions.”</p><p>In practical terms, students completed tasks faster, typed less, and made fewer mistakes when they could see the visual representations alongside the code.</p><h2 id="visualizations-change-how-learners-think-about-programs">Visualizations Change How Learners Think About Programs</h2>
<p>Perhaps most interesting were the qualitative observations about how the visualizations changed students’ mental models. Without visuals, students tended to interpret programs line-by-line, reciting statements like “when Cloudy is true, Rain is Bernoulli(0.7), otherwise it is Bernoulli(0.1).”</p><p>With the visual environment, they shifted to higher-level thinking about relationships: “It’s more likely to rain when it is Cloudy, and consequently it is more likely to be wet when it is Cloudy.”</p><p>One participant remarked: “Looking at the code is horrific… but looking at the graph is not that bad.” Another announced: “I’m not going to look at the code anymore!” after discovering the visual representation.</p><p>These comments reveal how the right visualization can transform an incomprehensible wall of code into an intuitive model that corresponds to how we naturally think about probability relationships.</p><h2 id="study-limitations-suggest-caution-when-interpreting-results">Study Limitations Suggest Caution When Interpreting Results</h2>
<p>While the results are promising, several limitations should be noted. The sample size was relatively small at 16 participants, and all were undergraduates with strong technical backgrounds. Whether these benefits would extend to other populations remains an open question.</p><p>Additionally, the study focused on relatively simple exercises rather than the complex models data scientists might build in professional settings. The researchers acknowledge this limitation, noting: “A natural next step would be to extend this investigation from interpretation and debugging tasks, to more exploratory development tasks (as carried out by professional data analysts).”</p><p>The study also didn’t include tasks where participants had to create programs from scratch, focusing instead on modification and interpretation exercises. This reflected the reality that novice end-user developers often modify existing programs rather than authoring new ones, but it doesn’t tell us whether the visual approach would help with program creation.</p><h2 id="visual-programming-tools-offer-promise-for-making-advanced-concepts-more-accessible">Visual Programming Tools Offer Promise for Making Advanced Concepts More Accessible</h2>
<p>Despite these limitations, the research offers compelling evidence that multiple representation environments can make challenging programming concepts more approachable. By leveraging our visual processing abilities and connecting abstract code to familiar diagrammatic conventions, these tools lower the barriers to entry for probabilistic programming.</p><p>As data analytics and machine learning become increasingly central to fields beyond computer science, tools that make these concepts accessible to broader audiences grow increasingly important. The visual approach tested in this study might be just the beginning of a new generation of programming environments designed not just for efficiency, but for learning and comprehension.</p><p>The next time you find yourself struggling with an abstract programming concept, remember: sometimes what you need isn’t just more code examples, but a different way of seeing the problem.</p><h2 id="references">References</h2>
<p>Gorinova, M. I., Sarkar, A., Blackwell, A. F., &amp; Syme, D. (2016). A live, multiple-representation probabilistic programming environment for novices. In <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em> (pp. 2533–2537). ACM. <a href="https://doi.org/10.1145/2858036.2858221">https://doi.org/10.1145/2858036.2858221</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Learning With Machines: How Constructivist Design Shapes Interactive Machine Learning Systems</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/learning-with-machines-how-constructivist-design-shapes-interactive-machine-learning-systems.html"/>
        <id>https://advaitsarkar.github.io/autoblog/learning-with-machines-how-constructivist-design-shapes-interactive-machine-learning-systems.html</id>

        <updated>2025-04-10T15:48:21+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the concepts presented in a paper exploring how constructivist learning theory can improve the design&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the concepts presented in a paper exploring how constructivist learning theory can improve the design of interactive machine learning systems. The research examines how the feedback loop between users and machine learning systems creates a learning environment similar to educational settings, suggesting new design approaches that could make these systems more effective and user-friendly. </p><p><strong>Reference</strong>: Sarkar, A. (2016). <em>Constructivist design for interactive machine learning</em>. In <em>Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em> (pp. 1467–1475). ACM. <a href="https://doi.org/10.1145/2851581.2892547">https://doi.org/10.1145/2851581.2892547</a></p><h2 id="interactive-machine-learning-systems-help-non-experts-build-statistical-models">Interactive Machine Learning Systems Help Non-Experts Build Statistical Models</h2>
<p>In today’s data-driven world, machine learning has become increasingly important. But what happens when ordinary people without statistical expertise need to use these complex tools? This is where interactive machine learning (IML) systems come into play.</p><p>Interactive machine learning systems are designed to help people who aren’t experts in statistics or computer science build and apply machine learning models for their own purposes. Unlike traditional machine learning approaches that require extensive technical knowledge, IML systems create user-friendly interfaces that allow regular people to train algorithms through simple interactions.</p><p>“As machine learning makes rapid advances, researchers are increasingly interested in enabling people to build and apply machine learning models for their own use in a variety of scenarios,” the paper explains. “These end-users are typically not experts in statistics or machine learning, so careful interaction design is applied, in order to reduce the expertise barriers imposed by the hard concepts of statistical modelling and model programming.”</p><p>The core of IML involves a training loop where users provide examples and judgments to help the system learn. As users label data or make adjustments, the system provides immediate visual feedback about what it’s learning, creating a continuous conversation between human and machine.</p><h2 id="constructivist-learning-theory-provides-a-framework-for-understanding-user-interaction">Constructivist Learning Theory Provides a Framework for Understanding User Interaction</h2>
<p>What exactly is constructivism? It’s an educational theory that views learning not as passive information reception but as an active process where knowledge is constructed through interactions between our ideas and experiences.</p><p>“Constructivism is a theory describing the learning process; the manner in which human knowledge is generated. It posits the view that human knowledge is constructed as a result of the interaction between a person’s mental models and their experiential perceptions,” the paper states.</p><p>This stands in contrast to what the paper calls the “instructionist” view, where learning is treated simply as information delivery. Constructivism, largely attributed to psychologist Jean Piaget, has significantly influenced educational approaches despite mixed evidence about its effectiveness as a teaching tool.</p><p>The connection between constructivism and IML becomes clear when we look at how users interact with these systems. When someone trains a machine learning model, they’re not just inputting data. They’re building understanding through a continuous cycle of action and feedback, much like how children learn through experimentation in constructivist educational environments.</p><h2 id="the-hidden-learning-objectives-of-interactive-machine-learning-systems">The Hidden Learning Objectives of Interactive Machine Learning Systems</h2>
<p>When people use interactive machine learning systems, they’re often focused on practical goals: building a working model or analyzing data. However, the paper argues that beneath these surface-level objectives lie implicit learning outcomes.</p><p>These learning outcomes include:</p><p>“Model-building: learning about the model instance, its strengths, weaknesses, coverage of training data, fitted parameters, etc.”</p><p>“Analysis: learning about the structure of the data, its statistical properties and features.”</p><p>“Exposition to statistical concepts: learning about a particular algorithm, or general concepts about training and testing such as class representation, noisy data, outliers, etc.”</p><p>The paper suggests that by recognizing these hidden learning objectives, developers can design better systems that explicitly support the learning process rather than treating it as an incidental side effect.</p><h2 id="errors-and-mistakes-should-be-embraced-as-learning-opportunities">Errors and Mistakes Should Be Embraced as Learning Opportunities</h2>
<p>In the world of software development, errors are typically viewed as problems to be eliminated. But in constructivist learning, “perturbations” or unexpected results are actually the engine that drives learning forward.</p><p>“The concept of perturbation (or disequilibration, in Piagetian terms) is the engine driving the learning process. It refers to a stimulus which does not conform, or gently subverts, the expectations and mental model of the user, forcing them to construct new knowledge in order to ‘accommodate’ this experience,” the paper explains.</p><p>This perspective suggests that when a machine learning model makes mistakes or produces unexpected results, these moments shouldn’t be treated merely as failures to be corrected. Instead, they represent valuable learning opportunities that help users develop deeper understanding of both their data and the statistical models being applied.</p><p>Current IML systems already focus on addressing errors, but they could benefit from a more constructivist approach that frames mistakes positively rather than negatively.</p><h2 id="future-interactive-machine-learning-systems-should-support-reflection-and-collaboration">Future Interactive Machine Learning Systems Should Support Reflection and Collaboration</h2>
<p>The paper identifies several key areas where interactive machine learning systems could be improved by drawing on constructivist design principles. Two particularly important ones are reflexivity and collaboration.</p><p>Reflexivity refers to users’ ability to critically reflect on their own learning process: “A critical self-awareness of one’s learning, beliefs and knowledge is central to constructivist environments. Reflective users take control over and responsibility for their thoughts, and create a defensible catalogue of provenance for their knowledge.”</p><p>Current IML systems don’t typically support this kind of self-reflection. The paper suggests that capturing detailed interaction histories and allowing users to query and browse these histories could help users better understand how they’ve arrived at particular conclusions.</p><p>Similarly, collaboration is largely missing from most interactive machine learning systems, despite the fact that learning is fundamentally a social activity: “Learning takes place in a social context. The construction of meaning, like so many other activities, seldom occurs individually.”</p><p>Incorporating collaborative features could significantly enhance the effectiveness of these systems, particularly for analytical tasks where multiple perspectives can lead to richer insights.</p><h2 id="the-philosophical-implications-of-machine-learning-tools-deserve-attention">The Philosophical Implications of Machine Learning Tools Deserve Attention</h2>
<p>The paper discusses how interactive machine learning systems act as “cultural mediators” that shape how we think about and understand data.</p><p>“IML systems, especially those with an emphasis on analytical outcomes, need to be aware of their role as cultural mediators,” the paper argues. It notes that these systems “embody epistemological and ontological assertions,” which are currently implicit rather than explicit.</p><p>In other words, the design choices embedded in these systems subtly influence what kinds of knowledge users can produce and how they conceptualize data relationships. This raises important questions about whose perspectives and assumptions are being encoded into these increasingly influential tools.</p><p>The paper highlights ongoing disputes between different statistical approaches, such as “frequentist versus Bayesian approaches, which differ on such fundamental axioms as the interpretation of ‘truth’.” These philosophical differences have practical implications for how systems are designed and what kinds of conclusions they facilitate.</p><p>Ultimately, interactive machine learning systems aren’t neutral tools. They actively shape our understanding of data and influence the kinds of knowledge we can create. By acknowledging this role and designing systems with constructivist principles in mind, developers can create more effective, transparent, and empowering tools.</p><h2 id="references">References</h2>
<p>Sarkar, A. (2016). <em>Constructivist design for interactive machine learning</em>. In <em>Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em> (pp. 1467–1475). ACM. <a href="https://doi.org/10.1145/2851581.2892547">https://doi.org/10.1145/2851581.2892547</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>When Spreadsheets Sing: The Fusion of Music and Programming</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/when-spreadsheets-sing-the-fusion-of-music-and-programming.html"/>
        <id>https://advaitsarkar.github.io/autoblog/when-spreadsheets-sing-the-fusion-of-music-and-programming.html</id>

        <updated>2025-04-10T15:18:07+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down a research paper that explores how spreadsheets can be transformed into accessible tools for music&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down a research paper that explores how spreadsheets can be transformed into accessible tools for music programming and data sonification. The paper introduces “SheetMusic,” a prototype that integrates musical sound effects into the familiar spreadsheet environment, examining whether such a tool should be considered a musical instrument, a programming language, or both. </p><p><strong>Reference</strong>: Sarkar, A. (2016, September). <em>Towards spreadsheet tools for end-user music programming</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 228–231).</p><h2 id="spreadsheets-are-musical-instruments-waiting-to-happen">Spreadsheets Are Musical Instruments Waiting to Happen</h2>
<p>The humble spreadsheet, typically associated with accounting and data analysis, has a secret musical talent that few have recognized. A prototype called “SheetMusic” reveals that beneath the rows and columns of traditional spreadsheets lies potential for creating and manipulating sound in ways that could transform both music composition and data analysis.</p><p>At first glance, SheetMusic looks like any standard spreadsheet application. However, it harbors special functions that allow users to generate musical notes and sequences directly from cells. For example, typing “p(‘c 5’)” plays a single note, while “s([‘c 5’, ‘g 4’, ‘a 4’, ‘e 4’], 1)” plays a sequence of notes.</p><p>“Apart from the play/pause controls, it is largely indistinguishable from a regular spreadsheet,” the paper notes. This familiarity is intentional, leveraging the widespread comfort many non-programmers have with spreadsheet environments.</p><p>This approach sits at the intersection of music and programming. SheetMusic allows users to create simple drum beats with formulas like “if(tick%2==0) p(‘snare’) else p(‘kick’)” which alternates between snare and kick drum sounds on each “tick” or beat.</p><h2 id="the-spreadsheet-format-offers-two-key-advantages-for-musical-coding">The Spreadsheet Format Offers Two Key Advantages for Musical Coding</h2>
<p>The grid-based layout of spreadsheets provides unique benefits that other programming environments might not. According to the paper, “The spreadsheet paradigm is well-known to be an excellent interface for novice end-user programming.” This accessibility means people without formal programming training can create musical compositions through a familiar interface.</p><p>Additionally, the grid format enables what programmers call “rich secondary notation” through cell layout. Users can organize related musical elements visually by grouping them together, separating them with blank cells, or highlighting them with different colors. This visual organization helps create intuitive structure without affecting how the program actually runs.</p><p>The combination of these advantages makes spreadsheets a potentially powerful platform for people to experiment with programmable music without the steep learning curve associated with traditional music programming languages.</p><h2 id="is-sheetmusic-an-instrument-or-a-programming-language">Is SheetMusic an Instrument or a Programming Language?</h2>
<p>The distinction between musical instruments and programming languages becomes blurry when examining tools like SheetMusic. The paper addresses this directly: “A central design question is whether SheetMusic is intended as a composition tool, a musical instrument, or a programming language.”</p><p>This question gets to the heart of how we define musical instruments in the digital age. The paper argues that “the defining characteristic of a musical instrument is a player with intent and agency to musically affect the output of the instrument.” Under this definition, programming environments can indeed function as instruments when users actively manipulate them to create sound.</p><p>What distinguishes SheetMusic from both traditional instruments and simple music playback is its ability to create “music as an interactive, reactive, data-dependent experience.” Unlike a piano that plays the note you press, or a recorded song that plays the same way each time, SheetMusic can generate music that responds to changing data or conditions.</p><h2 id="data-sonification-makes-spreadsheet-information-audible-and-accessible">Data Sonification Makes Spreadsheet Information Audible and Accessible</h2>
<p>One practical application of SheetMusic lies in data sonification, the process of converting data into sound. The paper suggests that “a few SheetMusic formulae could instantly create ‘auditory scatterplots’ or line graphs, where data values are mapped to pitches and played in rapid succession.”</p><p>This approach has been “known to be highly perceptually effective for several types of analysis,” according to the paper. Imagine hearing stock market trends as rising and falling pitches, or rainfall patterns translated into rhythmic intensity. Such auditory representations can reveal patterns that might be missed in visual analyses alone.</p><p>This capability also improves accessibility for visually impaired users, who could hear data trends rather than needing to see them on a chart or graph. The paper highlights that pitch mappings could be “tailored specifically to the data domain,” such as changing octaves or keys at important thresholds within the data.</p><h2 id="time-flows-differently-in-musical-spreadsheets">Time Flows Differently in Musical Spreadsheets</h2>
<p>Traditional musical notation and digital sequencers typically represent time explicitly, with measures, beats, and notes arranged in chronological order. SheetMusic takes a different approach by decoupling time from the physical layout of the spreadsheet.</p><p>The paper explains: “In SheetMusic, time passes independently of the spreadsheet, communicating its current value to all the formulae in the spreadsheet once per ‘tick’.” This means users can organize their musical elements based on logical relationships rather than temporal sequence.</p><p>This approach contrasts with other grid-based music tools like Manhattan, which “sacrifices much layout flexibility by committing columns to denote tracks which execute in parallel, and rows to represent time slices which execute sequentially.” SheetMusic prioritizes layout flexibility, “freeing the layout of the spreadsheet for use as arbitrary secondary notation.”</p><h2 id="limitations-that-need-to-be-addressed-in-future-versions">Limitations That Need to Be Addressed in Future Versions</h2>
<p>While the SheetMusic prototype demonstrates interesting possibilities, it remains in the early stages of development with several limitations. The paper acknowledges that “the precise specification of music/sound-generating library is still emergent,” suggesting that the current functionality is limited and experimental.</p><p>The implementation of time as discrete “ticks” may prove inadequate for complex musical compositions requiring precise timing variations. Currently, notes requiring “sub-tick durations” must be expressed through “stretching/squeezing and offset parameters,” which the paper admits is “an inconvenient notation.”</p><p>Additionally, as with any end-user programming tool, there will be a tension between simplicity and expressiveness. While the prototype allows for “arbitrary Javascript code” in each cell, providing tremendous flexibility for advanced users, this might create barriers for novices who lack programming experience.</p><p>The prototype also appears to lack many standard features of music production software, such as mixing capabilities, effects processing, and instrument selection. These would need to be developed for SheetMusic to compete with established music creation tools.</p><h2 id="references">References</h2>
<p>Sarkar, A. (2016, September). <em>Towards spreadsheet tools for end-user music programming</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 228–231).</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Looking Where You Click: How Eye-Tracking Technology Can Replace Your Mouse for Data Analysis</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/looking-where-you-click-how-eye-tracking-technology-can-replace-your-mouse-for-data-analysis.html"/>
        <id>https://advaitsarkar.github.io/autoblog/looking-where-you-click-how-eye-tracking-technology-can-replace-your-mouse-for-data-analysis.html</id>

        <updated>2025-04-10T15:02:19+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article examines a study developing a gaze-directed lens tool for data visualization that allows users to interact with&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article examines a study developing a gaze-directed lens tool for data visualization that allows users to interact with graphs using only their eye movements. The research shows that eye-tracking interfaces can be as effective as traditional mouse controls while encouraging more focused exploration of key data points. </p><p><strong>Reference</strong>: Chander, A., &amp; Sarkar, A. (2016, September). <em>A gaze-directed lens for touchless analytics</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 232–241).</p><h2 id="eye-tracking-technology-turns-your-gaze-into-a-powerful-analytical-tool">Eye-tracking technology turns your gaze into a powerful analytical tool</h2>
<p>Imagine exploring complex data visualizations without ever touching a mouse or keyboard. Your eyes naturally move to interesting parts of a graph - what if the computer could follow your gaze and automatically zoom in on whatever captures your attention? This isn’t science fiction; it’s the reality of gaze-directed interfaces for data analysis.</p><p>The study developed a specialized lens tool that follows your eye movements to help explore data visualizations. The technology addresses situations where traditional input devices are impractical, such as “shared public display walls, where each individual user cannot be given mice and keyboards, and touchscreens cannot be implemented for reasons of cost and robustness; or operating theatres, where sterility is a primary concern.”</p><p>This approach treats data visualization as a form of “end-user programming” - where each interaction with the visualization tool produces a new representation of data. The researchers created specialized magnification and filtering lenses that respond to eye movements rather than mouse clicks.</p><h2 id="the-challenge-of-eye-tracking-is-knowing-when-youre-looking-versus-controlling">The challenge of eye tracking is knowing when you’re looking versus controlling</h2>
<p>A fundamental challenge in eye-tracking interfaces is what researchers call the “gaze multiplexing problem” or the “Midas touch problem.” The system must determine whether you’re simply looking at something or actively trying to interact with it.</p><p>“Eye tracking applications have to guess whether the user is reading, intends for a lens to move, or engage a selection, etc., and act accordingly,” the paper explains. “It is tricky to infer intent from eye movements alone; for instance, when trying to read a label placed along the edge of a gaze-driven lens, the user might inadvertently move the lens itself because the centre of their gaze has shifted.”</p><p>Previous solutions to this problem often relied on “temporal multiplexing” - waiting for a certain amount of time (dwell time) before triggering an action. But this approach feels clunky and unnatural. Others used multimodal inputs, combining eye tracking with other controls like head movements or keyboards, sacrificing the touchless nature of pure eye-tracking interfaces.</p><h2 id="the-flat-lens-design-solves-the-problem-of-accidental-interactions">The flat lens design solves the problem of accidental interactions</h2>
<p>To overcome these challenges, the researchers created a “flat lens” design with two concentric boxes. The inner box shows what part of the graph is being magnified, while a translucent outer box displays the magnified content. </p><p>This design solves the gaze multiplexing problem because both boxes share a common center: “With a common centre, gaze location signals the user’s unambiguous intent to magnify a region and inspect it, greatly alleviating the gaze multiplexing problem.”</p><p>The lens can perform different functions, such as magnification or selective labeling (only showing labels within the lens area to reduce clutter in dense visualizations). But the raw data from eye-tracking is extremely jittery - if the lens followed every tiny eye movement exactly, it would be unusable.</p><h2 id="dynamic-exponential-smoothing-makes-eye-movements-usable-as-input">Dynamic Exponential Smoothing makes eye movements usable as input</h2>
<p>Raw eye-tracking data is notoriously jumpy. Even when you think you’re staring at a fixed point, the recorded gaze coordinates can jump around by up to 50 pixels. Standard smoothing techniques like exponential averaging couldn’t balance stability with responsiveness - either the lens moved too slowly when you wanted to look somewhere else, or it jiggled too much when you tried to inspect something within the lens.</p><p>The solution was an algorithm called Dynamic Exponential Smoothing (DES), which adapts to the user’s behavior. When you’re examining something within the lens, tiny eye movements are smoothed out to keep the lens stable. But when you look at a completely different part of the graph, the algorithm detects the larger change and moves the lens more quickly.</p><p>“DES is based on the principle that large shifts in the quantity being controlled should cause the controller to accelerate faster than small shifts… but unlike [other control methods], DES is altered to better fit the specific requirements of gaze data smoothing,” the paper explains.</p><h2 id="people-were-just-as-fast-with-their-eyes-as-with-a-mouse">People were just as fast with their eyes as with a mouse</h2>
<p>To test their system, the researchers conducted an experiment with 11 undergraduate students who performed analytical tasks using both the eye-tracking lens and a traditional mouse-controlled lens. The tasks involved finding specific features on graphs, like peaks, troughs, and inflection points.</p><p>The results were promising: “Using log-normalised times, a non-inferiority test… gives a confidence level of 94.4%… This demonstrates that the gaze-directed interface is strictly not inferior in efficiency when compared to using a mouse.” In fact, participants completed tasks slightly faster with the eye-tracking interface, spending an average of 0.115 seconds less per question.</p><p>More importantly, participants spent more time investigating key regions of the graphs when using the eye-tracking interface: “Participants spent a median of 2.56s more time investigating key regions with the eye tracker than with the mouse.”</p><h2 id="eye-tracking-interfaces-encourage-more-focused-data-exploration">Eye-tracking interfaces encourage more focused data exploration</h2>
<p>A noteworthy finding was how the eye-tracking interface changed user behavior. When using the mouse, participants spent only about 35.2% of their time investigating the key regions of graphs. With the eye-tracking interface, this increased to 48%.</p><p>This suggests that eye-tracking interfaces may not just match mouse interfaces in efficiency - they might actually improve how people interact with data. By creating a more direct connection between what catches your attention and what gets magnified, the eye-tracking lens encourages more focused exploration.</p><p>The research has limitations worth noting. The sample size of only 11 participants was relatively small, and all were undergraduate students at the same university. The tasks were also fairly simple, involving finding specific features on graphs, rather than complex analytical work. More extensive testing with diverse user groups and more complex analytical tasks would be needed to fully validate the approach.</p><p>Nevertheless, the study demonstrates the potential of touchless, gaze-directed interfaces for data visualization and analysis. As data visualization tools become more widespread and computing extends beyond traditional desktop environments, such interfaces could make interactive data analysis more accessible in a variety of settings.</p><h2 id="references">References</h2>
<p>Chander, A., &amp; Sarkar, A. (2016, September). <em>A gaze-directed lens for touchless analytics</em>. In <em>Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG 2016)</em> (pp. 232–241).</p>
            ]]>
        </content>
    </entry>
</feed>
