<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>Talking about papers</title>
    <link href="https://advaitsarkar.github.io/autoblog/feed.xml" rel="self" />
    <link href="https://advaitsarkar.github.io/autoblog" />
    <updated>2025-04-22T11:04:56+01:00</updated>
    <author>
        <name>TAP Communications</name>
    </author>
    <id>https://advaitsarkar.github.io/autoblog</id>

    <entry>
        <title>How Smart Spreadsheets That Infer Unit Information Can Prevent Costly Errors</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/how-smart-spreadsheets-that-infer-unit-information-can-prevent-costly-errors.html"/>
        <id>https://advaitsarkar.github.io/autoblog/how-smart-spreadsheets-that-infer-unit-information-can-prevent-costly-errors.html</id>

        <updated>2025-04-22T11:04:56+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the research findings on automatic unit inference in spreadsheets - a method to detect when&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the research findings on automatic unit inference in spreadsheets - a method to detect when calculations mix incompatible measurements like dollars and kilometers. The study introduces a novel approach combining logical constraint solving and machine learning to determine the units of spreadsheet values without requiring users to manually annotate them. Based on the paper “Understanding and Inferring Units in Spreadsheets”, this exploration reveals how attention investment theory can justify the benefits of automated unit checking despite inevitable imperfections.</p><p>Reference: Williams, J., Negreanu, C., Gordon, A. D., &amp; Sarkar, A. (2020). Understanding and inferring units in spreadsheets. In 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 1-9). <a href="https://doi.org/10.1109/VL/HCC50065.2020.9127254">https://doi.org/10.1109/VL/HCC50065.2020.9127254</a></p><h2 id="spreadsheet-errors-could-cost-you-but-units-might-save-you">Spreadsheet Errors Could Cost You, But Units Might Save You</h2>
<p>Imagine adding the cost of a laptop ($1,500) to its weight (2 kg). The result - $1,502 - is mathematically valid but conceptually nonsensical. Now imagine this happening in a corporate financial spreadsheet determining your company’s future budget or a scientific model calculating medication dosages.</p><p>Units matter. They’re the invisible guardrails that keep our numbers meaningful. Yet spreadsheets typically don’t track whether a cell contains dollars, kilograms, or seconds.</p><p>“Numbers in spreadsheets often have units: metres, grams, dollars, etc. Spreadsheet cells typically cannot carry unit information, and even where they can, users may not be motivated to provide it,” note the researchers. “However, unit information is extremely valuable: it allows us to detect and prevent an entire class of spreadsheet errors, such as accidentally adding values of different units.”</p><p>Previous research has estimated that dimension errors (mixing different types of measurements) occur in a whopping 42.5% of spreadsheets. These aren’t just academic concerns - real organizations have encountered difficulties due to spreadsheet errors that unit checking could have prevented.</p><h2 id="unit-checking-requires-costly-attention-investment-from-users">Unit Checking Requires Costly Attention Investment From Users</h2>
<p>The problem is that traditional unit checking systems require users to manually label every cell’s unit - a task few people would voluntarily undertake.</p><p>“We formally define and measure the annotation burden this method incurs, and find that it requires significant effort from the user,” the paper explains.</p><p>The researchers analyzed thousands of spreadsheets from the ENRON and EUSES datasets. They found that even after applying existing unit inference techniques, users would still need to manually annotate an average of 19-87 cells per spreadsheet. Even with a streamlined interface, this represents several minutes of dedicated effort - a significant barrier to adoption.</p><p>This is where the concept of “attention investment” becomes relevant. Attention investment theory suggests users will only invest their mental effort if they believe the expected payoff exceeds the cost. Asking users to spend minutes annotating units upfront, in hopes of possibly preventing future errors, is a tough sell.</p><h2 id="artificial-intelligence-can-infer-units-from-context-clues">Artificial Intelligence Can Infer Units From Context Clues</h2>
<p>The innovation in this research lies in applying machine learning to dramatically reduce the annotation burden. The system examines three types of clues:</p><ol>
<li>Number formats (like currency symbols)</li>
<li>Formulas that relate cells mathematically</li>
<li>Textual labels near the numbers</li>
</ol>
<p>The first two techniques were already established in previous research. The breakthrough comes from using natural language processing to understand indirect references to units in text labels.</p><p>For example, a header reading “Credit Card Charges” doesn’t explicitly mention currency, but clearly implies the values represent money. Using word embeddings (a technique that captures semantic relationships between words), the system can make this connection.</p><p>“To address this problem we extend our unit inference algorithm with a novel dimension inference phase that uses pre-trained word embeddings to predict a distribution over dimensions,” the paper explains. “Using our dimension inference phase we increase recall of unit inference by a factor of 30%.”</p><h2 id="machine-learning-isnt-perfect-but-the-tradeoff-is-worth-it">Machine Learning Isn’t Perfect, But The Tradeoff Is Worth It</h2>
<p>The researchers make an interesting observation: imperfect automated unit checking can still be beneficial if it makes fewer errors than humans would without assistance.</p><p>They developed a mathematical model combining attention investment theory with principles from mixed-initiative systems (where humans and computers can both take actions on data). Their analysis concludes that automatic unit inference is beneficial as long as its error rate stays below the natural rate of unit errors without assistance (42.5%).</p><p>“In order for an inference system to lower the expected attentional cost to the user, the rate of inference error must be less than the natural rate of the error that the system is designed to prevent,” they write.</p><p>Their system achieves 49.7% precision in unit prediction and 65.2% precision in dimension prediction - exceeding this threshold and justifying the approach despite its imperfections.</p><h2 id="human-knowledge-still-outperforms-artificial-intelligence-in-some-cases">Human Knowledge Still Outperforms Artificial Intelligence In Some Cases</h2>
<p>When tested against human evaluators on a task of inferring dimensions from textual headers, the machine learning system performed comparably to humans overall. However, detailed analysis revealed some interesting patterns.</p><p>Humans excelled at detecting mass, length, speed, and area (70-80% accuracy), but struggled with volume, power, percentages, and energy measurements (below 50% accuracy). The AI system showed more consistent performance across different dimension types.</p><p>The researchers identified three categories where their system struggled:</p><ul>
<li>Headers containing “distractor words” (like “Average Life of Loan” being misclassified as currency due to “Loan”)</li>
<li>Headers requiring domain knowledge (like knowing that “Gross Domestic Product” relates to currency)</li>
<li>Language understanding issues suggesting limitations in the word embeddings</li>
</ul>
<p>These limitations suggest that while automated unit inference can significantly reduce errors, human oversight remains valuable.</p><h2 id="spreadsheet-safety-doesnt-require-perfection-just-better-than-human-error">Spreadsheet Safety Doesn’t Require Perfection, Just Better Than Human Error</h2>
<p>This research underscores an important principle in human-computer interaction: assistive systems don’t need to be perfect to be valuable. They just need to reduce errors compared to unassisted humans.</p><p>“The criterion derived suggests that an inference precision of 57.5% is the minimum sufficient to justify the attention investment trade-off. Our precision for direct and indirect inference exceeds this threshold, but there is clear opportunity for improvement,” the paper concludes.</p><p>For everyday spreadsheet users, this suggests that future versions of Excel or Google Sheets might someday automatically flag potential unit errors without requiring tedious manual annotations. The technology could catch your mistakes before they become costly errors, making our numeric world a little safer.</p><h2 id="references">References</h2>
<p>Williams, J., Negreanu, C., Gordon, A. D., &amp; Sarkar, A. (2020). Understanding and inferring units in spreadsheets. In 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 1-9). <a href="https://doi.org/10.1109/VL/HCC50065.2020.9127254">https://doi.org/10.1109/VL/HCC50065.2020.9127254</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Correspondence-Based Analogies Allow Computers to Choose Better Problem Representations</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/correspondence-based-analogies-allow-computers-to-choose-better-problem-representations.html"/>
        <id>https://advaitsarkar.github.io/autoblog/correspondence-based-analogies-allow-computers-to-choose-better-problem-representations.html</id>

        <updated>2025-04-22T10:55:37+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a research paper that explores a new computational method for recommending appropriate&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a research paper that explores a new computational method for recommending appropriate representations of problems using “correspondences” - conceptual links between different ways of presenting the same information. The paper proposes a framework that discovers and applies these correspondences to help computers recommend the best representation for solving particular problems.</p><p>Reference: Stockdill, A., Raggi, D., Jamnik, M., Garcia, G. G., Sutherland, H. E. A., Cheng, P. C.-H., &amp; Sarkar, A. (2020). Correspondence-based analogies for choosing problem representations. In 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 1-5). <a href="https://doi.org/10.1109/VL/HCC50065.2020.9127258">https://doi.org/10.1109/VL/HCC50065.2020.9127258</a></p><h2 id="why-the-way-we-represent-a-problem-can-make-it-easy-or-impossible-to-solve">Why the way we represent a problem can make it easy or impossible to solve</h2>
<p>Imagine you’re faced with a challenge: find a formula for the sum of all integers from 1 to n. You could write this using standard mathematical notation: ∑(i=1 to n) i. This is compact and precise, but doesn’t give you many clues about how to solve it.</p><p>Now imagine representing the same problem differently - as rows of dots. One dot for 1, two dots for 2, and so on. Suddenly, you might notice something: these dots form a triangle. And if you reflect that triangle, you get a rectangle with n rows and n+1 columns. Count the dots, divide by two (since you doubled the triangle), and voilà - your formula is n × (n + 1) ÷ 2.</p><p>This example from the paper illustrates something profound: the way we represent a problem can make it either obvious or obscure to solve. As the authors note, “the representation of a problem, be it through algebra, diagrams, or code, is key to understanding and solving it.”</p><h2 id="correspondences-link-different-representations-through-structural-similarities">Correspondences link different representations through structural similarities</h2>
<p>At the heart of this research lies the concept of “correspondences” - formal links between elements in different representational systems. A correspondence connects a property in one system to a property in another, along with a measure of similarity called “strength.”</p><p>For example, in our sum problem, there’s a correspondence between the numeral “2” in algebra and “◦◦” (two dots) in the visual representation. Another correspondence links the summation symbol “∑” with “stacking” dots either horizontally or vertically.</p><p>The paper formalizes correspondences as triples: ⟨p₁, p₂, s⟩, where p₁ and p₂ are property formulas (which can be combined using AND, OR, and NOT), and s is a strength value between 0 and 1. When s = 0, the properties are unrelated; when s = 1, they’re perfectly corresponding.</p><p>As the authors explain: “A correspondence associates an encoding of informational content in one representational system to an encoding of content in another representational system, with a measure of how similar the content is.”</p><h2 id="how-correspondences-are-discovered-through-four-logical-rules">How correspondences are discovered through four logical rules</h2>
<p>One of the paper’s key contributions is a set of rules for discovering correspondences:</p><ol>
<li>Identity: Properties with the same kind and value correspond perfectly</li>
<li>Reversal: Flipping the direction of an existing correspondence </li>
<li>Composition: Chaining correspondences together</li>
<li>Relation: Using internal structure to suggest “parallel” correspondences</li>
</ol>
<p>For example, given a correspondence between “1” and “◦” (a single dot), the reversal rule creates a correspondence from “◦” back to “1”. Then, using the relation rule with the attribute “hasType”, correspondences between “number” and “dot-arrangement” types can be discovered.</p><p>“Applying rules in this way discovers new possible correspondences,” the paper notes. “A richer set of properties would allow for more rule applications and more discoveries.”</p><h2 id="correspondence-strength-is-calculated-using-probability-theory">Correspondence strength is calculated using probability theory</h2>
<p>The paper takes a probabilistic approach to calculating correspondence strength. Rather than simply stating that two properties correspond, the framework quantifies how strongly they correspond based on how often they appear together in analogous problems.</p><p>Specifically, strength is defined as the increase in probability of observing property p₂ after seeing property p₁, relative to the maximum possible increase:</p><p>s = (Pr(p₂|p₁) - Pr(p₂))/(1 - Pr(p₂))</p><p>When p₁ and p₂ are negatively correlated, the formula is adjusted accordingly.</p><p>This probabilistic foundation allows the framework to handle imperfect analogies. In real life, analogies rarely match perfectly, yet they still provide valuable insights. As the authors note, “analogies can be imperfect, but they still guide our reasoning in surprising and insightful ways.”</p><h2 id="correspondences-provide-both-descriptive-and-constructive-explanations">Correspondences provide both descriptive and constructive explanations</h2>
<p>The research highlights two ways correspondences can function as explanations:</p><p>Descriptive explanations reveal how two existing structures are analogous. If you have both an algebraic and visual representation of the sum problem, correspondences explain the links between them, such as how “∑” relates to stacking dots.</p><p>Constructive explanations suggest what properties an analogous representation should have. Given only the algebraic formulation, correspondences can recommend representing “1” as “◦” and using stacking to represent summation.</p><p>“Such hints could support progress through the problem, and potentially reveal deep insights into numbers and summation,” the paper explains.</p><h2 id="initial-evaluation-shows-correspondences-align-with-expert-opinions">Initial evaluation shows correspondences align with expert opinions</h2>
<p>To test the framework, 11 experts were asked to rate the suitability of different representations for a probability problem. The system’s automated recommendations showed strong correlation with expert opinions (r = 0.89).</p><p>An ablation study revealed that while both property importance and correspondence strength contribute to good recommendations, simply counting the number of matched correspondences provides reasonably good results.</p><p>These results suggest the framework captures important aspects of how experts choose representations, but the study’s small scale limits broad conclusions. The sample size of 11 experts and single test problem suggests caution when interpreting these findings.</p><h2 id="potential-applications-extend-beyond-mathematics">Potential applications extend beyond mathematics</h2>
<p>While developed for mathematical problem solving, the correspondence framework has potential applications in other domains. The paper demonstrates how it could recommend analogous entertainment across media, linking characters in books to characters in films, for instance.</p><p>“Correspondences can thus be used to find analogies between any sets of objects equipped with relations,” the paper notes, highlighting the framework’s generality.</p><p>This generality suggests applications in education, computational creativity, and recommendation systems, though practical implementations would require further research and development.</p><h2 id="references">References</h2>
<p>Stockdill, A., Raggi, D., Jamnik, M., Garcia, G. G., Sutherland, H. E. A., Cheng, P. C.-H., &amp; Sarkar, A. (2020). Correspondence-based analogies for choosing problem representations. In 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 1-5). <a href="https://doi.org/10.1109/VL/HCC50065.2020.9127258">https://doi.org/10.1109/VL/HCC50065.2020.9127258</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>How Spilled Arrays Are Rewriting the Rules of Spreadsheet Logic</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/how-spilled-arrays-are-rewriting-the-rules-of-spreadsheet-logic.html"/>
        <id>https://advaitsarkar.github.io/autoblog/how-spilled-arrays-are-rewriting-the-rules-of-spreadsheet-logic.html</id>

        <updated>2025-04-22T10:52:12+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the key findings of the paper Higher-Order Spreadsheets with Spilled Arrays, which introduces a formal&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the key findings of the paper <em>Higher-Order Spreadsheets with Spilled Arrays</em>, which introduces a formal theory of spilled arrays and gridlets: tools designed to enhance abstraction and reuse in spreadsheets. These innovations give spreadsheets object-oriented programming capabilities, but not without introducing new complexity. This blog explores what these changes mean for users and developers alike.</p><p>Reference: Williams, J., Joharizadeh, N., Gordon, A. D., &amp; Sarkar, A. (2020). Higher-order spreadsheets with spilled arrays. In P. Müller (Ed.), Programming languages and systems (pp. 743–769). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-44914-8">https://doi.org/10.1007/978-3-030-44914-8</a></p><h2 id="spreadsheets-are-quietly-becoming-more-like-programming-languages">Spreadsheets are quietly becoming more like programming languages</h2>
<p>Spreadsheets have long been the duct tape of the data world: messy, indispensable, and surprisingly powerful. But recent changes in how spreadsheets handle arrays, specifically through a mechanism called <em>spilled arrays</em>, are pushing them into the realm of programming languages. </p><p>In typical spreadsheets, when you wanted to reuse logic, you did a little dance: copy, paste, modify. But this is brittle. As the paper puts it, “a user must modify each copy manually, a process that is tedious and error-prone.” The new mechanisms: <em>spilled arrays</em> and <em>gridlets</em>, aim to replace that dance with something more robust and programmable.</p><h2 id="spilled-arrays-change-how-formulas-occupy-space-on-the-grid">Spilled arrays change how formulas occupy space on the grid</h2>
<p>The term “spilling” might sound like a mess, but it refers to a useful behavior: when a formula returns an array, it can now automatically populate adjacent cells. In Excel or Google Sheets, typing <code>={1, 2, 3}</code> into a cell spills those values horizontally.</p><p>This might seem like a convenience feature. But the authors argue that it’s more than that. They show that the semantics of spilling must be formalized to handle cases where arrays collide or depend on their own spill areas. These are not edge cases; they’re architectural issues. </p><p>For example, if two formulas try to spill into the same cell, a “spill collision” occurs. If a formula spills into cells it also depends on, a “spill cycle” forms. Without care, either one could break the spreadsheet’s core promise: that evaluation is deterministic and acyclic. As the paper states, “preserving determinism and acyclicity… is a challenge.”</p><p>To address this, the authors build what they call the <em>spill calculus</em>: a formal model that predicts and evaluates which formulas can spill and which must fail with an error. Their model iterates over possible outcomes until it reaches a stable state: one where no further changes are necessary and each cell has a deterministic value.</p><h2 id="gridlets-are-the-spreadsheet-equivalent-of-object-oriented-code-reuse">Gridlets are the spreadsheet equivalent of object-oriented code reuse</h2>
<p>If spilled arrays are the new plumbing, then <em>gridlets</em> are the fixtures they support. A gridlet is essentially a live copy-paste: a block of cells that you can reuse elsewhere while keeping a link to the original. Modifications are allowed locally, but updates to the source will propagate unless explicitly overridden.</p><p>This is not just a macro. It’s closer to what programmers call “object inheritance.” Gridlets are defined using a new operator <code>G</code>. For example, <code>=G(A1:C4,B2,7,B3,24)</code> copies the block <code>A1:C4</code> and substitutes new values at <code>B2</code> and <code>B3</code>.</p><p>To make this work, the authors introduce the <em>grid calculus</em>, a formal language that treats entire sheets as values and allows formulas to modify and extract parts of sheets. In their words, “the grid calculus… admits sheets as first-class values.”</p><p>This abstraction lets spreadsheets simulate features of object-oriented languages. Cells become like methods. Sheets become like objects. The formula <code>G</code> is no longer a one-off trick, but part of a higher-order system.</p><h2 id="spilled-arrays-bring-power-but-also-new-pitfalls">Spilled arrays bring power, but also new pitfalls</h2>
<p>As powerful as this is, there are limitations that shouldn’t be ignored.</p><p>The model the authors propose relies on a strict rule set to determine which arrays are allowed to spill. This works well in theory, but real spreadsheet software like Excel and Google Sheets take a more liberal approach. They allow some ambiguity in which formula “wins” during a spill collision. This flexibility can be user-friendly, but it undermines the formal guarantees that the spill calculus provides.</p><p>Moreover, the model assumes a static total ordering of cell addresses to resolve collisions, which can lead to unintuitive results. In some cases, a formula might not spill even though it appears to have room. The paper notes, “a root is prevented from spilling despite the potential spill area being blank.”</p><p>This is a key reason to be cautious: what is formally correct might not always match what users expect. And although the system guarantees convergence for acyclic spreadsheets, it doesn’t handle divergence or cycles explicitly, meaning real-world use may sometimes outpace the model.</p><h2 id="under-the-hood-spreadsheets-are-becoming-object-oriented">Under the hood, spreadsheets are becoming object-oriented</h2>
<p>Perhaps the most intriguing claim made in the paper is that spreadsheets now have a rigorous analogy to object-oriented programming. This is backed by a direct encoding of Abadi and Cardelli’s object calculus into the grid calculus.</p><p>In that translation, sheets become objects, and cell addresses become method names. A new operator, <code>GRID</code>, plays the role of the <code>this</code> keyword in object-oriented languages. The result is a spreadsheet language that supports higher-order manipulation, encapsulation, and even function definitions using just cells and formulas.</p><p>The authors acknowledge this with a degree of restraint, noting simply that “the resemblance runs deep.” But it’s not a small claim. If spreadsheet users are unknowingly writing object-oriented code, then the design of these systems carries implications far beyond formatting and sums.</p><h2 id="the-bottom-line-spreadsheets-are-evolving-into-real-programming-environments">The bottom line: spreadsheets are evolving into real programming environments</h2>
<p>Spilled arrays and gridlets don’t just make spreadsheets more expressive. They change how we think about what spreadsheets are. By formalizing their behavior, the paper lays a foundation for treating spreadsheets as serious, deterministic, compositional languages.</p><p>But with this power comes new complexity. The promise of live abstraction, reusable logic, and functional updates is compelling, but users and developers must be aware of its pitfalls. Especially when the behavior that feels right in the interface diverges from what the theory says should happen.</p><h2 id="references">References</h2>
<p>Williams, J., Joharizadeh, N., Gordon, A. D., &amp; Sarkar, A. (2020). Higher-order spreadsheets with spilled arrays. In P. Müller (Ed.), Programming languages and systems (pp. 743–769). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-44914-8">https://doi.org/10.1007/978-3-030-44914-8</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Spreadsheets for Symphony: How Spreadsheets Became a Musical Instrument</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/spreadsheets-for-symphony-how-spreadsheets-became-a-musical-instrument.html"/>
        <id>https://advaitsarkar.github.io/autoblog/spreadsheets-for-symphony-how-spreadsheets-became-a-musical-instrument.html</id>

        <updated>2025-04-22T10:41:48+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a paper introducing Excello, a spreadsheet-based music composition environment that transforms the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a paper introducing Excello, a spreadsheet-based music composition environment that transforms the spreadsheet into a tool for creating music. The study explores whether spreadsheet interfaces can support musical creativity, testing the system with 21 musicians of varying experience levels. The research demonstrates that familiar spreadsheet functionality can be repurposed for musical expression in ways that compare favorably with traditional music notation software.</p><p>Reference: Mattinson, H., &amp; Sarkar, A. (2020, July). Excello: Exploring spreadsheets for music composition. In R. Michon &amp; F. Schroeder (Eds.), Proceedings of the International Conference on New Interfaces for Musical Expression (pp. 11–16). Birmingham City University. <a href="https://www.nime.org/proceedings/2020/nime2020_paper2.pdf">https://www.nime.org/proceedings/2020/nime2020_paper2.pdf</a></p><h2 id="the-unlikely-marriage-of-spreadsheets-and-music-opens-new-creative-doors">The unlikely marriage of spreadsheets and music opens new creative doors</h2>
<p>When thinking about music composition tools, the spreadsheet probably doesn’t spring to mind. Yet this everyday office tool might be the next frontier for musical creativity. A research project called Excello has transformed the humble spreadsheet into a surprising music composition environment.</p><p>The concept leverages a simple truth: spreadsheets are already the world’s most popular programming environment. “There are four times more spreadsheet users than software developers,” the paper notes, making spreadsheets a familiar territory for people who might otherwise be intimidated by traditional music programming languages.</p><p>What makes this approach particularly interesting is how it opens music programming to a broader audience. Traditional music programming environments like ChucK or Sonic Pi offer powerful capabilities but present steep learning curves for newcomers. Spreadsheets, on the other hand, are already familiar to millions.</p><h2 id="turtles-carry-the-melody-through-a-sea-of-cells">Turtles carry the melody through a sea of cells</h2>
<p>At the heart of Excello’s design is a clever solution to representing time in a two-dimensional grid. Rather than dedicating one axis to time (as most music sequencers do), Excello employs programmable “turtles” that move through the spreadsheet, playing notes as they go.</p><p>These turtles are inspired by turtle graphics from the Logo programming language, where an agent moves around drawing shapes. In Excello, turtles follow paths defined by simple commands, picking up and playing notes placed in cells along their journey.</p><p>“Notes are written using scientific pitch notation (SPN); e.g. F#4 is the F♯ above middle C,” the paper explains. Empty cells or cells containing a period represent rests, while “s” or “-“ instruct the turtle to sustain the previously played note.</p><p>The turtle’s movements are programmed with straightforward commands: “l” and “r” turn the turtle 90 degrees left and right, “n”, “e”, “s” and “w” orient it to specific compass directions, and “m” moves it forward one cell. Numbers can be attached to commands for repetition—“m4” moves forward four cells. Nested commands in parentheses allow for complex patterns.</p><p>A helpful feature is the automatic path length counter. Instead of manually counting cells, users can simply use “m*” to instruct a turtle to move forward as far as there are notes defined in its path, making composition more fluid.</p><h2 id="excello-leverages-familiar-spreadsheet-features-musicians-never-knew-they-needed">Excello leverages familiar spreadsheet features musicians never knew they needed</h2>
<p>One of Excello’s strengths is how it taps into Excel’s built-in functionality. Cell formatting, copy-paste operations, formula references, and range notation all become tools for musical expression.</p><p>For example, chord input becomes streamlined through a dedicated tool in the interface. When users select cells and choose a chord type, notes are automatically inserted in a pattern that fits the selection—vertically or horizontally. For vertical selections, notes appear from top to bottom in decreasing pitch order, mimicking traditional staff notation.</p><p>Transposition—moving a melody up or down in pitch—is implemented as a spreadsheet function called EXCELLO.MODULATE. This allows users to leverage Excel’s drag-fill functionality to quickly transpose entire sections or build melodies from intervals.</p><p>The power of this approach is demonstrated in the paper’s example of Steve Reich’s “Piano Phase,” a minimalist composition featuring two identical piano melodies playing at slightly different speeds. While other systems require dozens of rows or lines of code to represent this piece, “Excello only requires two cells to define two turtles with different speeds, in addition to the notes.”</p><h2 id="user-testing-shows-excello-compares-favorably-with-professional-notation-software">User testing shows Excello compares favorably with professional notation software</h2>
<p>To evaluate Excello’s usability, the researchers applied a framework called Cognitive Dimensions of Notations (CDN), which assesses how well information systems support different types of thinking. Nineteen participants used Excello over a 7-8 week period, with their experiences compared against Sibelius, a professional music notation program.</p><p>Significantly, across multiple dimensions—closeness of mapping, consistency, secondary notation, viscosity, and visibility—statistical analysis found no significant differences between Excello and Sibelius. This suggests that despite its unconventional approach, Excello’s notation is just as effective as traditional music software for expressing musical ideas.</p><p>“We found no significant difference between Sibelius and Excello, suggesting Excello’s spreadsheet notation has not compromised the closeness of mapping (to the musical domain) of staff notation,” the study reports.</p><p>Users spent their time differently in each system, with more focus on modification and incrementation in Excello compared to Sibelius. However, both systems involved considerable translation activities—converting musical concepts into notation.</p><h2 id="limitations">Limitations</h2>
<p>While the results appear promising, several limitations should be considered. The study involved a relatively small sample of 21 participants, all university students, which may not represent the broader population of potential users. Additionally, the comparative evaluation focused primarily on Sibelius rather than other music programming environments like ChucK or Sonic Pi, which might be more directly comparable in some respects.</p><p>The seven to eight week usage period, while substantial, may not fully capture how users would incorporate Excello into long-term composition workflows. It’s also unclear how well Excello would handle extremely complex compositions or specialized musical notation beyond the examples demonstrated.</p><p>Furthermore, as a prototype add-in for Microsoft Excel, technical limitations and performance issues might arise when scaling to more complex compositions or real-time performance scenarios.</p><h2 id="spreadsheets-might-be-the-unexpected-future-of-accessible-music-programming">Spreadsheets might be the unexpected future of accessible music programming</h2>
<p>Despite these limitations, Excello demonstrates that everyday spreadsheet software can become a viable environment for musical creativity. The research highlights how familiar interfaces can be repurposed to lower barriers to entry for creative programming.</p><p>“Excello provides a simple, yet powerful interface for musical composition and programming to the hundreds of millions of users already familiar with the spreadsheet interface,” the paper concludes.</p><p>Future developments might include live editing during playback and visual enhancements to track turtle movements. Given the widespread availability of spreadsheet software, Excello represents an accessible pathway into music programming for people who might otherwise never experiment with algorithmic composition.</p><p>The next time a spreadsheet is opened to crunch numbers, it might also be used to compose the next minimalist masterpiece.</p><h2 id="references">References</h2>
<p>Mattinson, H., &amp; Sarkar, A. (2020, July). Excello: Exploring spreadsheets for music composition. In R. Michon &amp; F. Schroeder (Eds.), Proceedings of the International Conference on New Interfaces for Musical Expression (pp. 11–16). Birmingham City University. <a href="https://www.nime.org/proceedings/2020/nime2020_paper2.pdf">https://www.nime.org/proceedings/2020/nime2020_paper2.pdf</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Copy, Paste, and the Hidden Struggle of Spreadsheet Reuse: How Gridlets Could Save Your Workday</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/copy-paste-and-the-hidden-struggle-of-spreadsheet-reuse-how-gridlets-could-save-your-workday.html"/>
        <id>https://advaitsarkar.github.io/autoblog/copy-paste-and-the-hidden-struggle-of-spreadsheet-reuse-how-gridlets-could-save-your-workday.html</id>

        <updated>2025-04-22T10:34:12+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the research paper “Gridlets: Reusing Spreadsheet Grids” which explores a novel approach to solving the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the research paper “Gridlets: Reusing Spreadsheet Grids” which explores a novel approach to solving the tedious and error-prone process of updating multiple copies of spreadsheet content. The paper introduces Gridlets as an abstraction that maintains connections between original and copied content while allowing for local modifications, potentially reducing errors and saving time for spreadsheet users. </p><p>Reference: Joharizadeh, N., Sarkar, A., Gordon, A. D., &amp; Williams, J. (2020). Gridlets: Reusing spreadsheet grids. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1–7). Association for Computing Machinery. <a href="https://doi.org/10.1145/3334480.3382806">https://doi.org/10.1145/3334480.3382806</a></p><h2 id="the-spreadsheet-copy-paste-dance-that-everyone-knows-too-well">The Spreadsheet Copy-Paste Dance That Everyone Knows Too Well</h2>
<p>Picture this: you’ve created the perfect mortgage comparison spreadsheet with neat formatting, clever formulas, and a clean layout. You need to compare four different mortgage options, so naturally, you copy and paste your well-designed grid three more times, changing only the interest rates and loan amounts for each scenario.</p><p>Then you spot a formula error in the original. </p><p>Now begins what researchers call “uniform edit” - the exhausting process of locating every copy you’ve made, making the same correction to each one, and carefully preserving all the individual values you previously customized. It’s tedious work that millions of spreadsheet users perform daily, often leading to inconsistencies and errors that can hide in plain sight for months.</p><p>“We present the concept of Gridlets, an abstraction over calculation and presentation applicable in common use case scenarios,” states the paper that proposes a solution to this spreadsheet headache. The research addresses a fundamental tension in spreadsheets - they’re immensely flexible for calculations and layout, but surprisingly rigid when it comes to maintaining relationships between copied regions.</p><h2 id="spreadsheet-reuse-is-everywhere-but-current-methods-come-with-hidden-costs">Spreadsheet Reuse Is Everywhere, But Current Methods Come With Hidden Costs</h2>
<p>The research identifies two major problems with the standard copy-paste approach to spreadsheet reuse that experienced users will immediately recognize:</p><p>“High effort: When the source is edited, cognitive and physical effort is required to recall, locate, and edit every copy,” the paper explains. Anyone who has hunted through multiple tabs and regions to update every instance of a copied calculation knows this pain.</p><p>Even more concerning is the second issue: “High error-proneness: Since uniform edits are applied manually to every copy, there is the potential for error: the edit might be applied inconsistently, and not all copies might be recalled and edited.”</p><p>These aren’t minor inconveniences. Spreadsheet errors have real-world consequences, from budget miscalculations to financial planning mistakes. The research cites previous studies showing that inconsistencies between copied regions are “a major and common problem for spreadsheets.”</p><h2 id="the-gridlet-approach-creates-live-links-between-original-and-copied-content">The Gridlet Approach Creates Live Links Between Original and Copied Content</h2>
<p>The solution proposed is deceptively simple. Gridlets maintain a “live link” between copied regions while still allowing for specific local modifications.</p><p>Instead of using traditional copy and paste, a user would create a Gridlet using a formula that looks something like:
<code>=GRIDLET(A3:B11, B4, 300000, B5, 40000, B6, 40, B7, 0.027)</code></p><p>This formula tells the spreadsheet: “Take this entire region (A3:B11), but replace cell B4 with 300000, B5 with 40000, and so on.” The result looks identical to a pasted region, but with a crucial difference - when the original region changes, all Gridlet copies automatically update while preserving their local customizations.</p><p>For non-technical users, the paper suggests alternative interfaces could make Gridlets accessible without formula writing, such as a “paste as Gridlet” option or a graphical library of reusable regions.</p><h2 id="comparing-gridlets-to-existing-spreadsheet-reuse-methods-shows-clear-advantages">Comparing Gridlets To Existing Spreadsheet Reuse Methods Shows Clear Advantages</h2>
<p>To evaluate the Gridlet concept, the paper employs the Cognitive Dimensions of Notations framework, a set of criteria for analyzing programming interfaces. The comparison reveals that Gridlets offer significant advantages over traditional copy-paste for certain common tasks.</p><p>Spreadsheet users would particularly appreciate improvements in what the framework calls “viscosity” (the effort required to make changes) and “error-proneness” (the likelihood of making mistakes). When making uniform edits across multiple copies, Gridlets substantially reduce both these problem areas.</p><p>“Gridlets, by design, reduce the viscosity and error-proneness of uniform edit in most cases,” the research concludes after detailed analysis.</p><p>Gridlets also outperform another proposed solution called Sheet-Defined Functions (SDFs) in several important areas. While SDFs create reusable “black box” calculations, they don’t preserve the visual layout and formatting that many spreadsheet users rely on to understand their data. Gridlets maintain this “secondary notation” - the colors, borders, and spatial arrangements that help make spreadsheets comprehensible.</p><p>The paper notes: “Layout and formatting are an important and frequently used form of secondary notation. Copy/paste and Gridlets both carry this form of secondary notation, while SDFs do not.”</p><h2 id="gridlets-are-not-perfect-and-require-further-user-testing">Gridlets Are Not Perfect And Require Further User Testing</h2>
<p>Despite their promise, Gridlets aren’t without limitations. The paper acknowledges potential issues with “hidden dependencies” that could be difficult to track, especially in complex spreadsheets where Gridlets might reference other Gridlets. The researchers suggest that notification systems showing these relationships could help mitigate this problem.</p><p>There’s also a learning curve. While the standard copy-paste operation is universally understood, Gridlets require understanding the concept of maintaining linked copies with local modifications - a mental model that might take time for some users to adopt.</p><p>The analysis presented is primarily theoretical, based on heuristic evaluation rather than actual user testing. The researchers acknowledge this limitation, noting, “It is important to note that our analysis is a form of heuristic evaluation that gives us a preliminary indication of the potential tradeoffs, but which should be further substantiated with an empirical user study.”</p><h2 id="spreadsheet-evolution-shows-promise-for-professional-and-casual-users-alike">Spreadsheet Evolution Shows Promise For Professional And Casual Users Alike</h2>
<p>For the millions who use spreadsheets daily, features like Gridlets represent the kind of evolution that could improve productivity and reduce errors. The concept aligns with recent spreadsheet innovations like dynamic arrays and represents a step toward more robust data handling without sacrificing the flexibility that makes spreadsheets so popular.</p><p>The proposal demonstrates how even mature software tools can still be reimagined to solve long-standing pain points. By maintaining the familiar grid-based interface while adding powerful abstraction capabilities, Gridlets could help bridge the gap between programming languages and end-user computing.</p><p>Will your spreadsheets eventually feature Gridlets? Future user studies and software development will determine whether this concept moves from research to reality. But the clear analysis of spreadsheet reuse problems and the thoughtful solution proposed suggest that copy-paste may eventually get a much-needed upgrade.</p><h2 id="references">References</h2>
<p>Joharizadeh, N., Sarkar, A., Gordon, A. D., &amp; Williams, J. (2020). Gridlets: Reusing spreadsheet grids. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1–7). Association for Computing Machinery. <a href="https://doi.org/10.1145/3334480.3382806">https://doi.org/10.1145/3334480.3382806</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>The Hidden Connection Between Spreadsheet Formulas and Programming Skills</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/the-hidden-connection-between-spreadsheet-formulas-and-programming-skills.html"/>
        <id>https://advaitsarkar.github.io/autoblog/the-hidden-connection-between-spreadsheet-formulas-and-programming-skills.html</id>

        <updated>2025-04-22T10:29:07+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a survey study that investigated the relationship between spreadsheet experience and programming&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a survey study that investigated the relationship between spreadsheet experience and programming knowledge. The researchers found that while general spreadsheet experience doesn’t correlate with programming experience, expertise in spreadsheet formulas does show a significant correlation with programming skills. This suggests interesting connections between different forms of computational thinking that might not be immediately obvious to casual spreadsheet users. </p><p>Reference: Sarkar, A., Borghouts, J. W., Iyer, A., Khullar, S., Canton, C., Hermans, F., Gordon, A. D., &amp; Williams, J. (2020). Spreadsheet use and programming experience: An exploratory survey. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1–9). Association for Computing Machinery. <a href="https://doi.org/10.1145/3334480.3382807">https://doi.org/10.1145/3334480.3382807</a></p><h2 id="spreadsheets-are-secretly-teaching-us-how-to-code">Spreadsheets are secretly teaching us how to code</h2>
<p>We’ve all been there. You’re staring at a spreadsheet, trying to figure out how to get that column to calculate the thing you need. Maybe you’re creating a budget, tracking inventory, or analyzing sales figures. Then comes the moment of truth: writing a formula.</p><p>For some, this is where the cold sweat begins. For others, it’s a satisfying puzzle to solve. But what if these spreadsheet skills are more significant than we realize? What if they’re actually building programming muscles we didn’t know we had?</p><p>A recent study explored exactly this question, examining the relationship between spreadsheet experience and programming knowledge. The findings reveal something that might change how we think about those Excel skills on our resumes.</p><h2 id="formula-experts-are-more-likely-to-be-programmers-too">Formula experts are more likely to be programmers too</h2>
<p>The most intriguing discovery from this research is straightforward: people who report high expertise with spreadsheet formulas are significantly more likely to also have experience with traditional programming languages like Python, Java, or JavaScript.</p><p>“We find that formula experience and programming experience are positively correlated (Spearman’s rs = 0.42, p = 2.6 · 10^-3),” the paper reports. This statistical relationship suggests that these seemingly different skills might be more connected than previously thought.</p><p>But here’s the twist. General spreadsheet experience (how comfortable you are with spreadsheets overall) doesn’t show the same strong correlation with programming experience. You can be a spreadsheet wizard without necessarily knowing how to code.</p><p>What’s happening here? The researchers propose several explanations. Perhaps people who already know how to program find it easier to learn formula writing. After all, research shows that “acquiring expertise in a second programming language is considerably easier than the first, due to the pre-existence of suitable mental models.”</p><p>Or maybe it works the other way around. Perhaps those who master spreadsheet formulas develop mental frameworks that make traditional programming more accessible.</p><h2 id="why-this-matters-in-todays-computational-world">Why this matters in today’s computational world</h2>
<p>This finding arrives at a pivotal moment. Coding education is expanding rapidly, from elementary school curriculums to workplace upskilling programs. The researchers note this shift: “It is increasingly common for people to acquire formal experience in computing and programming, whether it is through a computing curriculum at school, after school code clubs, or at university.”</p><p>Yet spreadsheets remain one of the most widespread computational tools in the workplace. They’re the silent workhorses of global business, used by everyone from administrative assistants to CEOs. Understanding how spreadsheet skills relate to broader programming knowledge could reshape how we think about digital literacy and workplace training.</p><p>Consider the implications. If formula expertise transfers to programming skills, then the millions of spreadsheet users worldwide might be developing valuable computational thinking abilities without even realizing it. Spreadsheets could be the unsung heroes of computational literacy.</p><h2 id="the-science-of-measuring-expertise-isnt-straightforward">The science of measuring expertise isn’t straightforward</h2>
<p>Before getting too excited about these findings, some caution is warranted. The study had limitations that are worth considering.</p><p>The sample size was relatively small (49 participants after excluding underrepresented groups) and recruited through convenience and snowball sampling methods. This means the participants were likely not representative of the global population of spreadsheet users.</p><p>Additionally, the expertise measurements relied on self-reporting, which introduces its own uncertainties. As the paper acknowledges, “Self-reported experience measures do have the limitation that respondents may give levels different interpretations; respondents with lower experience may inadvertently place themselves on a higher experience level… and respondents with higher experience may rate themselves lower due to greater self-awareness or modesty.”</p><p>The researchers were careful not to draw population-level conclusions because of these limitations. Instead, they focused on exploring correlations within their specific sample.</p><h2 id="your-spreadsheet-skills-might-reveal-hidden-programming-potential">Your spreadsheet skills might reveal hidden programming potential</h2>
<p>What does this mean for the average spreadsheet user? Perhaps those complex formulas you’ve struggled to master are actually preparing you for a wider world of programming possibilities.</p><p>The paper suggests that certain personality traits might drive both formula and programming expertise: “Aghaee et al. have provided a personality-based account for intrinsic motivation in end-user programming. In particular, their research shows that certain personality profiles (which may be characterised as artistry, bricoleurism, and technophilia) are intrinsically predisposed to programming.”</p><p>In other words, if you enjoy tinkering with spreadsheet formulas, you might have an untapped affinity for programming. That =VLOOKUP() you mastered last week could be signaling latent coding abilities.</p><h2 id="spreadsheet-users-follow-diverse-paths-to-expertise">Spreadsheet users follow diverse paths to expertise</h2>
<p>Another interesting finding is the diversity in how people use spreadsheets. Some participants were highly experienced but used spreadsheets primarily for simpler tasks like maintaining lists or tracking data, tasks that require little formula use.</p><p>“It is possible, therefore, to have high levels of experience using spreadsheets in this manner, without necessarily acquiring expertise in formula authoring (and by extension, in programming),” the researchers note.</p><p>This diversity extends to how people learn spreadsheet skills. Previous research cited in the paper indicates that “spreadsheet learning tends to be goal-driven rather than structured,” and that users often learn “socially through colleagues” rather than through formal training. This informal learning pattern contrasts with traditional programming education, which often follows more structured paths.</p><p>The survey adds to growing evidence that our computational skills develop in complex, interconnected ways. The boundaries between “user” and “programmer” might be fuzzier than we’ve traditionally believed.</p><p>Next time you’re wrestling with a complex spreadsheet formula, take a moment to appreciate that you might be developing skills that extend far beyond that single Excel file. Those formula-writing muscles could be preparing you for a broader computational future.</p><h2 id="references">References</h2>
<p>Sarkar, A., Borghouts, J. W., Iyer, A., Khullar, S., Canton, C., Hermans, F., Gordon, A. D., &amp; Williams, J. (2020). Spreadsheet use and programming experience: An exploratory survey. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1–9). Association for Computing Machinery. <a href="https://doi.org/10.1145/3334480.3382807">https://doi.org/10.1145/3334480.3382807</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>How Computer Systems Can Make Meaningful Recommendations Through Analogies</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/how-computer-systems-can-make-meaningful-recommendations-through-analogies.html"/>
        <id>https://advaitsarkar.github.io/autoblog/how-computer-systems-can-make-meaningful-recommendations-through-analogies.html</id>

        <updated>2025-04-22T10:20:43+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article explores the concept of cross-domain correspondences in recommendation systems that could make computer suggestions more transparent and&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article explores the concept of cross-domain correspondences in recommendation systems that could make computer suggestions more transparent and understandable to users. Breaking down the research paper “Cross-domain Correspondences for Explainable Recommendations”, this piece examines how computers might use analogical reasoning to provide recommendations with clear explanations, similar to how humans naturally make connections between seemingly unrelated domains. The approach offers promising applications in education, product recommendations, and other fields where explanation matters.</p><p>Reference: Stockdill, A., Raggi, D., Jamnik, M., Garcia, G. G., Sutherland, H. E. A., Cheng, P. C.-H., &amp; Sarkar, A. (2020, March). Cross-domain correspondences for explainable recommendations. In Workshop on Explainable Smart Systems and Algorithmic Transparency in Emerging Technologies (ExSS-ATEC), held in conjunction with ACM Intelligent User Interfaces (IUI 2020).</p><h2 id="computers-cant-explain-their-recommendations-but-that-might-change">Computers Can’t Explain Their Recommendations, But That Might Change</h2>
<p>Imagine asking a friend to recommend a book. They might say, “You’ll love this fantasy novel because it reminds me of those strategy games you play - both involve building complex worlds with clear rules.” That’s an analogy in action, connecting two different domains through meaningful similarities.</p><p>Now imagine asking a computer algorithm for a recommendation. What you typically get is: “Based on your preferences, we think you’ll like this.” No explanation of why, no meaningful connection - just a statistical prediction.</p><p>This gap between human and machine recommendation styles might be closing, according to research published in 2020. The paper presents a formal framework for how computers might discover and explain cross-domain correspondences - the meaningful similarities between different areas of knowledge that make recommendations powerful.</p><p>“Humans use analogies to link seemingly unrelated domains,” the paper notes. “A mathematician might discover an analogy that allows them to use mathematical tools developed in one domain to prove a theorem in another. Someone could recommend a book to a friend, based on understanding their hobbies, and drawing an analogy between them.”</p><h2 id="a-mathematical-example-shows-what-makes-cross-domain-analogies-powerful">A Mathematical Example Shows What Makes Cross-Domain Analogies Powerful</h2>
<p>The researchers illustrate their concept with a simple but effective mathematics example. Consider the problem of finding a formula for the sum of integers from 1 to n. In formal mathematics notation, this would be written as:</p><p>∑i from i=1 to n</p><p>While precise, this notation doesn’t give us much intuition about how to solve the problem. But what if we represented those numbers as dots instead?</p><p>The paper describes how representing numbers as rows of dots (1 as one dot, 2 as two dots, etc.) transforms the abstract problem into a visual triangle. This new representation reveals a pattern that leads directly to the solution: n(n+1)/2.</p><p>The key insight: both representations contain the same core information, but they highlight different aspects of the problem. The transformation between them preserves what matters while changing the format to make certain patterns more visible.</p><p>This example demonstrates what the researchers call “correspondences” - the specific ways that elements in one domain map to elements in another. In this case, the number “2” corresponds to “••” (two dots), and the summation symbol corresponds to “stacking dots.”</p><h2 id="correspondence-theory-creates-formal-rules-for-making-analogies">Correspondence Theory Creates Formal Rules For Making Analogies</h2>
<p>At the heart of this research is a formal definition of correspondences as triples containing two “property formulae” and a strength value between 0 and 1. This mathematical formulation allows computers to represent how strongly two concepts correspond across domains.</p><p>The researchers explain, “Correspondences are thus both a heuristic for transformation between representations, and part of a justification for why a particular representation may be suitable for the given problem.”</p><p>What makes this approach particularly valuable is that it creates explanations automatically. When a system recommends switching from algebra to dot diagrams for a particular math problem, it can explain exactly which elements correspond to each other and why this new representation might help.</p><p>These explanations can be descriptive (showing how two existing representations relate) or constructive (suggesting what properties a helpful new representation should have).</p><h2 id="how-computers-might-discover-analogies-automatically">How Computers Might Discover Analogies Automatically</h2>
<p>Perhaps the most intriguing part of the research is the proposed rules for automatically discovering new correspondences between domains. The researchers outline five rules that allow computers to find meaningful connections without human intervention.</p><p>For example, the “rule of identity” recognizes when two properties with the same kind and value should correspond perfectly. The “rule of composition” allows chaining correspondences together: if A corresponds to B, and B corresponds to C, then A might correspond to C with some reduced strength.</p><p>Through these rules, a small set of “seed” correspondences provided by human experts can grow into a rich network of analogical connections that the system discovers on its own.</p><h2 id="recommendation-systems-could-finally-explain-themselves">Recommendation Systems Could Finally Explain Themselves</h2>
<p>The implications extend far beyond mathematical problem-solving. The paper briefly explores how this same framework could transform product recommendation systems.</p><p>Current recommendation algorithms typically work as black boxes. They suggest products based on statistical patterns but can’t explain the meaningful connections that would help users understand and trust the recommendations.</p><p>With correspondence-based recommendations, a system might explain that a science fiction book is recommended because its protagonist corresponds to a character you liked in a movie, or because its world-building style corresponds to elements you enjoy in video games.</p><p>“Correspondences aim to allow a degree of uncertainty through strength while retaining sufficient formality through properties to provide valuable explanations,” the paper states.</p><h2 id="this-approach-needs-additional-development">This Approach Needs Additional Development</h2>
<p>Despite its promise, this research represents an early theoretical framework rather than a complete solution. The authors acknowledge several limitations.</p><p>The discovery rules can generate both false positives (suggesting correspondences that don’t make sense) and false negatives (missing valid correspondences). The system still requires human experts to provide initial “seed” correspondences and validate the discovered ones.</p><p>Additionally, determining the appropriate “strength” of correspondences remains challenging, often requiring human judgment. And while the framework shows promise for educational tools and recommendation systems, it hasn’t yet been extensively tested with real users.</p><p>The research also doesn’t fully address how to handle the complex, multi-faceted nature of real-world recommendations, where dozens of factors might influence a suggestion. Simplifying these relationships for explanation without sacrificing accuracy remains an open challenge.</p><h2 id="references">References</h2>
<p>Stockdill, A., Raggi, D., Jamnik, M., Garcia, G. G., Sutherland, H. E. A., Cheng, P. C.-H., &amp; Sarkar, A. (2020, March). Cross-domain correspondences for explainable recommendations. In Workshop on Explainable Smart Systems and Algorithmic Transparency in Emerging Technologies (ExSS-ATEC), held in conjunction with ACM Intelligent User Interfaces (IUI 2020).</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Public Acceptance of Self-Driving Cars Reveals a Clear Divide Between Partial and Full Autonomy</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/public-acceptance-of-self-driving-cars-reveals-a-clear-divide-between-partial-and-full-autonomy.html"/>
        <id>https://advaitsarkar.github.io/autoblog/public-acceptance-of-self-driving-cars-reveals-a-clear-divide-between-partial-and-full-autonomy.html</id>

        <updated>2025-04-16T14:59:49+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article examines public perception of autonomous vehicles based on a 2019 research paper that introduced the Autonomous Vehicle&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article examines public perception of autonomous vehicles based on a 2019 research paper that introduced the Autonomous Vehicle Acceptance Model (AVAM). The study reveals that people perceive only two meaningful levels of autonomy: partial and full, with significantly lower acceptance of fully autonomous vehicles. The findings suggest that automotive manufacturers and technology companies may need different approaches when developing and marketing self-driving cars.</p><p>Reference: Hewitt, C., Politis, I., Amanatidis, T., &amp; Sarkar, A. (2019). Assessing public perception of self-driving cars: The autonomous vehicle acceptance model. In Proceedings of the 24th International Conference on Intelligent User Interfaces (IUI ‘19) (pp. 518–527). ACM. <a href="https://doi.org/10.1145/3301275.3302268">https://doi.org/10.1145/3301275.3302268</a></p><h2 id="the-road-to-autonomous-vehicles-is-paved-with-public-skepticism">The Road to Autonomous Vehicles Is Paved with Public Skepticism</h2>
<p>Imagine stepping into a car with no steering wheel, no pedals, and absolutely no way for you to take control if something goes wrong. How would you feel? According to recent research, probably not great.</p><p>As self-driving cars move from science fiction to reality, understanding public perception becomes crucial for their successful adoption. A study published in 2019 introduced a standardized framework called the Autonomous Vehicle Acceptance Model (AVAM) to measure how people view autonomous vehicles across different levels of automation.</p><p>“Most existing studies of public perception of AVs do not use established models of User Acceptance (UA). They are therefore difficult to make comparisons between or interpret in terms of UA,” the paper notes, highlighting a key gap the research aimed to fill.</p><p>The study examined responses from 187 participants across all six levels of vehicle autonomy defined by the Society of Automotive Engineers (SAE), ranging from Level 0 (fully manual) to Level 5 (fully autonomous with no human intervention required).</p><h2 id="user-acceptance-decreases-as-autonomy-increases">User Acceptance Decreases as Autonomy Increases</h2>
<p>The research findings contradict what might seem intuitive: people actually reported lower expected performance and more difficulty using vehicles as autonomy increased. One might expect that more autonomous vehicles would be perceived as easier to use and more efficient, but the opposite proved true.</p><p>“Not only were participants more anxious about higher levels of autonomy, but they also reported lower expected performance and lower perceived ease-of-use,” the study reports. “This is directly opposed to what we might expect based on the motivating factors in the development of AV.”</p><p>Across nearly all measured factors, including perceived safety, attitude toward the technology, and intention to use the vehicles, participants showed a clear preference for lower levels of autonomy. Anxiety increased significantly with higher autonomy levels, while perceived safety decreased.</p><p>This reluctance isn’t entirely surprising given the novelty of the technology. New innovations often face initial skepticism, especially when they involve removing human control from potentially dangerous situations. The researchers suggest that “more work is needed by experts to clarify the capabilities and limitations of AVs in order to increase trust in the technology.”</p><h2 id="people-see-only-two-real-categories-partial-and-full-automation">People See Only Two Real Categories: Partial and Full Automation</h2>
<p>Perhaps the most intriguing finding was how people conceptualized different levels of autonomy. Despite the industry’s detailed six-level classification system, participants essentially perceived only two categories: partially autonomous (Levels 0-4) and fully autonomous (Level 5).</p><p>This was particularly evident when participants were asked about expected physical engagement with the vehicle. For Levels 0 through 4, participants rated the importance of using hands, feet, and eyes as consistently high, with only minor variations. But for Level 5, there was a dramatic drop in the perceived need for all three forms of engagement.</p><p>“The most likely explanation for this is that users perceive only two levels of autonomy: partial and full, without being able to differentiate well between different levels of partial autonomy,” the paper explains.</p><p>This finding has significant implications for how autonomous vehicles should be designed and marketed. It suggests that incremental improvements in autonomy between Levels 1-4 may not meaningfully change how users interact with or perceive these vehicles.</p><h2 id="the-industry-divide-reflects-public-perception">The Industry Divide Reflects Public Perception</h2>
<p>The study provides an interesting perspective on the current divide in autonomous vehicle development strategies. Some companies, like Tesla and Audi, are introducing autonomous features incrementally, addressing all levels of autonomy. Others, like Waymo and Uber, are aiming directly for Level 5 full autonomy.</p><p>“Technology companies such as Waymo and Uber aiming to directly develop and introduce L5 autonomous vehicles in service of any individual, irrespective of whether they hold a driving licence. In contrast, automotive manufacturers such as Tesla or Audi are aiming to introduce autonomous features incrementally,” the paper notes.</p><p>This industrial split may actually reflect the public’s binary perception of autonomy. If people don’t meaningfully distinguish between different levels of partial autonomy, the incremental approach might not yield the expected benefits in terms of user acceptance and market adoption.</p><h2 id="limitations-of-the-research-should-be-considered">Limitations of the Research Should Be Considered</h2>
<p>Several limitations should be noted when interpreting these results. The study participants were all from the United States and were recruited through Amazon Mechanical Turk, which may have resulted in a more technology-oriented sample than the general population. Additionally, the researchers acknowledge that question order may have introduced some bias, though they defend this choice as helping participants “more easily envision vehicles when the technologies featured became incrementally more hypothetical.”</p><p>The study also primarily sampled current drivers, which might not reflect the views of non-drivers who might benefit most from fully autonomous vehicles. As the researchers note, “It might be interesting to assess the opinions of current non-drivers instead of existing drivers, who constitute the majority of the sample considered.”</p><h2 id="future-research-will-need-to-track-changing-perceptions">Future Research Will Need to Track Changing Perceptions</h2>
<p>Understanding and monitoring public perception will remain crucial as autonomous vehicle technology evolves. The researchers suggest that the AVAM provides a standardized framework that can be used to track changes in perception over time and across different cultures.</p><p>“The AVAM opens up scope for evaluation of many more causal factors of UA [User Acceptance] than are considered here,” the paper states, suggesting future studies could break down results by age, gender, driving experience, and cultural background.</p><p>As autonomous vehicles become more common on roads worldwide, these perceptions will likely shift. The key question remains whether the technology will evolve to meet user expectations, or whether user perceptions will adapt to embrace the new technology.</p><h2 id="references">References</h2>
<p>Hewitt, C., Politis, I., Amanatidis, T., &amp; Sarkar, A. (2019). Assessing public perception of self-driving cars: The autonomous vehicle acceptance model. In Proceedings of the 24th International Conference on Intelligent User Interfaces (IUI ‘19) (pp. 518–527). ACM. <a href="https://doi.org/10.1145/3301275.3302268">https://doi.org/10.1145/3301275.3302268</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>When Less Is More: Restricted Language May Improve User Experience in AI Chat Interfaces</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/when-less-is-more-restricted-language-may-improve-user-experience-in-ai-chat-interfaces.html"/>
        <id>https://advaitsarkar.github.io/autoblog/when-less-is-more-restricted-language-may-improve-user-experience-in-ai-chat-interfaces.html</id>

        <updated>2025-04-16T14:42:29+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down a research study examining whether restricting users to a simplified language interface might actually improve&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down a research study examining whether restricting users to a simplified language interface might actually improve their experience compared to allowing unrestricted natural language input. The researchers tested this hypothesis using an interactive language learning game in a 3D blocks world, finding that restricting communication to predefined tokens led to better task performance and improved user experience metrics.</p><p>Reference: Mu, J., &amp; Sarkar, A. (2019). Do we need natural language?: Exploring restricted language interfaces for complex domains. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (CHI EA ‘19) (pp. LBW2822:1–LBW2822:6). ACM. <a href="https://doi.org/10.1145/3290607.3312975">https://doi.org/10.1145/3290607.3312975</a></p><h2 id="natural-language-interfaces-promise-simplicity-but-often-frustrate-users">Natural language interfaces promise simplicity but often frustrate users</h2>
<p>We’ve all been there. “Alexa, play that song about rivers and roads.” <em>Confused robot noises.</em> “Alexa, play that song… you know… RIVERS AND ROADS?” <em>Alexa plays ‘Old Town Road’</em></p><p>The promise of natural language interfaces (NLIs) - systems where humans can talk to computers using everyday language - seems like the pinnacle of intuitive design. Just tell the computer what you want in your own words! No learning curve! No manuals!</p><p>But the reality often fails to match this promise. Current natural language systems frequently misunderstand users, leading to frustration and disappointment. As the paper notes, “several studies have demonstrated worrying user experience issues with current-generation personal assistants due to mismatches between user expectations and system capabilities.”</p><p>The fundamental problem is that truly understanding human language remains extraordinarily difficult for computers. When an interface suggests you can communicate naturally, it creates expectations that the system may not be able to fulfill.</p><h2 id="the-experiment-used-a-blocks-world-game-to-test-restricted-versus-unrestricted-language">The experiment used a blocks-world game to test restricted versus unrestricted language</h2>
<p>To test whether restricting user language might actually improve experience, the researchers employed an interactive language learning game called SHRDLURN (a nod to the classic SHRDLU system from the 1970s). In this game, users need to manipulate colored blocks from a starting configuration to a goal configuration by issuing text commands to the computer.</p><p>The game was intentionally designed so users would need to develop a communication system with the computer from scratch. The computer uses a learning algorithm to interpret commands, improving its understanding as more commands are issued.</p><p>Participants were split into two groups:</p><ul>
<li>An unrestricted group who could type any commands they wanted</li>
<li>A restricted group who could only use 11 specific tokens matching the system’s internal logical primitives (all, cyan, red, brown, orange, except, leftmost, rightmost, add, remove, to)</li>
</ul>
<p>What makes this setup fascinating is that it mirrors many real-world interactions with digital assistants, where the underlying capability of the system is hidden, and users must build a mental model of what the system can understand.</p><h2 id="restricted-language-led-to-better-performance-and-reduced-mental-effort">Restricted language led to better performance and reduced mental effort</h2>
<p>Counter to what many would expect, constraining users to a limited vocabulary resulted in the same or better task performance. Restricted participants needed an average of 7.63 scrolls per utterance, compared to unrestricted users’ 12.9 scrolls (scrolling was necessary when the computer misunderstood the command). Though the difference wasn’t statistically significant (p = 0.13), the trend favored the restricted condition.</p><p>More striking were the differences in user experience. On NASA Task Load Index (NASA-TLX) measurements, restricted users reported:</p><ul>
<li>Significantly less required effort (48.8 vs 72.5, p = 0.014)</li>
<li>Significantly higher perceived performance (66.9 vs 39.4, p = 0.005)</li>
<li>Lower mental demand (54.4 vs 75.6, p = 0.06, marginally significant)</li>
</ul>
<p>Frustration levels were similar between groups, suggesting that the restrictions didn’t annoy users as one might expect.</p><h2 id="users-naturally-gravitating-to-simpler-language-performed-better">Users naturally gravitating to simpler language performed better</h2>
<p>Perhaps the most telling finding came from analyzing the communication strategies of participants in the unrestricted condition. The most successful unrestricted users independently developed simple, consistent languages resembling the restricted condition.</p><p>The paper provides examples contrasting successful and unsuccessful users:</p><pre><code>Player 1/8 (Mean scrolls/utterance: 4.08)
add blue blocks to blue blocks
add red block on the last orange stack
remove last red block
remove top orange blocks
remove first red block

Player 8/8 (Mean scrolls/utterance: 28.9)
move nothing
move all but blue
move all but red
remove 5th
remove first
</code></pre>
<p>The high-performing player used a consistent pattern of commands, while the struggling player employed inconsistent terminology and structure. This suggests that even when given free rein, users who naturally adopt simpler, more structured communication fare better.</p><h2 id="the-implications-extend-to-real-world-interface-design">The implications extend to real-world interface design</h2>
<p>The results suggest an interesting middle ground for interface design. Rather than choosing between fully natural language (which is hard to build and creates unrealistic expectations) or traditional interfaces (which require learning curves), a “restricted language interface” might offer the best of both worlds.</p><p>“More generally, rather than considering only two extremes—a specialized programming language or GUI versus a human-level language understanding system—designers should consider ‘restricted’ language interfaces which trade off full expressivity for simplicity, learnability, and consistency,” the paper argues.</p><p>This approach could be particularly valuable for domains with well-defined action spaces, like data analysis, database querying, or robot control. By guiding users toward consistent language patterns that match what the system can understand, designers might create interfaces that not only perform better but also feel better to use.</p><h2 id="the-limitations-suggest-caution-when-generalizing-these-findings">The limitations suggest caution when generalizing these findings</h2>
<p>The study comes with important caveats. The findings apply primarily to text-based interfaces rather than voice, since text input “is more amenable to restriction and shorthand than voice communication.” The results may also be limited to well-specified domains with finite action spaces, unlike open-ended tasks such as general conversation or answering arbitrary questions.</p><p>The sample size was relatively small (16 participants), and the experimental task was quite specific. Additionally, the blocks world represents a simplified environment compared to many real-world applications.</p><p>Nevertheless, the study offers a fascinating counterpoint to the assumption that more freedom in language interfaces always leads to better experiences. Sometimes, a little structure goes a long way.</p><p>For designers of conversational interfaces, the message is clear: don’t try to boil the ocean by supporting every possible way a human might express a command. Instead, consider how to subtly guide users toward language patterns the system can reliably interpret. This guidance might not only make the system easier to build, but also more satisfying to use.</p><h2 id="references">References</h2>
<p>Mu, J., &amp; Sarkar, A. (2019). Do we need natural language?: Exploring restricted language interfaces for complex domains. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (CHI EA ‘19) (pp. LBW2822:1–LBW2822:6). ACM. <a href="https://doi.org/10.1145/3290607.3312975">https://doi.org/10.1145/3290607.3312975</a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Chatbots as Therapists: Why the Human Touch Still Matters in Mental Health Care</title>
        <author>
            <name>TAP Communications</name>
        </author>
        <link href="https://advaitsarkar.github.io/autoblog/chatbots-as-therapists-why-the-human-touch-still-matters-in-mental-health-care.html"/>
        <id>https://advaitsarkar.github.io/autoblog/chatbots-as-therapists-why-the-human-touch-still-matters-in-mental-health-care.html</id>

        <updated>2025-04-16T14:33:22+01:00</updated>
            <summary>
                <![CDATA[
                    Abstract This article breaks down the results of a comparative study examining how people perceive therapy sessions with chatbots versus&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="abstract">Abstract</h2>
<p>This article breaks down the results of a comparative study examining how people perceive therapy sessions with chatbots versus human therapists. The research found that participants rated chatbot therapy sessions as less enjoyable and less smooth than sessions with human therapists, highlighting significant challenges for AI-based mental health interventions. The study suggests important limitations and future directions for research into automated therapeutic approaches. </p><p>Reference: Bell, S., Wood, C., &amp; Sarkar, A. (2019). Perceptions of chatbots in therapy. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (CHI EA ‘19) (pp. LBW1712:1–LBW1712:6). ACM. <a href="https://doi.org/10.1145/3290607.3313072">https://doi.org/10.1145/3290607.3313072</a></p><h2 id="the-mental-health-accessibility-gap-is-driving-innovation-in-digital-therapy">The mental health accessibility gap is driving innovation in digital therapy</h2>
<p>Mental illness affects approximately one-third of the global population during their lifetime, creating an enormous health burden. While effective treatments like Cognitive Behavioral Therapy (CBT) exist, many people struggle to access care due to long wait lists, high costs, and logistical barriers such as transportation and scheduling challenges.</p><p>This accessibility gap has spurred interest in technology-based solutions. Internet-based CBT has shown promising clinical results by eliminating the need for face-to-face sessions. However, a significant bottleneck remains: the limited number of qualified therapists available to provide treatment.</p><p>Enter the chatbot therapist, an AI-powered solution that could potentially scale therapy access dramatically. But a key question remains unanswered: How do people actually experience therapy when it’s delivered by a chatbot instead of a human?</p><h2 id="university-researchers-conducted-a-controlled-experiment-comparing-human-and-ai-therapy">University researchers conducted a controlled experiment comparing human and AI therapy</h2>
<p>To investigate this question, researchers designed a comparative study with 10 participants who self-identified as experiencing subclinical symptoms of stress. These participants were divided into two groups of five people each.</p><p>Group A received two 30-minute sessions of internet-based CBT from a human therapist through a chat interface. Group B believed they were receiving therapy from a chatbot through the same interface, though in reality, the “chatbot” was simulated using a technique called Wizard of Oz, where human operators selected appropriate responses from a predetermined script.</p><p>After each session, participants completed questionnaires measuring their perceptions of the therapy across multiple dimensions, including sharing ease (how comfortable they felt sharing personal information), conversation smoothness, usefulness, and enjoyment. The researchers also conducted qualitative interviews to gather additional insights.</p><h2 id="chatbot-therapy-conversations-were-significantly-less-smooth-than-human-interactions">Chatbot therapy conversations were significantly less smooth than human interactions</h2>
<p>One of the clearest findings from the study concerned conversation flow. Participants who believed they were talking to a chatbot rated their conversations as significantly less smooth than those speaking with human therapists. Group A (human therapist) reported a mean smoothness score of 6.1 out of 7, while Group B (chatbot) scored only 5.0 - a statistically significant difference.</p><p>This lack of conversational flow was frequently mentioned in participant interviews. One participant noted that the chatbot’s responses felt like “repetition of what I said, not an expansion of what I said.” Another complained that the responses seemed generic: “I felt standard answers come back… anybody could say that.”</p><p>The perceived rigidity of chatbot communication appears to be a major drawback, with 60% of participants in the chatbot group spontaneously commenting on the difficulties of chat-based therapy. As one participant put it: “Text is not always as nice as sitting down to something face-to-face, especially with body language.”</p><h2 id="therapy-sessions-with-chatbots-were-perceived-as-less-enjoyable-than-with-humans">Therapy sessions with chatbots were perceived as less enjoyable than with humans</h2>
<p>Another significant finding concerned enjoyment. Participants in the chatbot group reported lower enjoyment levels than those in the human therapist group, a difference that reached statistical significance. When asked to rate their sessions on a scale from “Bad” to “Good,” the human therapist group gave a median rating of 6.0, while the chatbot group gave only 5.0.</p><p>The qualitative interviews shed light on why chatbot sessions might be less enjoyable. Many participants commented on the superficial nature of the chatbot’s responses and its inability to provide detailed or personalized guidance. One participant said the chatbot “suggested keeping a thought journal, but then it didn’t really expand on what it meant.”</p><h2 id="the-empathy-gap-presents-a-fundamental-challenge-for-automated-therapy">The empathy gap presents a fundamental challenge for automated therapy</h2>
<p>Perhaps the most fundamental issue identified in the study was the difficulty chatbots have in conveying empathy and building therapeutic relationships. One participant articulated this challenge clearly:</p><p>“When you tell something to someone, it’s better, because they might have gone through something similar… there’s no sense that the robot cares or understands or empathizes.”</p><p>This highlights a core element of effective therapy: the therapeutic alliance between patient and provider. This relationship is built on trust, empathy, and shared understanding, qualities that current chatbot technology struggles to replicate.</p><p>The study suggests that future research should focus specifically on addressing these limitations. Without developing ways for chatbots to demonstrate empathy, maintain conversational context, and build meaningful relationships with users, fully automated therapy may remain significantly less effective than human-delivered care.</p><h2 id="small-sample-size-limits-the-generalizability-of-these-findings">Small sample size limits the generalizability of these findings</h2>
<p>While this study provides valuable insights, several limitations should be considered when interpreting the results. The most obvious is the small sample size of just 10 participants, which limits statistical power and generalizability. Additionally, participants only engaged in two 30-minute sessions, which doesn’t reflect the longer-term nature of most therapeutic relationships.</p><p>The Wizard of Oz technique used to simulate the chatbot may not perfectly represent the capabilities of actual therapeutic chatbots. The researchers note that “no suitably advanced therapy chatbot exists today,” underscoring the emerging nature of this technology.</p><h2 id="the-future-of-therapy-may-involve-human-ai-collaboration-rather-than-replacement">The future of therapy may involve human-AI collaboration rather than replacement</h2>
<p>Despite these challenges, the potential benefits of chatbot-assisted therapy remain significant. Increased accessibility, lower costs, and 24/7 availability could help address the enormous unmet need for mental health services globally.</p><p>Rather than viewing chatbots as replacements for human therapists, a more promising approach might be combining humans and AI in a therapeutic system. Chatbots could handle initial screening, routine check-ins, and homework assistance, while human therapists focus on building relationships and addressing complex issues.</p><p>The study concludes with a call for research that acknowledges and directly addresses the current limitations of chatbot therapy: “We suggest that future research into chatbot CBT acknowledges and explores these areas of conversational recall, empathy, and the challenge of shared experience, in the hope that we may benefit from scalable, accessible therapy where needed.”</p><p>As technology continues to advance, the gap between human and chatbot therapy may narrow. But this research suggests that for now, the human touch remains an essential component of effective mental health care.</p><h2 id="references">References</h2>
<p>Bell, S., Wood, C., &amp; Sarkar, A. (2019). Perceptions of chatbots in therapy. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (CHI EA ‘19) (pp. LBW1712:1–LBW1712:6). ACM. <a href="https://doi.org/10.1145/3290607.3313072">https://doi.org/10.1145/3290607.3313072</a></p>
            ]]>
        </content>
    </entry>
</feed>
