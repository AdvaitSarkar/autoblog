<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>When Technology Asks &quot;What Do You Want?&quot;: Rule-Based Programming vs. Reinforcement Learning in System Personalization - Talking about papers</title><meta name="description" content="How should smart systems learn what you want? Users weigh in on rules vs. AI feedback in a study on personalized meeting tech."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://advaitsarkar.github.io/autoblog/when-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html"><link rel="alternate" type="application/atom+xml" href="https://advaitsarkar.github.io/autoblog/feed.xml" title="Talking about papers - RSS"><link rel="alternate" type="application/json" href="https://advaitsarkar.github.io/autoblog/feed.json" title="Talking about papers - JSON"><meta property="og:title" content="When Technology Asks 'What Do You Want?': Rule-Based Programming vs. Reinforcement Learning in System Personalization"><meta property="og:site_name" content="Talking about papers"><meta property="og:description" content="How should smart systems learn what you want? Users weigh in on rules vs. AI feedback in a study on personalized meeting tech."><meta property="og:url" content="https://advaitsarkar.github.io/autoblog/when-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html"><meta property="og:type" content="article"><link rel="preload" href="https://advaitsarkar.github.io/autoblog/assets/dynamic/fonts/besley/besley.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="https://advaitsarkar.github.io/autoblog/assets/css/style.css?v=a140eaf7c0c846228243e21f6218751b"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://advaitsarkar.github.io/autoblog/when-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html"},"headline":"When Technology Asks \"What Do You Want?\": Rule-Based Programming vs. Reinforcement Learning in System Personalization","datePublished":"2025-04-16T13:35+01:00","dateModified":"2025-04-16T13:35+01:00","description":"How should smart systems learn what you want? Users weigh in on rules vs. AI feedback in a study on personalized meeting tech.","author":{"@type":"Person","name":"TAP Communications","url":"https://advaitsarkar.github.io/autoblog/authors/anonymous/"},"publisher":{"@type":"Organization","name":"TAP Communications"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><div class="container"><div class="left-bar"><div class="left-bar__inner"><header class="header"><a class="logo" href="https://advaitsarkar.github.io/autoblog/">Talking about papers</a><nav class="navbar"><button class="navbar__toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle__box"><span class="navbar__toggle__inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://advaitsarkar.github.io/autoblog/about.html" target="_self" aria-label="About"><span>About</span></a></li></ul></nav><a class="logo logo--atbottom" href="https://advaitsarkar.github.io/autoblog/">Talking about papers</a></header></div></div><main class="main post"><article class="content"><div class="main__inner"><div class="content__meta"><div class="content__date"><time datetime="2025-04-16T13:35">16 April 2025</time></div></div><header class="content__header"><h1 class="content__title">When Technology Asks &quot;What Do You Want?&quot;: Rule-Based Programming vs. Reinforcement Learning in System Personalization</h1></header><div class="content__entry"><h2 id="abstract">Abstract</h2><p>This article examines the trade-offs between two approaches to personalizing technology systems: explicit rule-based programming versus reinforcement learning through feedback. The results break down a comparative user study that investigated how people respond to and interact with these different personalization methods.</p><p><strong>Reference</strong>: Liu, R., Sarkar, A., Solovey, E., &amp; Tschiatschek, S. (2019, March). Evaluating rule-based programming and reinforcement learning for personalising an intelligent system. In 2nd Workshop on Explainable Smart Systems (ExSS 2019), held in conjunction with ACM Intelligent User Interfaces (IUI 2019).</p><h2 id="smart-technology-personalization-faces-a-fundamental-user-experience-dilemma">Smart technology personalization faces a fundamental user experience dilemma</h2><p>Imagine walking into a meeting room late. You’re a person who is blind or low vision or joining remotely without video. Everyone else immediately sees who’s there, what they’re doing, and the general vibe of the room. You don’t. This information gap can significantly impact your ability to participate effectively.</p><p>Now imagine having a smart device that could tell you exactly what’s happening in that room - but first, you need to teach it what information matters most to you. Would you prefer to write specific rules for what you want to know (“If someone is speaking, tell me who they are”), or would you rather just give thumbs up or down feedback as the system makes guesses about what might be useful?</p><p>This seemingly simple design question sits at the heart of how we interact with increasingly intelligent technologies.</p><p>“Many intelligent systems can be personalised by end-users to suit their specific needs. However, the interface for personalisation often trades off the degree of personalisation achievable with time, effort, and level of expertise required by the user,” notes the paper published in the Proceedings of ACM IUI 2019 Workshops.</p><h2 id="the-study-created-a-simulated-meeting-environment-to-test-personalization-methods">The study created a simulated meeting environment to test personalization methods</h2><p>To explore this question systematically, the research team built a meeting simulator. This generated random meetings with 3-8 attendees, each with specific attributes like their name, pose (sitting/standing), activity (speaking/typing/listening), where they were looking, and their location in the room.</p><p>For the study, 15 participants were asked to personalize a notification system that would provide only the information they cared about when entering a meeting. Each person tried two different approaches: rule-based programming and reinforcement learning.</p><p>With rule-based programming, participants directly specified what information they wanted through if-then rules. For example: “When there is an attendee whose activity is speaking, tell me the name and activity for attendees whose activity is speaking.”</p><p>With reinforcement learning, participants simply gave binary yes/no feedback about whether each notification was useful. Behind the scenes, an AI system learned from this feedback using a neural network to predict what information would be valuable.</p><h2 id="rule-based-programming-produced-higher-satisfaction-but-required-more-understanding">Rule-based programming produced higher satisfaction but required more understanding</h2><p>Participants reported significantly lower cognitive load and higher satisfaction when using rule-based programming compared to reinforcement learning. With rules, people felt more in control. As one participant put it: “Of course I am satisfied, because I chose the rules myself.”</p><p>However, rule-based programming wasn’t without challenges. Some participants struggled to understand the filtered information resulting from their rules. One participant expressed frustration: “I want to know when people are speaking, whether people are listening and what they are doing now, this includes much more information… When I told you to use the rules, it lost too much information.”</p><p>Others found it difficult to maintain multiple rules and understand how they worked together. “So all of these rules are stacking up?” asked one participant. Another wondered: “Okay, the last one was very short, did some of these [rules] override each other?”</p><h2 id="reinforcement-learning-proved-easier-but-slower-and-less-transparent">Reinforcement learning proved easier but slower and less transparent</h2><p>Reinforcement learning presented a different set of challenges. Many participants found it difficult to provide simple yes/no feedback when their preferences were more nuanced. “If I am okay with that, but if I need some new information, should I press yes or no?” asked one participant.</p><p>Participants also worried about giving inconsistent feedback, feeling responsible if the system didn’t learn properly. “It is hard for me to give the feedback. At the beginning, I thought this information is good to understand this meeting. but then after training for a while, I begin to notice that some information is too much for me,” explained one participant.</p><p>Perhaps most notably, participants expressed uncertainty about what the system was capable of learning and what would happen after training. “Does the training, for the whole training period, give me all the options?” asked one participant. Another simply wondered: “So what is its conclusion?”</p><h2 id="the-ideal-personalization-system-might-combine-both-approaches">The ideal personalization system might combine both approaches</h2><p>Interestingly, the majority of participants expressed a preference for a hybrid approach. They wanted the control and transparency of rule-based programming combined with the learning capabilities of reinforcement learning.</p><p>“For people who wanted a combination of the two approaches, all of them expressed a similar idea of being able to control the system’s behaviour by entering or editing rules, while also using a pre-training phase with the reinforcement learning model to help them figure out what they want or to suggest rules,” the paper reports.</p><p>This suggests that the ideal personalization interface might use reinforcement learning to help users discover their preferences and generate suggested rules, which users could then refine directly.</p><h2 id="the-findings-reveal-important-design-considerations-for-personalized-technology">The findings reveal important design considerations for personalized technology</h2><p>The study highlights three critical design considerations for personalization interfaces:</p><p>First, systems need to clearly communicate their capabilities. Users struggled to understand what kinds of rules they could create or what preferences the reinforcement learning system could learn. This limited their ability to form and express preferences effectively.</p><p>Second, interfaces need to support flexible preference expression. Binary feedback proved too restrictive for many participants, while rule creation was challenging for others. A hybrid approach could offer the best of both worlds.</p><p>Third, systems should help users develop accurate mental models. Participants became confused when managing multiple rules or when trying to understand what the reinforcement learning system had learned. Providing context and explanations could help users build better mental models of how the system works.</p><h2 id="notes-on-generalizability">Notes on generalizability</h2><p>The study has limitations. The sample size of 15 participants is relatively small, and the laboratory setting with simulated meetings may not fully capture real-world complexity. Additionally, the researcher helped participants navigate the rule-based interface, which likely influenced the cognitive load measurements.</p><p>This study examined a specific personalization scenario (meeting notifications). Different contexts might yield different results depending on the complexity of user preferences and the nature of the personalization task.</p><p>The findings nonetheless offer valuable insights into how users experience different personalization approaches, highlighting the potential benefits of combining rule-based control with machine learning assistance.</p><h2 id="references">References</h2><p>Liu, R., Sarkar, A., Solovey, E., &amp; Tschiatschek, S. (2019, March). Evaluating rule-based programming and reinforcement learning for personalising an intelligent system. In 2nd Workshop on Explainable Smart Systems (ExSS 2019), held in conjunction with ACM Intelligent User Interfaces (IUI 2019).</p></div><footer class="content__footer"><div class="content__last-updated">This article was updated on <time datetime="2025-04-16T13:35">16 April 2025</time></div><div class="content__share"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fadvaitsarkar.github.io%2Fautoblog%2Fwhen-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html" class="js-share facebook tltp tltp--top" aria-label="Facebook" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://advaitsarkar.github.io/autoblog/assets/svg/svg-map.svg#facebook"/></svg> <span>Facebook</span> </a><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fadvaitsarkar.github.io%2Fautoblog%2Fwhen-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html&amp;via=Talking%20about%20papers&amp;text=When%20Technology%20Asks%20%22What%20Do%20You%20Want%3F%22%3A%20Rule-Based%20Programming%20vs.%20Reinforcement%20Learning%20in%20System%20Personalization" class="js-share twitter tltp tltp--top" aria-label="Twitter" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://advaitsarkar.github.io/autoblog/assets/svg/svg-map.svg#twitter"/></svg> <span>Twitter</span> </a><a href="https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fadvaitsarkar.github.io%2Fautoblog%2Fwhen-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html&amp;media=undefined&amp;description=When%20Technology%20Asks%20%22What%20Do%20You%20Want%3F%22%3A%20Rule-Based%20Programming%20vs.%20Reinforcement%20Learning%20in%20System%20Personalization" class="js-share pinterest tltp tltp--top" aria-label="Pinterest" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://advaitsarkar.github.io/autoblog/assets/svg/svg-map.svg#pinterest"/></svg> <span>Pinterest</span> </a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fadvaitsarkar.github.io%2Fautoblog%2Fwhen-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html" class="js-share linkedin tltp tltp--top" aria-label="Share with LinkedIn" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://advaitsarkar.github.io/autoblog/assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span> </a><a href="https://api.whatsapp.com/send?text=When%20Technology%20Asks%20%22What%20Do%20You%20Want%3F%22%3A%20Rule-Based%20Programming%20vs.%20Reinforcement%20Learning%20in%20System%20Personalization https%3A%2F%2Fadvaitsarkar.github.io%2Fautoblog%2Fwhen-technology-asks-what-do-you-want-rule-based-programming-vs-reinforcement-learning-in-system-personalization.html" class="js-share whatsapp tltp tltp--top" title="Share with LinkedIn" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://advaitsarkar.github.io/autoblog/assets/svg/svg-map.svg#whatsapp"/></svg> <span>WhatsApp</span></a></div></footer></div></article><div class="content__section post__related"><div class="main__inner"><h3 class="content__section__title">Related post</h3><div class="post__related__wrap"><article class="c-card"><div class="c-card__meta"><div class="c-card__author"><a href="https://advaitsarkar.github.io/autoblog/authors/anonymous/">TAP Communications</a></div><time datetime="2025-04-16T13:53">16 April 2025</time></div><header class="c-card__header"><h2 class="c-card__title"><a href="https://advaitsarkar.github.io/autoblog/when-complexity-becomes-relative-improving-word-complexity-identification-through-comparative-judgments.html">When Complexity Becomes Relative: Improving Word Complexity Identification Through Comparative Judgments</a></h2><p>Abstract This article breaks down the results of a research paper investigating whether comparative judgments lead to more reliable annotations&hellip;</p></header></article><article class="c-card"><div class="c-card__meta"><div class="c-card__author"><a href="https://advaitsarkar.github.io/autoblog/authors/anonymous/">TAP Communications</a></div><time datetime="2025-04-10T16:07">10 April 2025</time></div><header class="c-card__header"><h2 class="c-card__title"><a href="https://advaitsarkar.github.io/autoblog/visual-programming-tools-make-learning-probabilistic-programming-easier-for-novices.html">Visual Programming Tools Make Learning Probabilistic Programming Easier for Novices</a></h2><p>Abstract This article breaks down the findings of a study that explored how multiple representation tools can help novice programmers&hellip;</p></header></article><article class="c-card"><div class="c-card__meta"><div class="c-card__author"><a href="https://advaitsarkar.github.io/autoblog/authors/anonymous/">TAP Communications</a></div><time datetime="2025-04-10T15:48">10 April 2025</time></div><header class="c-card__header"><h2 class="c-card__title"><a href="https://advaitsarkar.github.io/autoblog/learning-with-machines-how-constructivist-design-shapes-interactive-machine-learning-systems.html">Learning With Machines: How Constructivist Design Shapes Interactive Machine Learning Systems</a></h2><p>Abstract This article breaks down the concepts presented in a paper exploring how constructivist learning theory can improve the design&hellip;</p></header></article><article class="c-card"><div class="c-card__meta"><div class="c-card__author"><a href="https://advaitsarkar.github.io/autoblog/authors/anonymous/">TAP Communications</a></div><time datetime="2025-04-10T15:18">10 April 2025</time></div><header class="c-card__header"><h2 class="c-card__title"><a href="https://advaitsarkar.github.io/autoblog/when-spreadsheets-sing-the-fusion-of-music-and-programming.html">When Spreadsheets Sing: The Fusion of Music and Programming</a></h2><p>Abstract This article breaks down a research paper that explores how spreadsheets can be transformed into accessible tools for music&hellip;</p></header></article><article class="c-card"><div class="c-card__meta"><div class="c-card__author"><a href="https://advaitsarkar.github.io/autoblog/authors/anonymous/">TAP Communications</a></div><time datetime="2025-04-10T15:02">10 April 2025</time></div><header class="c-card__header"><h2 class="c-card__title"><a href="https://advaitsarkar.github.io/autoblog/looking-where-you-click-how-eye-tracking-technology-can-replace-your-mouse-for-data-analysis.html">Looking Where You Click: How Eye-Tracking Technology Can Replace Your Mouse for Data Analysis</a></h2><p>Abstract This article examines a study developing a gaze-directed lens tool for data visualization that allows users to interact with&hellip;</p></header></article></div></div></div></main><div class="right-bar"><div class="right-bar__inner"><div class="sidebar"><div class="box copyright">© Copyright 2025-present. All rights reserved.</div></div></div></div></div><script defer="defer" src="https://advaitsarkar.github.io/autoblog/assets/js/scripts.min.js?v=b2d91bcadbf5db401b76eb5bb3092eb7"></script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>